## Interpretability of Neural Networks

Artificial neural networks are loosely inspired by the architecture of the neural cells in the brain: A network of connected cells. 
Mathematically, neural networks are chained matrix multiplications that map the first 'layer', your features, to the outcome of interest. 
The backpropagation method updates the multiplication weights to minimalize the predicted and actual outcome. 
Since mid of the 19th century. 

In 2012, neural networks became famous under the term 'Deep Learning'. 
Deep Learning are deep neural networks with a few additional tricks up it's sleeves. 
A zoo of different architectures for the neural networks arose and new animals are coming to the zoo every day. 
As broad classes We have convolutional neural networks (CNNs) for images, recursive neural networks (RNNs) for sequential data like text. 
At their core, all of them consist of weights ... a lot of weights. 
The weights are interacting heavily, so neural networks are not interpretable. 


But of course, a lot of researchers are concerned with making  neural networks more interpretable. 
As colorful as the architectures for neural networks are, are the approaches to make them interpretable.
This Chapter takes you on stride through the zoo of interpretability methods for neural networks. 
After reading this Chapter you will have a few pointers how to make networks interpretable, but no in depth knowledge about any of them. 
Your guide actually does not know much about each animal in this kind of zoo, but knows where to find the cages. 
Let's begin!
The feeding of the penguins is about to start!

```{r, fig.cap="Neural network architecture overview from www.asimovinstitute.org/neural-network-zoo/. This is just the broadest of classification. Each type of network can have different number of layers and so on. "}
knitr::include_graphics("images/neural-zoo.png")
```

### Looking at components, but not changing them

#### Partial occlusion
Not necessarily model-specific
https://medium.com/merantix/picasso-a-free-open-source-visualizer-for-cnns-d8ed3a35cfc5
TODO: Check if it fits in here

#### Looking at live activations
@Yosinski2015
https://github.com/yosinski/deep-visualization-toolbox

#### Optimizing images
See google cat thing.


#### Analysing the gradient


### Modifying the neural networks somewhat

#### Enforcing monotonicity
Making them monotonic: @Sill

#### Including attention mechanisms

https://www.oreilly.com/ideas/interpretability-via-attentional-and-memory-based-interfaces-using-tensorflow

#### Tweaking the loss function
@Ross2017 in the paper "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations".
Their approach is to penalize the loss function of gradient based models (read: neural networks) to influence the decision boundaries.
The result is both explaining predictions and regularizing the networks.

Some examples from the paper and comparisons with the LIME method (also described in book Chapter \@ref(lime)) are shown in Figure \@ref(fig:right-for-the-right-reasons-1) and \@ref(fig:right-for-the-right-reasons-2).

```{r right-for-the-right-reasons-1, fig.cap = 'Example from "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations"'}
knitr::include_graphics("images/ross-2017-1.png")
```

```{r right-for-the-right-reasons-2, fig.cap = 'Another example from "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations"'}
knitr::include_graphics("images/ross-2017-2.png")
```

### New architectures

#### Attention based

#### Generator and encoder
@Lei2016 propose in their paper "Rationalizing neural predictions" an architecture consisting of a generator and an encoder, which are trained in unison.
The goal is text classification and generate short and coherent text pieces from the target text as rationales for explaining the prediction.
The generator specifies a distribution over the text as candidate rationales and these are passed through the encoder for prediction.
The short text pieces (aka rationales) are highlighted to explain the prediction.
The training method is powered by reinforcement learning and the authors state that encoder and generator can be chosen by the user.
Figure \@ref(fig:rationalizing-neural-predictions) shows an example how such a text highlight looks like.

The code is available at https://github.com/taolei87/rcnn.

```{r rationalizing-neural-predictions, fig.cap = 'Example from "Rationalizing Neural Predictions"'}
knitr::include_graphics("images/rationalizing-neural-predictions.png")
```


### Use MORE neural networks!!!



#### Tweaking the loss function of CNN, using RNN to create sentence

paper: Generating Visual Explanations from @hendricks2016generating.
for classification, new loss function based on sampling and reeinforcement learning generates sentence
TODO: Use image from paper.


#### create explanations of action of RL RNN
The paper "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations" from @harrison2017rationalization presents an approach that uses neural machine translation to translate the inner state of an autonomous agent into natural language.
The training data for these explanations come from players thinking out loud as they play the game.
Really cool idea!
See Figure \@ref(fig:frogger) for an example.


```{r frogger, fig.cap = 'Example from "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations"'}
knitr::include_graphics("images/frogger.png")
```
