## Storytime {#storytime}

Get started with a few short stories.
Each story is an - admittedly exaggerated - call for interpretable machine learning.
If you are in a hurry, you can skip the stories.
If you want to be entertained and (de-)motivated, read on!

The format is inspired by Jack Clark's short stories in his [Import AI Newsletter](https://jack-clark.net/).
If you like this kind of tech short stories or are interested in AI, I recommend signing up.

### Lightning never strikes twice {-}

**2030: A medical lab in Switzerland**

"It's definitely not the worst way to die!" Tom summarised, trying to find something positive in the tragedy.
He was removing the pump's computer from the intravenous pole. <br>
"Just the wrong reasons." Lena added. <br>
"And certainly the with the wrong morphine pump!
Just creating more work for us!" Tom complained, while he was unscrewing the pump's back plate.
After he had removed all the screws he lifted the plate and put it aside.
He plugged a cable into the diagnostic port. <br>
"You didn't just complain about having a job, did you?" Lena gave him a mocking smile. <br>
"Of course not. Never!" he exclaimed with an ironic undertone.

He booted the pump.<br>
Lena plugged the other side of the cable into her tablet.
"Alright, diagnostics are running." she announced.
"I am really curious what went wrong."<br>
"It certainly shot our John Doe to Nirvana.
This high concentration of this morphine stuff.
Man. I mean ... that's a first, right?
Normally a broken pump gives too little of the sweet stuff or sometimes nothing.
But never, you know, like the most golden shot on earth." Tom explained. <br>
"I know. You don't have to convince me ... Hey, look at that." Lena held up the tablet.
"See this peek here? That's the painkiller cocktails potency.
Look, here is the reference level.
The machine pumped a painkiller mix into the poor guys blood system that could kill roughly 17 people.
And here ..." she swiped, "Here you can see the moment of the patient's demise." <br>
"So, any first ideas what happened, boss?" Tom asked his supervisor. <br>
"Hm ... The sensory functions seem to be okay.
Heart rate, oxygen levels, glucose, ... The data was collected as expected.
Some missing values in the blood oxygen data, but that's not unusual. Look.
It also picked up the slowing heart rate of the patient and the extremely low cortisol levels caused by the morphine derivate and the other pain blocking agents."
She continued swiping through the diagnostics. <br>
Tom stared in a gaze at the screen.
It was his first real device failure to investigate.

"Ok, here is our first piece of the puzzle.
The system failed to send a warning to the hospital communication channel.
The warning was triggered, but rejected on the protocol level.
Might be our fault, but could be also the hospitals fault.
Please send the logs over to the IT team." Lena told Tom. <br>
Tom nodded, eyes locked on the screen. <br>
Lena continued.
"It's weird.
The warning should also have triggered the shutdown of the pump.
But it obviously failed to do so.
That must be a bug.
Something the quality team missed.
Something really bad.
Maybe connected to the protocol issue." <br>
"So, the pump's emergency systems did not work, but why did it pump up our guy in the first place?" Tom wondered. <br>
"Good question.
You are right.
Protocol emergency failure aside, the pump shouldn't have administered this amount of medication in the first place.
The algorithm should have stopped much earlier on its own, given this sensory input." Lena explained. <br>
"Maybe some bad luck, like a one in a million thing, like being hit by a lightning?" Tom asked her. <br>
"No Tom.
If you would have read the documentation, which I sent to you, you would know that the pump was first trained on animal trials, later on humans to learn on its own how to find the perfect amount of pain killer, given the sensory input.
The pump's algorithm might be opaque and complex, but it is not random.
That means the pump would show the same behaviour given the same input.
Our patient would die again.
Some combination of sensory inputs must have triggered the erratical behaviour of the pump.
That's why we have to dig deeper and find out what happened here." Lena explained.

"I see ..." Tom responded thoughtfully.
"Wasn't the patient going to die soon anyway? Because of cancer or something?" <br>
Lena nodded while reading the analysis report. <br>
Tom got up, and walked to the window.
He looked outside, eyes fixating on some point in the distance.
"Maybe the machine did him a favour, you know, like freeing him from the pain. No suffering.
Maybe it just did the right thing.
Like a lightning, but, you know, a good one. I mean like the lottery, but not random. For a reason." <br>
She finally lifted her head and looked at him. <br>
He continued looking at something outside. <br>
Nobody said anything for a minute or two. <br>
Lena lowered her head again and continued the analysis.
"No Tom. It's a bug... Just some goddam bug".

### Trust fall {-}

**2050, A subway station in Singapore**

With her thoughts she was already at work, while she was rushing to Bishan subway station.
The tests for the new neural architecture should have finished by now.
She lead the re-design of the government's "tax affinity prediction system for individual entities", which predicts if an individual will hide money from the tax office.
Her team came up with an elegant piece of engineering.
If successful, the system would not only serve the tax office, but also feed into other systems, like the anti-terrorist defence and the trade registry.
One day, the government might even integrate it into the civic trust score.
The trust system estimates how trustworthy an individual is. The estimate affects every part of your daily life, like getting a loan or how long you have to wait when getting a new passport.
Descending the escalator, she imagined how an integration into the current trust score system could look like.

Routinely, she wiped her hand over the RFID reader without reducing her walking speed.
Her mind was occupied, yet a dissonance of sensory expectations and reality rang alarm bells in her brain.

Too late.

Nose first she ran into the subway entrance gate and fell, bottom first, onto the floor.
The door was supposed to open, ... but it didn't.
Baffled, she stood up and looked at the gate's screen.
"Please try again some other time." it suggested in friendly colours.
A person walked by, and, ignoring her, wiped his hand over the reader.
The doors opened and he walked through.
The doors closed again.
She wiped her nose.
It hurt, but at least it wasn't bleeding.
She tried to open the door, but got rejected again.
It was odd.
Maybe her public transport account did not have sufficient tokens, which would be unusual, because funding should happen automatically.
She raised her watch to check the account balance.

"Login denied. Please contact your Citizens Advice Bureau!", the projection informed her.

A feeling of nausea hit her like a fist to the stomach.
With trembling hands she started the mobile game "Sniper Guild", an ego-shooter.
After a few seconds, the loading screen shut down.
She felt dizzy and dropped down on the floor.

There was only one possible explanation:
Her trust score had dropped. Substantially.
A small drop meant minor inconveniences, like not getting first class flights.
A low trust score was rare and meant that you were classified as a threat to society.
One part of dealing with those people was to keep them from public places - for example the subway.
The government restricted financial transactions of low-trust subjects.
The authorities started active monitoring of your behaviour on social media, even going so far as to restrict certain contents, like violent games.
It became exponentially more difficult to increase your trust score, the lower it was.
People with a very low trust score usually never recovered.

She could think of no reason why her score should have dropped.
The score was based on machine learning.
It worked like a well oiled engine, stabilising society.
The performance of the trust score system was always closely monitored.
Machine learning had become A LOT better since the beginning of the century.
It had become so efficient that decisions made by the trust score system could not be disputed.
An infallible system.

She laughed hysterically.
Infallible system.
If only.
The system failed rarely.
But it did fail.
She was an edge case;
an error of the system;
from now on, an outcast.
Nobody dared to question the system, it had become too integrated into the government, into society itself to be questioned.
In democratic countries it was forbidden to form anti-democratic movements, not because they where inherently vicious, but because they would de-stabilise the current system.
The same logic applied to algorithmic critique:
Critique in the algorithms was forbidden, because of its danger to the status quo.

The algorithmic trust was the very fabric the societal order was made of.
For the greater good, rare incorrect trust scorings were accepted silently.
Hundreds of other prediction systems and databases were feeding into the score, so it was impossible to know what triggered the drop in her score.
Wild emotions twisted her, most of all, terror.
She vomited on the floor.

Her tax affinity system was eventually integrated into the trust system, but she never got to know.
