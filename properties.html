<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2018-11-13">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="evaluating-interpretability.html">
<link rel="next" href="explanation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>2.1</b> Storytime</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#lightning-never-strikes-twice"><i class="fa fa-check"></i>Lightning Never Strikes Twice</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#trust-fall"><i class="fa fa-check"></i>Trust Fall</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#fermis-paperclips"><i class="fa fa-check"></i>Fermi’s Paperclips</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2.2</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="2.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>2.3</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>3</b> Interpretability</a><ul>
<li class="chapter" data-level="3.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>3.1</b> The Importance of Interpretability</a></li>
<li class="chapter" data-level="3.2" data-path="taxonomy-of-interpretability-methods.html"><a href="taxonomy-of-interpretability-methods.html"><i class="fa fa-check"></i><b>3.2</b> Taxonomy of Interpretability Methods</a></li>
<li class="chapter" data-level="3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>3.3</b> Scope of Interpretability</a><ul>
<li class="chapter" data-level="3.3.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>3.3.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="3.3.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>3.3.2</b> Global, Holistic Model Interpretability</a></li>
<li class="chapter" data-level="3.3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>3.3.3</b> Global Model Interpretability on a Modular Level</a></li>
<li class="chapter" data-level="3.3.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>3.3.4</b> Local Interpretability for a Single Prediction</a></li>
<li class="chapter" data-level="3.3.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-predictions"><i class="fa fa-check"></i><b>3.3.5</b> Local Interpretability for a Group of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html"><i class="fa fa-check"></i><b>3.4</b> Evaluating Interpretability</a></li>
<li class="chapter" data-level="3.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>3.5</b> Properties of Explanations</a></li>
<li class="chapter" data-level="3.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>3.6</b> Human-friendly Explanations</a><ul>
<li class="chapter" data-level="3.6.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>3.6.1</b> What is an explanation?</a></li>
<li class="chapter" data-level="3.6.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>3.6.2</b> What is a “good” explanation?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b> Datasets</a><ul>
<li class="chapter" data-level="4.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>4.1</b> Bike Sharing Counts (Regression)</a></li>
<li class="chapter" data-level="4.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>4.2</b> YouTube Spam Comments (Text Classification)</a></li>
<li class="chapter" data-level="4.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>4.3</b> Risk Factors for Cervical Cancer (Classification)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Interpretable Models</a><ul>
<li class="chapter" data-level="5.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>5.1</b> Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>5.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.1.2" data-path="limo.html"><a href="limo.html#example"><i class="fa fa-check"></i><b>5.1.2</b> Example</a></li>
<li class="chapter" data-level="5.1.3" data-path="limo.html"><a href="limo.html#visual-interpretation"><i class="fa fa-check"></i><b>5.1.3</b> Visual Interpretation</a></li>
<li class="chapter" data-level="5.1.4" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>5.1.4</b> Explaining Single Predictions</a></li>
<li class="chapter" data-level="5.1.5" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>5.1.5</b> Coding Categorical Features</a></li>
<li class="chapter" data-level="5.1.6" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>5.1.6</b> Do Linear Models Create Good Explanations?</a></li>
<li class="chapter" data-level="5.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>5.1.7</b> Sparse Linear Models</a></li>
<li class="chapter" data-level="5.1.8" data-path="limo.html"><a href="limo.html#advantages"><i class="fa fa-check"></i><b>5.1.8</b> Advantages</a></li>
<li class="chapter" data-level="5.1.9" data-path="limo.html"><a href="limo.html#disadvantages"><i class="fa fa-check"></i><b>5.1.9</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic.html"><a href="logistic.html#whats-wrong-with-linear-regression-models-for-classification"><i class="fa fa-check"></i><b>5.2.1</b> What’s Wrong with Linear Regression Models for Classification?</a></li>
<li class="chapter" data-level="5.2.2" data-path="logistic.html"><a href="logistic.html#theory"><i class="fa fa-check"></i><b>5.2.2</b> Theory</a></li>
<li class="chapter" data-level="5.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>5.2.3</b> Interpretation</a></li>
<li class="chapter" data-level="5.2.4" data-path="logistic.html"><a href="logistic.html#example-1"><i class="fa fa-check"></i><b>5.2.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="advantages-and-disadvantages.html"><a href="advantages-and-disadvantages.html"><i class="fa fa-check"></i><b>6</b> Advantages and Disadvantages</a><ul>
<li class="chapter" data-level="6.1" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>6.1</b> Linear Model 2.0 - GLMs, GAMs and more</a><ul>
<li class="chapter" data-level="6.1.1" data-path="extend-lm.html"><a href="extend-lm.html#non-gaussian-outcomes---glms"><i class="fa fa-check"></i><b>6.1.1</b> Non-Gaussian Outcomes - GLMs</a></li>
<li class="chapter" data-level="6.1.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>6.1.2</b> Interactions</a></li>
<li class="chapter" data-level="6.1.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>6.1.3</b> Nonlinear Effects - GAMs</a></li>
<li class="chapter" data-level="6.1.4" data-path="extend-lm.html"><a href="extend-lm.html#advantages-1"><i class="fa fa-check"></i><b>6.1.4</b> Advantages</a></li>
<li class="chapter" data-level="6.1.5" data-path="extend-lm.html"><a href="extend-lm.html#disadvantages-1"><i class="fa fa-check"></i><b>6.1.5</b> Disadvantages</a></li>
<li class="chapter" data-level="6.1.6" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>6.1.6</b> Further extensions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>6.2</b> Decision Tree</a><ul>
<li class="chapter" data-level="6.2.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>6.2.1</b> Interpretation</a></li>
<li class="chapter" data-level="6.2.2" data-path="tree.html"><a href="tree.html#interpretation-example"><i class="fa fa-check"></i><b>6.2.2</b> Interpretation Example</a></li>
<li class="chapter" data-level="6.2.3" data-path="tree.html"><a href="tree.html#advantages-2"><i class="fa fa-check"></i><b>6.2.3</b> Advantages</a></li>
<li class="chapter" data-level="6.2.4" data-path="tree.html"><a href="tree.html#disadvantages-2"><i class="fa fa-check"></i><b>6.2.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>6.3</b> Decision Rules (IF-THEN)</a><ul>
<li class="chapter" data-level="6.3.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>6.3.1</b> Learn Rules from a Single Feature (OneR)</a></li>
<li class="chapter" data-level="6.3.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>6.3.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="6.3.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>6.3.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="6.3.4" data-path="rules.html"><a href="rules.html#advantages-3"><i class="fa fa-check"></i><b>6.3.4</b> Advantages</a></li>
<li class="chapter" data-level="6.3.5" data-path="rules.html"><a href="rules.html#disadvantages-3"><i class="fa fa-check"></i><b>6.3.5</b> Disadvantages</a></li>
<li class="chapter" data-level="6.3.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>6.3.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>6.4</b> RuleFit</a><ul>
<li class="chapter" data-level="6.4.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>6.4.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="6.4.2" data-path="rulefit.html"><a href="rulefit.html#guidelines"><i class="fa fa-check"></i><b>6.4.2</b> Guidelines</a></li>
<li class="chapter" data-level="6.4.3" data-path="rulefit.html"><a href="rulefit.html#theory-1"><i class="fa fa-check"></i><b>6.4.3</b> Theory</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>6.5</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="6.5.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>6.5.1</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="6.5.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>6.5.2</b> K-Nearest Neighbours</a></li>
<li class="chapter" data-level="6.5.3" data-path="other-interpretable.html"><a href="other-interpretable.html#and-so-many-more"><i class="fa fa-check"></i><b>6.5.3</b> And so many more …</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>7</b> Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="7.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>7.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="7.1.1" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>7.1.1</b> Examples</a></li>
<li class="chapter" data-level="7.1.2" data-path="pdp.html"><a href="pdp.html#advantages-4"><i class="fa fa-check"></i><b>7.1.2</b> Advantages</a></li>
<li class="chapter" data-level="7.1.3" data-path="pdp.html"><a href="pdp.html#disadvantages-4"><i class="fa fa-check"></i><b>7.1.3</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>7.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ice.html"><a href="ice.html#example-2"><i class="fa fa-check"></i><b>7.2.1</b> Example</a></li>
<li class="chapter" data-level="7.2.2" data-path="ice.html"><a href="ice.html#advantages-5"><i class="fa fa-check"></i><b>7.2.2</b> Advantages</a></li>
<li class="chapter" data-level="7.2.3" data-path="ice.html"><a href="ice.html#disadvantages-5"><i class="fa fa-check"></i><b>7.2.3</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>7.3</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>7.3.1</b> Motivation and Intuition</a></li>
<li class="chapter" data-level="7.3.2" data-path="ale.html"><a href="ale.html#theory-2"><i class="fa fa-check"></i><b>7.3.2</b> Theory</a></li>
<li class="chapter" data-level="7.3.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>7.3.3</b> Estimation</a></li>
<li class="chapter" data-level="7.3.4" data-path="ale.html"><a href="ale.html#examples-1"><i class="fa fa-check"></i><b>7.3.4</b> Examples</a></li>
<li class="chapter" data-level="7.3.5" data-path="ale.html"><a href="ale.html#advantages-6"><i class="fa fa-check"></i><b>7.3.5</b> Advantages</a></li>
<li class="chapter" data-level="7.3.6" data-path="ale.html"><a href="ale.html#disadvantages-6"><i class="fa fa-check"></i><b>7.3.6</b> Disadvantages</a></li>
<li class="chapter" data-level="7.3.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>7.3.7</b> Implementation and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>7.4</b> Feature Interaction</a><ul>
<li class="chapter" data-level="7.4.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>7.4.1</b> Feature Interaction?</a></li>
<li class="chapter" data-level="7.4.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>7.4.2</b> Theory: Friedman’s H-statistic</a></li>
<li class="chapter" data-level="7.4.3" data-path="interaction.html"><a href="interaction.html#examples-2"><i class="fa fa-check"></i><b>7.4.3</b> Examples</a></li>
<li class="chapter" data-level="7.4.4" data-path="interaction.html"><a href="interaction.html#advantages-7"><i class="fa fa-check"></i><b>7.4.4</b> Advantages</a></li>
<li class="chapter" data-level="7.4.5" data-path="interaction.html"><a href="interaction.html#disadvantages-7"><i class="fa fa-check"></i><b>7.4.5</b> Disadvantages</a></li>
<li class="chapter" data-level="7.4.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>7.4.6</b> Implementations</a></li>
<li class="chapter" data-level="7.4.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>7.4.7</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>7.5</b> Feature Importance</a><ul>
<li class="chapter" data-level="7.5.1" data-path="feature-importance.html"><a href="feature-importance.html#the-theory"><i class="fa fa-check"></i><b>7.5.1</b> The Theory</a></li>
<li class="chapter" data-level="7.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>7.5.2</b> Should I Compute Importance on Training or Test Data?</a></li>
<li class="chapter" data-level="7.5.3" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>7.5.3</b> Example and Interpretation</a></li>
<li class="chapter" data-level="7.5.4" data-path="feature-importance.html"><a href="feature-importance.html#advantages-8"><i class="fa fa-check"></i><b>7.5.4</b> Advantages</a></li>
<li class="chapter" data-level="7.5.5" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-8"><i class="fa fa-check"></i><b>7.5.5</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>7.6</b> Global Surrogate Models</a><ul>
<li class="chapter" data-level="7.6.1" data-path="global.html"><a href="global.html#theory-3"><i class="fa fa-check"></i><b>7.6.1</b> Theory</a></li>
<li class="chapter" data-level="7.6.2" data-path="global.html"><a href="global.html#example-4"><i class="fa fa-check"></i><b>7.6.2</b> Example</a></li>
<li class="chapter" data-level="7.6.3" data-path="global.html"><a href="global.html#advantages-9"><i class="fa fa-check"></i><b>7.6.3</b> Advantages</a></li>
<li class="chapter" data-level="7.6.4" data-path="global.html"><a href="global.html#disadvantages-9"><i class="fa fa-check"></i><b>7.6.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>7.7</b> Local Surrogate Models (LIME)</a><ul>
<li class="chapter" data-level="7.7.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>7.7.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="7.7.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>7.7.2</b> LIME for Text</a></li>
<li class="chapter" data-level="7.7.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>7.7.3</b> LIME for Images</a></li>
<li class="chapter" data-level="7.7.4" data-path="lime.html"><a href="lime.html#advantages-10"><i class="fa fa-check"></i><b>7.7.4</b> Advantages</a></li>
<li class="chapter" data-level="7.7.5" data-path="lime.html"><a href="lime.html#disadvantages-10"><i class="fa fa-check"></i><b>7.7.5</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>7.8</b> Shapley Value Explanations</a><ul>
<li class="chapter" data-level="7.8.1" data-path="shapley.html"><a href="shapley.html#the-general-idea"><i class="fa fa-check"></i><b>7.8.1</b> The general idea</a></li>
<li class="chapter" data-level="7.8.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>7.8.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="7.8.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>7.8.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="7.8.4" data-path="shapley.html"><a href="shapley.html#advantages-11"><i class="fa fa-check"></i><b>7.8.4</b> Advantages</a></li>
<li class="chapter" data-level="7.8.5" data-path="shapley.html"><a href="shapley.html#disadvantages-11"><i class="fa fa-check"></i><b>7.8.5</b> Disadvantages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>8</b> Example-based Explanations</a><ul>
<li class="chapter" data-level="8.1" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>8.1</b> Counterfactual explanations</a><ul>
<li class="chapter" data-level="8.1.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>8.1.1</b> Generating counterfactual explanations</a></li>
<li class="chapter" data-level="8.1.2" data-path="counterfactual.html"><a href="counterfactual.html#examples-3"><i class="fa fa-check"></i><b>8.1.2</b> Examples</a></li>
<li class="chapter" data-level="8.1.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-12"><i class="fa fa-check"></i><b>8.1.3</b> Advantages</a></li>
<li class="chapter" data-level="8.1.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-12"><i class="fa fa-check"></i><b>8.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="8.1.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>8.1.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>8.2</b> Adversarial Examples</a><ul>
<li class="chapter" data-level="8.2.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>8.2.1</b> Methods and Examples</a></li>
<li class="chapter" data-level="8.2.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>8.2.2</b> The Cybersecurity Perspective</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>8.3</b> Prototypes and Criticisms</a><ul>
<li class="chapter" data-level="8.3.1" data-path="proto.html"><a href="proto.html#theory-4"><i class="fa fa-check"></i><b>8.3.1</b> Theory</a></li>
<li class="chapter" data-level="8.3.2" data-path="proto.html"><a href="proto.html#examples-4"><i class="fa fa-check"></i><b>8.3.2</b> Examples</a></li>
<li class="chapter" data-level="8.3.3" data-path="proto.html"><a href="proto.html#advantages-13"><i class="fa fa-check"></i><b>8.3.3</b> Advantages</a></li>
<li class="chapter" data-level="8.3.4" data-path="proto.html"><a href="proto.html#disadvantages-13"><i class="fa fa-check"></i><b>8.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="8.3.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>8.3.5</b> Code and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>8.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="8.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>8.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="8.4.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>8.4.2</b> Influence Functions</a></li>
<li class="chapter" data-level="8.4.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>8.4.3</b> Advantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="8.4.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>8.4.4</b> Disadvantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="8.4.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>8.4.5</b> Software and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>9</b> A Look into the Crystal Ball</a><ul>
<li class="chapter" data-level="9.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>9.1</b> The Future of Machine Learning</a></li>
<li class="chapter" data-level="9.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>9.2</b> The Future of Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>10</b> Contribute</a></li>
<li class="chapter" data-level="11" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>11</b> Citation</a></li>
<li class="chapter" data-level="12" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>12</b> Acknowledgements</a></li>
<li class="chapter" data-level="13" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>13</b> References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties" class="section level2">
<h2><span class="header-section-number">3.5</span> Properties of Explanations</h2>
<p>We want to explain the predictions of a machine learning model. To achieve this, we rely on an explanation method, an algorithm that generates explanations. <strong>An explanation usually relates the feature values of an instance to its model prediction in a humanly understandable way.</strong> Other types of explanations consist of a set of data instances (e.g in the case of the k-nearest neighbour model). For example, we could predict cancer risk using a support vector machine and explain predictions using the <a href="lime.html#lime">local surrogate method</a>, which generates decision trees as explanations. Or we could use a linear regression model instead of a support vector machine. The linear regression model is already equipped with an explanation method (interpretation of the parameters of the weighted sum).</p>
<p>We take a closer look at the properties of explanation methods and explanations (Robnik-Sikonja, Marko, and Marko Bohanec, 2018 <a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>). These properties can be used to judge how good an explanation (method) is. It’s not clear for all these properties how to measure them correctly, so one of the challenges is to formalize how they could be calculated.</p>
<p><strong>Properties of Explanation Methods</strong></p>
<ul>
<li><strong>Expressive Power</strong> is the “language” or structure of the explanations the method is able to generate. An explanation method could generate IF-THEN rules, decision trees, a weighted sum, natural language or something else.</li>
<li><strong>Translucency</strong> describes how much the explanation method relies on looking into the machine learning model, like its parameters. For example, explanation methods relying on intrinsically interpretable models like the linear regression model (model-specific) are highly translucent. Methods only relying on manipulating inputs and observing the predictions have zero translucency. Depending on the scenario, different levels of translucency might be desirable. The advantage of high translucency is that the method can rely on more information to generate explanations. The advantage of low translucency is that the explanation is more portable.</li>
<li><strong>Portability</strong> describes the range of machine learning models with which the explanation method can be used. Methods with a low translucency have a higher portability because they treat the machine learning model as a black box. <a href="lime.html#lime">Local-surrogate models</a> might be the explanation method with the highest portability. Methods that only work for e.g. recurrent neural networks have low portability.</li>
<li><strong>Algorithmic Complexity</strong> describes the computational complexity of the method that generates the explanation. This property is important to consider when computation time is a bottleneck in generating explanations.</li>
</ul>
<p><strong>Properties of Individual Explanations</strong></p>
<ul>
<li><strong>Accuracy</strong>: How well does an explanation predict unseen data? High accuracy is especially important if the explanation is used for predictions in place of the machine learning model. Low accuracy can be fine if the accuracy of the machine learning model is also low, and if the goal is to explain what the black box model does. In this case, only fidelity is important.</li>
<li><strong>Fidelity</strong>: How well does the explanation approximate the prediction of the black box model? High fidelity is one of the most important properties of an explanation, because an explanation with low fidelity is useless to explain the machine learning model. Accuracy and fidelity are closely linked. If the black box model has high accuracy and the explanation has high fidelity, the explanation also has high accuracy. Some explanations offer only local fidelity, meaning the explanation only approximates well to the model prediction for a subset of the data (e.g. <a href="lime.html#lime">local surrogate models</a>) or even for only an individual data instance (e.g. <a href="shapley.html#shapley">Shapley Values</a>).</li>
<li><strong>Consistency</strong>: How much does an explanation differ between models that have been trained on the same task and that produce similar predictions? For example, I train an SVM and a linear regression model on the same task and both produce very similar predictions. I compute explanations using a method of my choice and analyse how different the explanations are. If the explanations are very similar, the explanations are highly consistent. I find this property somewhat tricky, since the two models could use different features, but get similar predictions (also called <a href="https://en.wikipedia.org/wiki/Rashomon_effect">“Rashomon Effect”</a>). In this case a high consistency is not desirable because the explanations have to be very different. High consistency is desirable if the models really rely on similar relationships.</li>
<li><strong>Stability</strong>: How similar are the explanations for similar instances? While consistency compares explanations between models, stability compares explanations between similar instance for a fixed model. High stability means that slight variations in the features of an instance don’t substantially change the explanation (unless these slight variations also strongly change the prediction). A lack of stability can be the result of a high variance of the explanation method, i.e. the explanation method is strongly affected by slight changes of the feature values of the instance to be explained. A lack of stability can also be caused by non-deterministic components of the explanation method, such as a data sampling step, like the <a href="lime.html#lime">local surrogate method</a> has. High stability is always desirable.</li>
<li><strong>Comprehensibility</strong>: How well do humans understand the explanations? This looks just like one more property among many, but it’s the elephant in the room. Difficult to define and measure, but extremely important to get right. Many people agree that comprehensibility depends on the audience. Ideas for measuring comprehensibility include measuring the size of the explanation (number of features with a non-zero weights in a linear model, number of decision rules, …) or testing how well people can predict the behaviour of the machine learning model from the explanations. The comprehensibility of the features used in the explanation should also be considered. A complex transformation of features might be less comprehensible than the original features.</li>
<li><strong>Certainty</strong>: Does the explanation reflect the certainty of the machine learning model? Many machine learning models only give predictions without a statement about the models confidence that the prediction is correct. If the model predicts a 4% probability of cancer for one patient, is it as certain as the 4% probability that another patient, with different attributes, received? An explanation that includes the model’s certainty is very useful.</li>
<li><strong>Degree of Importance</strong>: How well does the explanation reflect the importance of features or parts of the explanation? For example, if a decision rule is generated as an explanation for an individual prediction, is it clear which of the conditions of the rule was the most important?</li>
<li><strong>Novelty</strong>: Does the explanation reflect whether a data instance to be explained comes from a “new” region far removed from the distribution of training data? In such cases, the model may be inaccurate and the explanation may be useless. The concept of novelty is related to the concept of certainty. The higher the novelty, the more likely it is that the model will have low certainty due to lack of data.</li>
<li><strong>Representativeness</strong>: How many instances does an explanation cover? Explanations can cover the entire model (e.g. interpretation of weights in a linear regression model) or represent only an individual prediction (e.g. <a href="shapley.html#shapley">Shapley values, a game theoretical attribution method</a>).</li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>Robnik-Sikonja, Marko, and Marko Bohanec. “Perturbation-Based Explanations of Prediction Models.” Human and Machine Learning. Springer, Cham, 2018. 159-175.<a href="properties.html#fnref8">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="evaluating-interpretability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="explanation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/02-interpretability.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
