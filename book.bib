@article{friedman2008predictive,
  title={Predictive learning via rule ensembles},
  author={Friedman, Jerome H and Popescu, Bogdan E},
  journal={The Annals of Applied Statistics},
  pages={916--954},
  year={2008},
  publisher={JSTOR}
}

@article{bike2013,
year={2013},
issn={2192-6352},
journal={Progress in Artificial Intelligence},
doi={10.1007/s13748-013-0040-3},
title={Event labeling combining ensemble detectors and background knowledge},
url={[Web Link]},
publisher={Springer Berlin Heidelberg},
keywords={Event labeling; Event detection; Ensemble learning; Background knowledge},
author={Fanaee-T, Hadi and Gama, Joao},
pages={1-15}
}

@inproceedings{fernandes2017transfer,
  title={Transfer Learning with Partial Observability Applied to Cervical Cancer Screening},
  author={Fernandes, Kelwin and Cardoso, Jaime S and Fernandes, Jessica},
  booktitle={Iberian Conference on Pattern Recognition and Image Analysis},
  pages={243--250},
  year={2017},
  organization={Springer}
}

@article{Strobl2008,
abstract = {BACKGROUND: Random forests are becoming increasingly popular in many scientific fields because they can cope with "small n large p" problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables. RESULTS: We identify two mechanisms responsible for this finding: (i) A preference for the selection of correlated predictors in the tree building process and (ii) an additional advantage for correlated predictor variables induced by the unconditional permutation scheme that is employed in the computation of the variable importance measure. Based on these considerations we develop a new, conditional permutation scheme for the computation of the variable importance measure. CONCLUSION: The resulting conditional variable importance reflects the true impact of each predictor variable more reliably than the original marginal approach.},
author = {Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
doi = {10.1186/1471-2105-9-307},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Strobl et al. - 2008 - Conditional variable importance for random forests.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Amino Acid Sequence,Binding Sites,Biometry,Biometry: methods,Computational Biology,Computational Biology: methods,Decision Trees,Factor Analysis,Major Histocompatibility Complex,Major Histocompatibility Complex: genetics,Nonparametric,Regression Analysis,Research Design,Statistical,Statistics,xai-book},
mendeley-groups = {Master Thesis},
mendeley-tags = {xai-book},
month = {jan},
pages = {307},
pmid = {18620558},
title = {{Conditional variable importance for random forests.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2491635{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {9},
year = {2008}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{goldstein2015peeking,
  title={Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation},
  author={Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  journal={Journal of Computational and Graphical Statistics},
  volume={24},
  number={1},
  pages={44--65},
  year={2015},
  publisher={Taylor \& Francis}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@inproceedings{alberto2015tubespam,
  title={Tubespam: Comment spam filtering on youtube},
  author={Alberto, T{\'u}lio C and Lochter, Johannes V and Almeida, Tiago A},
  booktitle={Machine Learning and Applications (ICMLA), 2015 IEEE 14th International Conference on},
  pages={138--143},
  year={2015},
  organization={IEEE}
}


@article{Lipton2016,
abstract = {Supervised machine learning models boast re-markable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but inter-pretable. And yet the task of interpretation ap-pears underspecified. Papers provide diverse and sometimes non-overlapping motivations for in-terpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim inter-pretability axiomatically, absent further explana-tion. In this paper, we seek to refine the dis-course on interpretability. First, we examine the motivations underlying interest in interpretabil-ity, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of dif-ferent notions, and question the oft-made asser-tions that linear models are interpretable and that deep neural networks are not.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.03490v1},
author = {Lipton, Zachary C},
eprint = {arXiv:1606.03490v1},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Lipton - 2016 - The Mythos of Model Interpretability.pdf:pdf},
journal = {ICML Workshop on Human Interpretability in Machine Learning},
keywords = {Black Box,Deep Learning,Interpretability,Machine Learning,Supervised Learning,xai-book},
mendeley-tags = {xai-book},
number = {Whi},
title = {{The Mythos of Model Interpretability}},
year = {2016}
}

@book{Hastie2009,
author = {Hastie, T and Tibshirani, R and Friedman, J},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - 2009 - The elements of statistical learning.pdf:pdf},
title = {{The elements of statistical learning}},
url = {http://link.springer.com/content/pdf/10.1007/978-0-387-84858-7.pdf},
year = {2009}
}



@article{Ribeiro2016b,
abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found re-newed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to inter-pretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, com-parison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
archivePrefix = {arXiv},
arxivId = {1606.05386},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1606.05386},
file = {:Users/chris/Downloads/1606.05386.pdf:pdf},
journal = {ICML Workshop on Human Interpretability in Machine Learning},
keywords = {comprehensibil,interpretability, machine learning, comprehensibil,machine learning,model-agnostic,xai-book},
mendeley-tags = {model-agnostic,xai-book},
number = {Whi},
title = {{Model-Agnostic Interpretability of Machine Learning}},
year = {2016}
}

@article{Doshi-Velez2017,
abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
archivePrefix = {arXiv},
arxivId = {1702.08608},
author = {Doshi-Velez, Finale and Kim, Been},
eprint = {1702.08608},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Doshi-Velez, Kim - 2017 - Towards A Rigorous Science of Interpretable Machine Learning.pdf:pdf},
keywords = {xai-book},
mendeley-tags = {xai-book},
number = {Ml},
pages = {1--13},
title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
url = {http://arxiv.org/abs/1702.08608},
year = {2017}
}

@misc{algorithm,
  title = {Definition of Algorithm},
  howpublished = {\url{https://www.merriam-webster.com/dictionary/algorithm}},
  note = {Accessed: 2017-02-12},
  year = {2017}
}


@article{Turner2015,
abstract = {We propose a general model explanation system (MES) for “explaining” the output of black box classifiers. In this introduction we use the motivating example of a classifier trained to detect fraud in a credit card transaction history. The key aspect is that we provide explanations applicable to a single prediction, rather than provide an interpretable set of parameters. The labels in the provided examples are usually negative. Hence, we focus on explaining positive predictions (alerts).},
annote = {From Duplicate 2 (A Model Explanation System - Turner, Ryan) Extensions to multi-class should be easy, because if you only look at one class of interest it is again the same as the binary case. How to extend this to regression?},
archivePrefix = {arXiv},
arxivId = {1606.09517},
author = {Turner, Ryan},
eprint = {1606.09517},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Turner - Unknown - A Model Explanation System.pdf:pdf;:Users/chris/Downloads/1606.09517.pdf:pdf},
isbn = {9781509007462},
journal = {NIPS Workshop},
keywords = {,xai-book},
mendeley-tags = {xai-book},
pages = {1--5},
title = {{A Model Explanation System}},
url = {http://www.blackboxworkshop.org/pdf/Turner2015{\_}MES.pdf},
volume = {0},
year = {2015}
}

@article{miller2017explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={arXiv preprint arXiv:1706.07269},
  year={2017}
}

@article{lipton1990contrastive,
  title={Contrastive explanation},
  author={Lipton, Peter},
  journal={Royal Institute of Philosophy Supplements},
  volume={27},
  pages={247--266},
  year={1990},
  publisher={Cambridge University Press}
}

@article{heider1944experimental,
  title={An experimental study of apparent behavior},
  author={Heider, Fritz and Simmel, Marianne},
  journal={The American journal of psychology},
  volume={57},
  number={2},
  pages={243--259},
  year={1944},
  publisher={JSTOR}
}


@techreport{kahneman1981simulation,
  title={The simulation heuristic.},
  author={Kahneman, Daniel and Tversky, Amos},
  year={1981},
  institution={STANFORD UNIV CA DEPT OF PSYCHOLOGY}
}

@misc{grice1975logic,
  title={Logic and Conversation: In Syntax and Seman-tics3},
  author={Grice, HP},
  year={1975},
  publisher={New York: New York Press}
}


@inproceedings{vstrumbelj2011general,
  title={A general method for visualizing and explaining black-box regression models},
  author={{\v{S}}trumbelj, Erik and Kononenko, Igor},
  booktitle={International Conference on Adaptive and Natural Computing Algorithms},
  pages={21--30},
  year={2011},
  organization={Springer}
}


@article{nickerson1998confirmation,
  title={Confirmation bias: A ubiquitous phenomenon in many guises.},
  author={Nickerson, Raymond S},
  journal={Review of general psychology},
  volume={2},
  number={2},
  pages={175},
  year={1998},
  publisher={Educational Publishing Foundation}
}

@Manual{pre2017,
  title = {pre: Prediction Rule Ensembles},
  author = {Marjolein Fokkema and Benjamin Christoffersen},
  year = {2017},
  note = {R package version 0.4},
  url = {https://CRAN.R-project.org/package=pre},
}


@inproceedings{hendricks2016generating,
  title={Generating visual explanations},
  author={Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Marcus and Donahue, Jeff and Schiele, Bernt and Darrell, Trevor},
  booktitle={European Conference on Computer Vision},
  pages={3--19},
  year={2016},
  organization={Springer}
}

@article{harrison2017rationalization,
  title={Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations},
  author={Harrison, Brent and Ehsan, Upol and Riedl, Mark O},
  journal={arXiv preprint arXiv:1702.07826},
  year={2017}
}


@article{Lei2016,
abstract = {Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.},
archivePrefix = {arXiv},
arxivId = {1606.04155},
author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
eprint = {1606.04155},
file = {:home/chris/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lei, Barzilay, Jaakkola - 2016 - Rationalizing Neural Predictions.pdf:pdf},
journal = {EMNLP 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
keywords = {deep learning,xai-book},
mendeley-groups = {IML,IML/5. done},
mendeley-tags = {deep learning,xai-book},
pages = {107--117},
title = {{Rationalizing Neural Predictions}},
url = {http://arxiv.org/abs/1606.04155},
year = {2016}
}

@article{Sill,
author = {Sill, Joseph},
file = {:home/chris/Downloads{\_}paper/1358-monotonic-networks.pdf:pdf},
title = {{Monotonic Networks}}
}

@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:home/chris/Downloads{\_}paper/1506.06579.pdf:pdf},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}


@article{Ross2017,
abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
archivePrefix = {arXiv},
arxivId = {1703.03717},
author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
eprint = {1703.03717},
file = {:home/chris/Downloads{\_}paper/0371.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning: Learning Theory,Machine Learning: Machine Learning,Machine Learning: Neural Networks,Multidisciplinary Topics and Applications: Validat},
mendeley-groups = {IML/1. read maybe},
pages = {2662--2670},
title = {{Right for the right reasons: Training differentiable models by constraining their explanations}},
year = {2017}
}
