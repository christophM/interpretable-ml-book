<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.2 Pixel Attribution (Saliency Maps) | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="7.2 Pixel Attribution (Saliency Maps) | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.2 Pixel Attribution (Saliency Maps) | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2021-08-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="cnn-features.html"/>
<link rel="next" href="detecting-concepts.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="preface-by-the-author.html"><a href="preface-by-the-author.html"><i class="fa fa-check"></i>Preface by the Author</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> Story Time</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#lightning-never-strikes-twice"><i class="fa fa-check"></i>Lightning Never Strikes Twice</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#trust-fall"><i class="fa fa-check"></i>Trust Fall</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#fermis-paperclips"><i class="fa fa-check"></i>Fermi’s Paperclips</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>1.2</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> Interpretability</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> Importance of Interpretability</a></li>
<li class="chapter" data-level="2.2" data-path="taxonomy-of-interpretability-methods.html"><a href="taxonomy-of-interpretability-methods.html"><i class="fa fa-check"></i><b>2.2</b> Taxonomy of Interpretability Methods</a></li>
<li class="chapter" data-level="2.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>2.3</b> Scope of Interpretability</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.3.1</b> Algorithm Transparency</a></li>
<li class="chapter" data-level="2.3.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>2.3.2</b> Global, Holistic Model Interpretability</a></li>
<li class="chapter" data-level="2.3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>2.3.3</b> Global Model Interpretability on a Modular Level</a></li>
<li class="chapter" data-level="2.3.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>2.3.4</b> Local Interpretability for a Single Prediction</a></li>
<li class="chapter" data-level="2.3.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-predictions"><i class="fa fa-check"></i><b>2.3.5</b> Local Interpretability for a Group of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="evaluation-of-interpretability.html"><a href="evaluation-of-interpretability.html"><i class="fa fa-check"></i><b>2.4</b> Evaluation of Interpretability</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> Properties of Explanations</a></li>
<li class="chapter" data-level="2.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.6</b> Human-friendly Explanations</a><ul>
<li class="chapter" data-level="2.6.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>2.6.1</b> What Is an Explanation?</a></li>
<li class="chapter" data-level="2.6.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>2.6.2</b> What Is a Good Explanation?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Bike Rentals (Regression)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube Spam Comments (Text Classification)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> Risk Factors for Cervical Cancer (Classification)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> Interpretable Models</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>4.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#example"><i class="fa fa-check"></i><b>4.1.2</b> Example</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#visual-interpretation"><i class="fa fa-check"></i><b>4.1.3</b> Visual Interpretation</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#explain-individual-predictions"><i class="fa fa-check"></i><b>4.1.4</b> Explain Individual Predictions</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>4.1.5</b> Encoding of Categorical Features</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>4.1.6</b> Do Linear Models Create Good Explanations?</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.7</b> Sparse Linear Models</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#advantages"><i class="fa fa-check"></i><b>4.1.8</b> Advantages</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#disadvantages"><i class="fa fa-check"></i><b>4.1.9</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#what-is-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>4.2.1</b> What is Wrong with Linear Regression for Classification?</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic.html"><a href="logistic.html#theory"><i class="fa fa-check"></i><b>4.2.2</b> Theory</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>4.2.3</b> Interpretation</a></li>
<li class="chapter" data-level="4.2.4" data-path="logistic.html"><a href="logistic.html#example-1"><i class="fa fa-check"></i><b>4.2.4</b> Example</a></li>
<li class="chapter" data-level="4.2.5" data-path="logistic.html"><a href="logistic.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>4.2.5</b> Advantages and Disadvantages</a></li>
<li class="chapter" data-level="4.2.6" data-path="logistic.html"><a href="logistic.html#software"><i class="fa fa-check"></i><b>4.2.6</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>4.3</b> GLM, GAM and more</a><ul>
<li class="chapter" data-level="4.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>4.3.1</b> Non-Gaussian Outcomes - GLMs</a></li>
<li class="chapter" data-level="4.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>4.3.2</b> Interactions</a></li>
<li class="chapter" data-level="4.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>4.3.3</b> Nonlinear Effects - GAMs</a></li>
<li class="chapter" data-level="4.3.4" data-path="extend-lm.html"><a href="extend-lm.html#advantages-1"><i class="fa fa-check"></i><b>4.3.4</b> Advantages</a></li>
<li class="chapter" data-level="4.3.5" data-path="extend-lm.html"><a href="extend-lm.html#disadvantages-1"><i class="fa fa-check"></i><b>4.3.5</b> Disadvantages</a></li>
<li class="chapter" data-level="4.3.6" data-path="extend-lm.html"><a href="extend-lm.html#software-1"><i class="fa fa-check"></i><b>4.3.6</b> Software</a></li>
<li class="chapter" data-level="4.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>4.3.7</b> Further Extensions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.4</b> Decision Tree</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>4.4.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree.html"><a href="tree.html#example-2"><i class="fa fa-check"></i><b>4.4.2</b> Example</a></li>
<li class="chapter" data-level="4.4.3" data-path="tree.html"><a href="tree.html#advantages-2"><i class="fa fa-check"></i><b>4.4.3</b> Advantages</a></li>
<li class="chapter" data-level="4.4.4" data-path="tree.html"><a href="tree.html#disadvantages-2"><i class="fa fa-check"></i><b>4.4.4</b> Disadvantages</a></li>
<li class="chapter" data-level="4.4.5" data-path="tree.html"><a href="tree.html#software-2"><i class="fa fa-check"></i><b>4.4.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>4.5</b> Decision Rules</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>4.5.1</b> Learn Rules from a Single Feature (OneR)</a></li>
<li class="chapter" data-level="4.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>4.5.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="4.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>4.5.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="4.5.4" data-path="rules.html"><a href="rules.html#advantages-3"><i class="fa fa-check"></i><b>4.5.4</b> Advantages</a></li>
<li class="chapter" data-level="4.5.5" data-path="rules.html"><a href="rules.html#disadvantages-3"><i class="fa fa-check"></i><b>4.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="4.5.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>4.5.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.6</b> RuleFit</a><ul>
<li class="chapter" data-level="4.6.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>4.6.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="4.6.2" data-path="rulefit.html"><a href="rulefit.html#theory-1"><i class="fa fa-check"></i><b>4.6.2</b> Theory</a></li>
<li class="chapter" data-level="4.6.3" data-path="rulefit.html"><a href="rulefit.html#advantages-4"><i class="fa fa-check"></i><b>4.6.3</b> Advantages</a></li>
<li class="chapter" data-level="4.6.4" data-path="rulefit.html"><a href="rulefit.html#disadvantages-4"><i class="fa fa-check"></i><b>4.6.4</b> Disadvantages</a></li>
<li class="chapter" data-level="4.6.5" data-path="rulefit.html"><a href="rulefit.html#software-and-alternative"><i class="fa fa-check"></i><b>4.6.5</b> Software and Alternative</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>4.7</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="4.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>4.7.1</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="4.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>4.7.2</b> K-Nearest Neighbors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>5.1</b> Functional Decompositon</a><ul>
<li class="chapter" data-level="5.1.1" data-path="decomposition.html"><a href="decomposition.html#deeper-into-the-mathematics"><i class="fa fa-check"></i><b>5.1.1</b> Deeper Into the Mathematics</a></li>
<li class="chapter" data-level="5.1.2" data-path="decomposition.html"><a href="decomposition.html#how-to-compute-the-components"><i class="fa fa-check"></i><b>5.1.2</b> How to Compute the Components</a></li>
<li class="chapter" data-level="5.1.3" data-path="decomposition.html"><a href="decomposition.html#statistical-regression-models"><i class="fa fa-check"></i><b>5.1.3</b> Statistical Regression Models</a></li>
<li class="chapter" data-level="5.1.4" data-path="decomposition.html"><a href="decomposition.html#functional-anova"><i class="fa fa-check"></i><b>5.1.4</b> Functional ANOVA</a></li>
<li class="chapter" data-level="5.1.5" data-path="decomposition.html"><a href="decomposition.html#generalized-functional-anova-for-dependent-features"><i class="fa fa-check"></i><b>5.1.5</b> Generalized functional ANOVA for dependent features</a></li>
<li class="chapter" data-level="5.1.6" data-path="decomposition.html"><a href="decomposition.html#viewing-other-methods-through-the-lens-of-decomposition"><i class="fa fa-check"></i><b>5.1.6</b> Viewing Other Methods Through the Lens of Decomposition</a></li>
<li class="chapter" data-level="5.1.7" data-path="decomposition.html"><a href="decomposition.html#advantages-5"><i class="fa fa-check"></i><b>5.1.7</b> Advantages</a></li>
<li class="chapter" data-level="5.1.8" data-path="decomposition.html"><a href="decomposition.html#disadvantages-5"><i class="fa fa-check"></i><b>5.1.8</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.2</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>5.2.1</b> Examples</a></li>
<li class="chapter" data-level="5.2.2" data-path="pdp.html"><a href="pdp.html#advantages-6"><i class="fa fa-check"></i><b>5.2.2</b> Advantages</a></li>
<li class="chapter" data-level="5.2.3" data-path="pdp.html"><a href="pdp.html#disadvantages-6"><i class="fa fa-check"></i><b>5.2.3</b> Disadvantages</a></li>
<li class="chapter" data-level="5.2.4" data-path="pdp.html"><a href="pdp.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>5.2.4</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.3</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ice.html"><a href="ice.html#examples-1"><i class="fa fa-check"></i><b>5.3.1</b> Examples</a></li>
<li class="chapter" data-level="5.3.2" data-path="ice.html"><a href="ice.html#advantages-7"><i class="fa fa-check"></i><b>5.3.2</b> Advantages</a></li>
<li class="chapter" data-level="5.3.3" data-path="ice.html"><a href="ice.html#disadvantages-7"><i class="fa fa-check"></i><b>5.3.3</b> Disadvantages</a></li>
<li class="chapter" data-level="5.3.4" data-path="ice.html"><a href="ice.html#software-and-alternatives-2"><i class="fa fa-check"></i><b>5.3.4</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5.4</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="5.4.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>5.4.1</b> Motivation and Intuition</a></li>
<li class="chapter" data-level="5.4.2" data-path="ale.html"><a href="ale.html#theory-2"><i class="fa fa-check"></i><b>5.4.2</b> Theory</a></li>
<li class="chapter" data-level="5.4.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>5.4.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4.4" data-path="ale.html"><a href="ale.html#examples-2"><i class="fa fa-check"></i><b>5.4.4</b> Examples</a></li>
<li class="chapter" data-level="5.4.5" data-path="ale.html"><a href="ale.html#advantages-8"><i class="fa fa-check"></i><b>5.4.5</b> Advantages</a></li>
<li class="chapter" data-level="5.4.6" data-path="ale.html"><a href="ale.html#disadvantages-8"><i class="fa fa-check"></i><b>5.4.6</b> Disadvantages</a></li>
<li class="chapter" data-level="5.4.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>5.4.7</b> Implementation and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.5</b> Feature Interaction</a><ul>
<li class="chapter" data-level="5.5.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>5.5.1</b> Feature Interaction?</a></li>
<li class="chapter" data-level="5.5.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>5.5.2</b> Theory: Friedman’s H-statistic</a></li>
<li class="chapter" data-level="5.5.3" data-path="interaction.html"><a href="interaction.html#examples-3"><i class="fa fa-check"></i><b>5.5.3</b> Examples</a></li>
<li class="chapter" data-level="5.5.4" data-path="interaction.html"><a href="interaction.html#advantages-9"><i class="fa fa-check"></i><b>5.5.4</b> Advantages</a></li>
<li class="chapter" data-level="5.5.5" data-path="interaction.html"><a href="interaction.html#disadvantages-9"><i class="fa fa-check"></i><b>5.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>5.5.6</b> Implementations</a></li>
<li class="chapter" data-level="5.5.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>5.5.7</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.6</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="5.6.1" data-path="feature-importance.html"><a href="feature-importance.html#theory-3"><i class="fa fa-check"></i><b>5.6.1</b> Theory</a></li>
<li class="chapter" data-level="5.6.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>5.6.2</b> Should I Compute Importance on Training or Test Data?</a></li>
<li class="chapter" data-level="5.6.3" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>5.6.3</b> Example and Interpretation</a></li>
<li class="chapter" data-level="5.6.4" data-path="feature-importance.html"><a href="feature-importance.html#advantages-10"><i class="fa fa-check"></i><b>5.6.4</b> Advantages</a></li>
<li class="chapter" data-level="5.6.5" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-10"><i class="fa fa-check"></i><b>5.6.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.6.6" data-path="feature-importance.html"><a href="feature-importance.html#software-and-alternatives-3"><i class="fa fa-check"></i><b>5.6.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.7</b> Global Surrogate</a><ul>
<li class="chapter" data-level="5.7.1" data-path="global.html"><a href="global.html#theory-4"><i class="fa fa-check"></i><b>5.7.1</b> Theory</a></li>
<li class="chapter" data-level="5.7.2" data-path="global.html"><a href="global.html#example-4"><i class="fa fa-check"></i><b>5.7.2</b> Example</a></li>
<li class="chapter" data-level="5.7.3" data-path="global.html"><a href="global.html#advantages-11"><i class="fa fa-check"></i><b>5.7.3</b> Advantages</a></li>
<li class="chapter" data-level="5.7.4" data-path="global.html"><a href="global.html#disadvantages-11"><i class="fa fa-check"></i><b>5.7.4</b> Disadvantages</a></li>
<li class="chapter" data-level="5.7.5" data-path="global.html"><a href="global.html#software-3"><i class="fa fa-check"></i><b>5.7.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.8</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="5.8.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>5.8.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="5.8.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>5.8.2</b> LIME for Text</a></li>
<li class="chapter" data-level="5.8.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.8.3</b> LIME for Images</a></li>
<li class="chapter" data-level="5.8.4" data-path="lime.html"><a href="lime.html#advantages-12"><i class="fa fa-check"></i><b>5.8.4</b> Advantages</a></li>
<li class="chapter" data-level="5.8.5" data-path="lime.html"><a href="lime.html#disadvantages-12"><i class="fa fa-check"></i><b>5.8.5</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>5.9</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="5.9.1" data-path="anchors.html"><a href="anchors.html#finding-anchors"><i class="fa fa-check"></i><b>5.9.1</b> Finding Anchors</a></li>
<li class="chapter" data-level="5.9.2" data-path="anchors.html"><a href="anchors.html#complexity-and-runtime"><i class="fa fa-check"></i><b>5.9.2</b> Complexity and Runtime</a></li>
<li class="chapter" data-level="5.9.3" data-path="anchors.html"><a href="anchors.html#tabular-data-example"><i class="fa fa-check"></i><b>5.9.3</b> Tabular Data Example</a></li>
<li class="chapter" data-level="5.9.4" data-path="anchors.html"><a href="anchors.html#advantages-13"><i class="fa fa-check"></i><b>5.9.4</b> Advantages</a></li>
<li class="chapter" data-level="5.9.5" data-path="anchors.html"><a href="anchors.html#disadvantages-13"><i class="fa fa-check"></i><b>5.9.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.9.6" data-path="anchors.html"><a href="anchors.html#software-and-alternatives-4"><i class="fa fa-check"></i><b>5.9.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.10</b> Shapley Values</a><ul>
<li class="chapter" data-level="5.10.1" data-path="shapley.html"><a href="shapley.html#general-idea"><i class="fa fa-check"></i><b>5.10.1</b> General Idea</a></li>
<li class="chapter" data-level="5.10.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>5.10.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="5.10.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>5.10.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="5.10.4" data-path="shapley.html"><a href="shapley.html#advantages-14"><i class="fa fa-check"></i><b>5.10.4</b> Advantages</a></li>
<li class="chapter" data-level="5.10.5" data-path="shapley.html"><a href="shapley.html#disadvantages-14"><i class="fa fa-check"></i><b>5.10.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.10.6" data-path="shapley.html"><a href="shapley.html#software-and-alternatives-5"><i class="fa fa-check"></i><b>5.10.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>5.11</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="5.11.1" data-path="shap.html"><a href="shap.html#definition"><i class="fa fa-check"></i><b>5.11.1</b> Definition</a></li>
<li class="chapter" data-level="5.11.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>5.11.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="5.11.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>5.11.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="5.11.4" data-path="shap.html"><a href="shap.html#examples-4"><i class="fa fa-check"></i><b>5.11.4</b> Examples</a></li>
<li class="chapter" data-level="5.11.5" data-path="shap.html"><a href="shap.html#shap-feature-importance"><i class="fa fa-check"></i><b>5.11.5</b> SHAP Feature Importance</a></li>
<li class="chapter" data-level="5.11.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>5.11.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="5.11.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>5.11.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="5.11.8" data-path="shap.html"><a href="shap.html#shap-interaction-values"><i class="fa fa-check"></i><b>5.11.8</b> SHAP Interaction Values</a></li>
<li class="chapter" data-level="5.11.9" data-path="shap.html"><a href="shap.html#clustering-shap-values"><i class="fa fa-check"></i><b>5.11.9</b> Clustering SHAP values</a></li>
<li class="chapter" data-level="5.11.10" data-path="shap.html"><a href="shap.html#advantages-15"><i class="fa fa-check"></i><b>5.11.10</b> Advantages</a></li>
<li class="chapter" data-level="5.11.11" data-path="shap.html"><a href="shap.html#disadvantages-15"><i class="fa fa-check"></i><b>5.11.11</b> Disadvantages</a></li>
<li class="chapter" data-level="5.11.12" data-path="shap.html"><a href="shap.html#software-4"><i class="fa fa-check"></i><b>5.11.12</b> Software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>6</b> Example-Based Explanations</a><ul>
<li class="chapter" data-level="6.1" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>6.1</b> Counterfactual Explanations</a><ul>
<li class="chapter" data-level="6.1.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>6.1.1</b> Generating Counterfactual Explanations</a></li>
<li class="chapter" data-level="6.1.2" data-path="counterfactual.html"><a href="counterfactual.html#example-8"><i class="fa fa-check"></i><b>6.1.2</b> Example</a></li>
<li class="chapter" data-level="6.1.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-16"><i class="fa fa-check"></i><b>6.1.3</b> Advantages</a></li>
<li class="chapter" data-level="6.1.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-16"><i class="fa fa-check"></i><b>6.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="6.1.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>6.1.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>6.2</b> Adversarial Examples</a><ul>
<li class="chapter" data-level="6.2.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>6.2.1</b> Methods and Examples</a></li>
<li class="chapter" data-level="6.2.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>6.2.2</b> The Cybersecurity Perspective</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>6.3</b> Prototypes and Criticisms</a><ul>
<li class="chapter" data-level="6.3.1" data-path="proto.html"><a href="proto.html#theory-5"><i class="fa fa-check"></i><b>6.3.1</b> Theory</a></li>
<li class="chapter" data-level="6.3.2" data-path="proto.html"><a href="proto.html#examples-5"><i class="fa fa-check"></i><b>6.3.2</b> Examples</a></li>
<li class="chapter" data-level="6.3.3" data-path="proto.html"><a href="proto.html#advantages-17"><i class="fa fa-check"></i><b>6.3.3</b> Advantages</a></li>
<li class="chapter" data-level="6.3.4" data-path="proto.html"><a href="proto.html#disadvantages-17"><i class="fa fa-check"></i><b>6.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="6.3.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>6.3.5</b> Code and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>6.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="6.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>6.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="6.4.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>6.4.2</b> Influence Functions</a></li>
<li class="chapter" data-level="6.4.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>6.4.3</b> Advantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="6.4.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>6.4.4</b> Disadvantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="6.4.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-6"><i class="fa fa-check"></i><b>6.4.5</b> Software and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Network Interpretation</a><ul>
<li class="chapter" data-level="7.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>7.1</b> Learned Features</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cnn-features.html"><a href="cnn-features.html#feature-visualization"><i class="fa fa-check"></i><b>7.1.1</b> Feature Visualization</a></li>
<li class="chapter" data-level="7.1.2" data-path="cnn-features.html"><a href="cnn-features.html#network-dissection"><i class="fa fa-check"></i><b>7.1.2</b> Network Dissection</a></li>
<li class="chapter" data-level="7.1.3" data-path="cnn-features.html"><a href="cnn-features.html#advantages-18"><i class="fa fa-check"></i><b>7.1.3</b> Advantages</a></li>
<li class="chapter" data-level="7.1.4" data-path="cnn-features.html"><a href="cnn-features.html#disadvantages-18"><i class="fa fa-check"></i><b>7.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="7.1.5" data-path="cnn-features.html"><a href="cnn-features.html#software-and-further-material"><i class="fa fa-check"></i><b>7.1.5</b> Software and Further Material</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html"><i class="fa fa-check"></i><b>7.2</b> Pixel Attribution (Saliency Maps)</a><ul>
<li class="chapter" data-level="7.2.1" data-path="pixel-attribution.html"><a href="pixel-attribution.html#vanilla-gradient-saliency-maps"><i class="fa fa-check"></i><b>7.2.1</b> Vanilla Gradient (Saliency Maps)</a></li>
<li class="chapter" data-level="7.2.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html#deconvnet"><i class="fa fa-check"></i><b>7.2.2</b> DeconvNet</a></li>
<li class="chapter" data-level="7.2.3" data-path="pixel-attribution.html"><a href="pixel-attribution.html#grad-cam"><i class="fa fa-check"></i><b>7.2.3</b> Grad-CAM</a></li>
<li class="chapter" data-level="7.2.4" data-path="pixel-attribution.html"><a href="pixel-attribution.html#guided-grad-cam"><i class="fa fa-check"></i><b>7.2.4</b> Guided Grad-CAM</a></li>
<li class="chapter" data-level="7.2.5" data-path="pixel-attribution.html"><a href="pixel-attribution.html#smoothgrad"><i class="fa fa-check"></i><b>7.2.5</b> SmoothGrad</a></li>
<li class="chapter" data-level="7.2.6" data-path="pixel-attribution.html"><a href="pixel-attribution.html#examples-6"><i class="fa fa-check"></i><b>7.2.6</b> Examples</a></li>
<li class="chapter" data-level="7.2.7" data-path="pixel-attribution.html"><a href="pixel-attribution.html#advantages-19"><i class="fa fa-check"></i><b>7.2.7</b> Advantages</a></li>
<li class="chapter" data-level="7.2.8" data-path="pixel-attribution.html"><a href="pixel-attribution.html#disadvantages-19"><i class="fa fa-check"></i><b>7.2.8</b> Disadvantages</a></li>
<li class="chapter" data-level="7.2.9" data-path="pixel-attribution.html"><a href="pixel-attribution.html#software-5"><i class="fa fa-check"></i><b>7.2.9</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html"><i class="fa fa-check"></i><b>7.3</b> Detecting Concepts</a><ul>
<li class="chapter" data-level="7.3.1" data-path="detecting-concepts.html"><a href="detecting-concepts.html#tcav-testing-with-concept-activation-vectors"><i class="fa fa-check"></i><b>7.3.1</b> TCAV: Testing with Concept Activation Vectors</a></li>
<li class="chapter" data-level="7.3.2" data-path="detecting-concepts.html"><a href="detecting-concepts.html#example-9"><i class="fa fa-check"></i><b>7.3.2</b> Example</a></li>
<li class="chapter" data-level="7.3.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html#advantages-20"><i class="fa fa-check"></i><b>7.3.3</b> Advantages</a></li>
<li class="chapter" data-level="7.3.4" data-path="detecting-concepts.html"><a href="detecting-concepts.html#disadvantages-20"><i class="fa fa-check"></i><b>7.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="7.3.5" data-path="detecting-concepts.html"><a href="detecting-concepts.html#bonus-other-concept-based-approaches"><i class="fa fa-check"></i><b>7.3.5</b> Bonus: Other Concept-based Approaches</a></li>
<li class="chapter" data-level="7.3.6" data-path="detecting-concepts.html"><a href="detecting-concepts.html#software-6"><i class="fa fa-check"></i><b>7.3.6</b> Software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> A Look into the Crystal Ball</a><ul>
<li class="chapter" data-level="8.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>8.1</b> The Future of Machine Learning</a></li>
<li class="chapter" data-level="8.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>8.2</b> The Future of Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> Contribute to the Book</a></li>
<li class="chapter" data-level="10" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>10</b> Citing this Book</a></li>
<li class="chapter" data-level="11" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>11</b> Translations</a></li>
<li class="chapter" data-level="12" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>12</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used.html"><a href="r-packages-used.html"><i class="fa fa-check"></i>R Packages Used</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pixel-attribution" class="section level2">
<h2><span class="header-section-number">7.2</span> Pixel Attribution (Saliency Maps)</h2>
<p><em>This chapter is currently only available in this web version. ebook and print will follow.</em></p>
<!-- short summary -->
<p>Pixel attribution methods highlight the pixels that were relevant for a certain image classification by a neural network.
The following image is an example of an explanation:</p>
<p><img src="images/vanilla.png" width="80%" /></p>
<!-- Other names -->
<p>You will see later in this chapter what is going on in this particular image.
Pixel attribution methods can be found under various names: sensitivity map, saliency map, pixel attribution map, gradient-based attribution methods, feature relevance, feature attribution, and feature contribution.</p>
<!-- General idea and distinction -->
<p>Pixel attribution is a special case of feature attribution, but for images.
Feature attribution explains individual predictions by attributing each input feature according to how much it changed the prediction (negatively or positively).
The features can be input pixels, tabular data or words.
<a href="shap.html#shap">SHAP</a>, <a href="shapley.html#shapley">Shapley Values</a> and <a href="lime.html#lime">LIME</a> are examples of general feature attribution methods.</p>
<p>We consider neural networks that output as prediction a vector of length <span class="math inline">\(C\)</span>, which includes regression where <span class="math inline">\(C=1\)</span>.
The output of the neural network for image I is called <span class="math inline">\(S(I)=[S_1(I),\ldots,S_C(I)]\)</span>.
All these methods take as input <span class="math inline">\(x\in\mathbb{R}^p\)</span> (can be image pixels, tabular data, words, …) with p features and output as explanation a relevance value for each of the p input features: <span class="math inline">\(R^c=[R_1^c,\ldots,R_p^c]\)</span>.
The c indicates the relevance for the c-th output <span class="math inline">\(S_C(I)\)</span>.</p>
<!-- Distinction -->
<p>There is a confusing amount of pixel attribution approaches.
It helps to understand that there are two different types of attribution methods:</p>
<p><strong>Occlusion- or perturbation-based</strong>: Methods like <a href="#shap%7D">SHAP</a> and <a href="lime.html#lime">LIME</a> manipulate parts of the image to generate explanations (model-agnostic).</p>
<p><strong>Gradient-based</strong>: Many methods compute the gradient of the prediction (or classification score) with respect to the input features. The gradient-based methods (of which there are many) mostly differ in how the gradient is computed.</p>
<p>Both approaches have in common that the explanation has the same size as the input image (or at least can be meaningfully projected onto it) and they assign each pixel a value that can be interpreted as the relevance of the pixel to the prediction or classification of that image.</p>
<p>Another useful categorization for pixel attribution methods is the baseline question:</p>
<p><strong>Gradient-only methods</strong> tell us whether a change in a pixel would change the prediction. Examples are Vanilla Gradient and Grad-CAM.
The interpretation of the gradient-only attribution is:
If I were to change this pixel, the predicted class probability would go up (for positive gradient) or down (for negative gradient).
The larger the absolute value of the gradient, the stronger the effect of a change at this pixel.</p>
<p><strong>Path-attribution methods</strong> compare the current image to a reference image, which can be an artificial “zero” image such as a completely grey image.
The difference in actual and baseline prediction is divided among the pixels.
The baseline image can also be multiple images: a distribution of images.
This category includes model-specific gradient-based methods such as Deep Taylor and Integrated Gradients, as well as model-agnostic methods such as LIME and SHAP.
Some path attribution methods are “complete”, meaning that the sum of the relevance values for all input features is the difference between the prediction of the image minus the prediction of a reference image.
Examples are SHAP and Integrated Gradients.
For path-attribution methods, the interpretation is always done with respect to the baseline:
The difference between classification scores of the actual image and the baseline image are attributed to the pixels.
The choice of the reference image (distribution) has a big effect on the explanation.
The usual assumption is to use a “neutral” image (distribution).
Of course, it is perfectly possible to use your favorite selfie, but you should ask yourself if it makes sense in an application.
It would certainly assert dominance among the other project members.</p>
<p>Add this point I would normally give an intuition about how these methods work, but I think it is best if we just start with the Vanilla Gradient method, because it shows very nicely the general recipe that many other methods follow.</p>
<div id="vanilla-gradient-saliency-maps" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Vanilla Gradient (Saliency Maps)</h3>
<p>The idea of Vanilla Gradient, introduced by <a href="#fn83" class="footnote-ref" id="fnref83"><sup>83</sup></a> as one of the first pixel attribution approaches is quite simple if you already know backpropagation.
(They called their approach “Image-Specific Class Saliency”, but I like Vanilla Gradient better).
We calculate the gradient of the loss function for the class we are interested in with respect to the input pixels.
This gives us a map of the size of the input features with negative to positive values.</p>
<p>The recipe for this approach is:</p>
<ol style="list-style-type: decimal">
<li>Perform a forward pass of the image of interest.</li>
<li>Compute the gradient of class score of interest with respect to the input pixels:
<span class="math display">\[E_{grad}(I_0)=\frac{\delta{}S_c}{\delta{}I}|_{I=I_0}\]</span>
Here we set all other classes to zero.</li>
<li>Visualize the gradients. You can either show the absolute values or highlight negative and positive contributions separately.</li>
</ol>
<p>More formally, we have an image I and the convolutional neural network gives it a score <span class="math inline">\(S_c(I)\)</span> for class c.
The score is a highly non-linear function of our image.
The idea behind using the gradient is that we can approximate that score by applying a first-order Taylor expansion</p>
<p><span class="math display">\[S_c(I)\approx{}w^T{}I+b\]</span></p>
<p>where w is the derivate of our score:</p>
<p><span class="math display">\[w = \frac{\delta S_C}{\delta I}|_{I_0}\]</span></p>
<p>Now, there is some ambiguity how to perform a backward pass of the gradients, since non-linear units such as ReLU (Rectifying Linear Unit) “remove” the sign.
So when we do a backpass, we do not know whether to assign a positive or negative activation.
Using my incredible ASCII art skill, the ReLU function looks like this: _/ and is defined as <span class="math inline">\(X_{n+1}(x)=max(0,X_n)\)</span> from layer <span class="math inline">\(X_n\)</span> to layer <span class="math inline">\(X_{n-1}\)</span>.
This means that when the activation of a neuron is 0, we do not know which value to backpropagate.
In the case of Vanilla Gradient, the ambiguity is resolved as follows:</p>
<p><span class="math display">\[\frac{\delta f}{\delta X_n} = \frac{\delta f}{\delta X_{n+1}} \cdot \mathbf{I}(X_n &gt; 0)\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{I}\)</span> is the element-wise indicator function, which is 0 where the activation at the lower layer was negative, and 1 where it is positive or zero.
Vanilla Gradient takes the gradient we have back-propagated so far up to layer n+1, and then simply sets the gradients to zero where the activation at the layer below is negative.</p>
<p>Let us look at an example where we have layers <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X_{n+1}=ReLU(X_{n+1})\)</span>.
Our fictive activation at <span class="math inline">\(X_n\)</span> is:</p>
<p><span class="math display">\[
\begin{pmatrix}
1 &amp; 0 \\
-1 &amp; -10 \\
\end{pmatrix}
\]</span></p>
<p>And these are our gradients at <span class="math inline">\(X_{(n+1)}\)</span>:</p>
<p><span class="math display">\[
\begin{pmatrix}
0.4 &amp; 1.1 \\
-0.5 &amp; -0.1  \\
\end{pmatrix}
\]</span></p>
<p>Then our gradients at <span class="math inline">\(X_n\)</span> are:</p>
<p><span class="math display">\[
\begin{pmatrix}
0.4 &amp; 0 \\
 0 &amp; 0  \\
\end{pmatrix}
\]</span></p>
<div id="problems-with-vanilla-gradient" class="section level4">
<h4><span class="header-section-number">7.2.1.1</span> Problems with Vanilla Gradient</h4>
<p>Vanilla Gradient has a saturation problem (explained in Avanti et. al, 2017 <a href="#fn84" class="footnote-ref" id="fnref84"><sup>84</sup></a>):
When ReLU is used, and when the activation goes below zero, then the activation is capped at zero and does not change any more.
The activation is saturated.
For example: The input to the layer is two neurons with weights <span class="math inline">\(-1\)</span> and <span class="math inline">\(-1\)</span> and a bias of <span class="math inline">\(1\)</span>.
When passing through the ReLU layer, the activation will be neuron1 + neuron2 if the sum of both neurons is less than 1.
If the sum of both is greater than 1, the activation will remain saturated at an activation of 1.
Also the gradient at this point will be zero, and Vanilla Gradient will say that this neuron is not important.</p>
<p>And now, my dear readers, learn another method, more or less for free: DeconvNet</p>
</div>
</div>
<div id="deconvnet" class="section level3">
<h3><span class="header-section-number">7.2.2</span> DeconvNet</h3>
<p>DeconvNet by Zeiler an Fergus (2014) <a href="#fn85" class="footnote-ref" id="fnref85"><sup>85</sup></a> is almost identical to Vanilla Gradient.
The goal of DeconvNet is to reverse a neural network and the paper proposes operations that are reversals of the filtering, pooling and activation layers.
If you take a look into the paper, it looks very different to Vanilla Gradient, but apart from the reversal of the ReLU layer, DeconvNet is equivalent to the Vanilla Gradient approach.
Vanilla Gradient can be seen as a generalization of DeconvNet.
DeconvNet makes a different choice for backpropagating the gradient through ReLU:</p>
<p><span class="math inline">\(R_n=R_{n+1}\mathbb{I}(R_{n+1}&gt;0)\)</span>$,</p>
<p>where <span class="math inline">\(R_n\)</span> and <span class="math inline">\(R_{n+1}\)</span> are the layer reconstructions.
When backpassing from layer n to layer n-1, DeconvNet “remembers” which of the activations in layer n were set to zero in the forward pass and sets them to 0 in layer n-1.
Activations with a negative value in layer x are set to zero in layer n-1.
The gradient <span class="math inline">\(X_n\)</span> for the example from earlier becomes:</p>
<p><span class="math display">\[
\begin{pmatrix}
0.4 &amp; 1.1 \\
0 &amp; 0  \\
\end{pmatrix}
\]</span></p>
</div>
<div id="grad-cam" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Grad-CAM</h3>
<p>Grad-CAM provides visual explanations for CNN decisions.
Unlike other methods, the gradient is not backpropagated all the way back to the image, but (usually) to the last convolutional layer to produce a coarse localization map that highlights important regions of the image.</p>
<p>Grad-CAM stands for Gradient-weighted Class Activation Map.
And, as the name suggests, it is based on the gradient of the neural networks.
Grad-CAM, like other techniques, assigns each neuron a relevance score for the decision of interest.
This decision of interest can be the class prediction (which we find in the output layer), but can theoretically be any other layer in the neural network.
Grad-CAM backpropagates this information to the last convolutional layer.
Grad-CAM can be used with different CNNs: with fully-connected layers, for structured output such as captioning and in multi-task outputs, and for reinforcement learning.</p>
<!-- An intuitive explanation -->
<p>Let us start with an intuitive consideration of Grad-CAM.
The goal of Grad-CAM is to understand at which parts of an image a convolutional layer “looks” for a certain classification.
As a reminder, the first convolutional layer of a CNN takes as input the images and outputs feature maps that encode learned features (see the chapter on <a href="cnn-features.html#cnn-features">Learned Features</a>).
The higher-level convolutional layers do the same, but take as input the feature maps of the previous convolutional layers.
To understand how the CNN makes decisions, Grad-CAM analyzes which regions are activated in the feature maps of the last convolutional layers.
There are k feature maps in the last convolutional layer, and I will call them <span class="math inline">\(A_1, A_2, \ldots, A_k\)</span>.
How can we “see” from the feature maps how the convolutional neural network has made a certain classification?
In the first approach, we could simply visualize the raw values of each feature map, average over the feature maps and overlay this over our image.
This would not be helpful since the feature maps encode information for <strong>all classes</strong>, but we are interested in a particular class.
Grad-CAM has to decide how important each of the k feature map was to our class c that we are interested in.
We have to weight each pixel of each feature map with the gradient before we average over the feature maps.
This give us a heatmap which highlights regions that positively or negatively affect the class of interest.
This heatmap is send through the ReLU function, which is a fancy way of saying that we set all negative values to zero.
Grad-CAM removes all negative values by using a ReLU function, with the argument that we are only interested in the parts that contribute to the selected class c and not to other classes.
The word pixel might be misleading here as the feature map is smaller than the image (because of the pooling units) but is mapped back to the original image.
We then scale the Grad-CAM map to the interval [0,1] for visualization purposes and overlay it over the original image.</p>
<!-- Pseudo code-->
<p>Let us look at the recipe for Grad-CAM
Our goal is to find the localization map, which is defined as:</p>
<p><span class="math display">\[L^c_{Grad-CAM} \in \mathbb{R}^{u\times v} = \underbrace{ReLU}_{\text{Pick positive values}}\left(\sum_{k} \alpha_k^c A^k\right)\]</span></p>
<p>Here, u is the width, v the height of the explanation and c the class of interest.</p>
<ol style="list-style-type: decimal">
<li>Forward propagate the input image through the convolutional neural network.</li>
<li>Obtain the raw score for the class of interest, meaning the activation of the neuron before the softmax layer.</li>
<li>Set all other class activations to zero.</li>
<li>Backpropagate the gradient of the class of interest to the last convolutional layer before the fully connected layers: <span class="math inline">\(\frac{\delta{}y^c}{\delta{}A^k}\)</span>.</li>
<li>Weight each feature map “pixel” by the gradient for the class. Indices i and j refer to the width and height dimensions:
<span class="math display">\[\alpha_k^c = \overbrace{\frac{1}{Z}\sum_{i}\sum_{j}}^{\text{global average pooling}} \underbrace{\frac{\delta y^c}{\delta A_{ij}^k}}_{\text{gradients via backprop}}\]</span>
This means that the gradients are globally pooled.</li>
<li>Calculate an average of the feature maps, weighted per pixel by the gradient.</li>
<li>Apply ReLU to the averaged feature map.</li>
<li>For visualization: Scale values to the interval between 0 and 1. Upscale the image and overlay it over the original image.</li>
<li>Additional step for Guided Grad-CAM: Multiply heatmap with guided backpropagation.</li>
</ol>
</div>
<div id="guided-grad-cam" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Guided Grad-CAM</h3>
<p>From the description of Grad-CAM you can guess that the localization is very coarse since the last convolutional feature maps have a much coarser resolution compared to the input image.
In contrast, other attribution techniques backpropagate all the way to the input pixels.
They are therefore much more detailed and can show you individual edges or spots that contributed most to a prediction.
A fusion of both methods is called Guided Grad-CAM.
And it is super simple.
You compute for an image both the Grad-CAM explanation and the explanation from another attribution method, such as Vanilla Gradient.
The Grad-CAM output is then upsampled with bilinear interpolation, then both maps are multiplied element-wise.
Grad-CAM works like a lense that focuses on specific parts of the pixel-wise attribution map.</p>
</div>
<div id="smoothgrad" class="section level3">
<h3><span class="header-section-number">7.2.5</span> SmoothGrad</h3>
<p>The idea of SmoothGrad by Smilkov et. al 2017 <a href="#fn86" class="footnote-ref" id="fnref86"><sup>86</sup></a> is to make gradient-based explanations less noisy by adding noise and averaging over these artificially noisy gradients.
SmoothGrad is not an standalone explanation method, but an extension to any gradient-based explanation method.</p>
<p>SmoothGrad works in the following way:</p>
<ol style="list-style-type: decimal">
<li>Generate multiple versions of the image of interest by adding noise to it.</li>
<li>Create pixel attribution maps for all images.</li>
<li>Average the pixel attribution maps.</li>
</ol>
<p>Yes, it is that simple.
Why should this work?
The theory is that the derivative fluctuates greatly at small scales.
Neural networks have no incentive during training to keep the gradients smooth, their goal is to classify images correctly.
Averaging over multiple maps “smooths out” these fluctuations:</p>
<p><span class="math display">\[R_{sg}(x)=\frac{1}{N}\sum_{i=1}^n{}R(x+g_i),\]</span></p>
<p>Here, <span class="math inline">\(g_i\sim{}N(0,\sigma^2)\)</span> are noise vectors sampled from the Gaussian distribution.
The “ideal” noise level depends on the input image and the network.
The authors suggest a noise level of 10%-20%, which means that <span class="math inline">\(\frac{\sigma}{x_{max} - x_{min}}\)</span> should be between 0.1 and 0.2.
The limits <span class="math inline">\(x_{max}\)</span> and <span class="math inline">\(x_{min}\)</span> refer to minimum and maximum pixel values of the image.
The other parameter is the number of samples n, for which was suggested to use n = 50, since there are diminishing returns above that.</p>
</div>
<div id="examples-6" class="section level3">
<h3><span class="header-section-number">7.2.6</span> Examples</h3>
<p>Let us see some examples of what these maps look like and how the methods compare qualitatively.
The network under examination is VGG-16 (Simonyan et. al 2014 <a href="#fn87" class="footnote-ref" id="fnref87"><sup>87</sup></a>) which was trained on ImageNet and can therefore distinguish more than 20,000 classes.
For the following images we will create explanations for the class with the highest classification score.</p>
<p>These are the images and their classification by the neural network:</p>
<p><img src="images/original-images-classification.png" width="80%" /></p>
<p>The image on the left with the honorable dog guarding the Interpretable Machine Learning book was classified as “Greyhound” with a probability of 35% (seems like “Interpretable Machine Learning book” was not one of the 20k classes).
The image in the middle shows a bowl of yummy ramen soup and is correctly classified as “Soup Bowl” with a probability of 50%.
The third image shows an octopus on the ocean floor, which is incorrectly classified as “Eel” with a high probability of 70%.</p>
<p>And these are the pixel attributions that aim to explain the classification:</p>
<p><img src="images/smoothgrad.png" width="80%" /></p>
<p>Unfortunately, it is a bit of a mess.
But let us look at the individual explanations, starting with the dog.
Vanilla Gradient and Vanilla Gradient + SmoothGrad both highlight the dog, which makes sense.
But they also highlight some areas around the book, which is odd.
Grad-CAM highlights only the book area, which makes no sense at all.
And from here on, it gets a bit messier.
The Vanilla Gradient method seems to fail for both the soup bowl and the octopus (or, as the network thinks, eel).
Both images look like afterimages from looking into the sun for too long.
(Please do not look at the sun directly).
SmoothGrad helps a lot, at least the areas are more defined.
In the soup example, some of the ingredients are highlighted, such as the eggs and the meat, but also the area around the chop sticks.
In the octopus image, mostly the animal itself is highlighted.</p>
<p>For the soup bowl, Grad-CAM highlights the egg part and, for some reason, the upper part of the bowl.
The octopus explanations by Grad-CAM are even messier.</p>
<p>You can already see here the difficulties in assessing whether we trust the explanations.
As a first step, we need to consider which parts of the image contain information that is relevant to the images classification.
But then we also need to think about what the neural network might have used for the classification.
Perhaps the soup bowl was correctly classified based on the combination of eggs and chopstick, as SmoothGrad implies?
Or maybe the neural network recognized the shape of the bow plus some ingredients, as Grad-CAM suggests?
We just do not know.</p>
<p>And that is the big issue with all of these methods.
We do not have a ground truth for the explanations.
We can only, in a first step, reject explanations that obviously make no sense (and even in this step we don’t have strong confidence. The prediction process in the neural network is very complicated).</p>
</div>
<div id="advantages-19" class="section level3">
<h3><span class="header-section-number">7.2.7</span> Advantages</h3>
<p>The explanations are <strong>visual</strong> and we are quick to recognize images.
In particular, when methods only highlight important pixels, it is easy to immediately recognize the important regions of the image.</p>
<p>Gradient-based methods are usually <strong>faster to compute than model-agnostic methods</strong>.
For example, <a href="lime.html#lime">LIME</a> and <a href="shap.html#shap">SHAP</a> can also be used to explain image classifications, but are more expensive to compute.</p>
<p>There are <strong>many methods to choose from</strong>.</p>
</div>
<div id="disadvantages-19" class="section level3">
<h3><span class="header-section-number">7.2.8</span> Disadvantages</h3>
<p>As with most interpretation methods, it is <strong>difficult to know whether an explanation is correct</strong>, and a huge part of the evaluation is only qualitative (“These explanations look about right, let’s publish the paper already.”).</p>
<p>Pixel attribution methods can be very <strong>fragile</strong>.
Ghorbani et. al (2019)<a href="#fn88" class="footnote-ref" id="fnref88"><sup>88</sup></a> showed that introducing small (adversarial) perturbations to an image, that still lead to the same prediction, can lead to very different pixels being highlighted as explanations.</p>
<p>Kindermanns et. al (2019) <a href="#fn89" class="footnote-ref" id="fnref89"><sup>89</sup></a> also showed that these pixel attribution methods <strong>can be highly unreliable</strong>.
They added a constant shift to the input data, meaning they added the same pixel changes to all images.
They compared two networks, the original network and “shifted” network where the bias of the first layer is changed to adapt for the constant pixel shift.
Both networks produce the same predictions.
Also the gradient is the same for both.
But the explanations changed, which is an undesirable property.
They looked at DeepLift, Vanilla Gradient and Integrated Gradients.</p>
<p>The paper “Sanity Checks for Saliency Maps” <a href="#fn90" class="footnote-ref" id="fnref90"><sup>90</sup></a> investigated whether saliency methods are <strong>insensitive to model and data</strong>.
Insensitivity is highly undesirable because it would mean that the “explanation” is unrelated to model and data.
Methods that are insensitive to model and training data are similar to edge detectors.
Edge detectors simply highlight strong pixel color changes in images and are unrelated to a prediction model or abstract features of the image, and require not training.
The methods tested were Vanilla Gradient, Gradient x Input, Integrated Gradients, Guided Backpropagation, Guided Grad-CAM and SmoothGrad (with vanilla gradient).
Vanilla gradients and Grad-CAM passed the insensitivity check, while Guided Backpropagation and Guided GradCAM fail.
However, the sanity checks paper itself has found some criticism from Tomsett et. al (2020) <a href="#fn91" class="footnote-ref" id="fnref91"><sup>91</sup></a> with a paper called “Sanity Checks for Saliency Metrics” (of course).
They found that there is a lack of consistency for evaluation metrics (I know, it is getting pretty meta now).
So we are back to where we started … It remains difficult to evaluate the visual explanations.
This makes it very difficult for a practitioner.</p>
<p>All in all, this is a <strong>very unsatisfactory state of affairs</strong>.
We have to wait a little bit for more research on this topic.
And please, no more invention of new methods, but more scrutiny of how to evaluate these methods.</p>
</div>
<div id="software-5" class="section level3">
<h3><span class="header-section-number">7.2.9</span> Software</h3>
<p>There are several software implementations of pixel attribution methods.
For the example, I used <a href="https://pypi.org/project/tf-keras-vis/">tf-keras-vis</a>.
One of the most comprehensive libraries is <a href="https://github.com/albermax/innvestigate">iNNvestigate</a>, which implements Vanilla gradient, Smoothgrad, Deconvnet, Guided Backpropagation, PatternNet, LRP and more.
A lot of the methods are implemented in the <a href="https://github.com/marcoancona/DeepExplain">DeepExplain Toolbox</a>.</p>
<!-- References about problems -->
<!-- Toolboxes -->

</div>
</div>
<div class="footnotes">
<hr />
<ol start="83">
<li id="fn83"><p>Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. “Deep inside convolutional networks: Visualising image classification models and saliency maps.” arXiv preprint arXiv:1312.6034 (2013).<a href="pixel-attribution.html#fnref83" class="footnote-back">↩︎</a></p></li>
<li id="fn84"><p>Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. “Learning important features through propagating activation differences.” Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, (2017).<a href="pixel-attribution.html#fnref84" class="footnote-back">↩︎</a></p></li>
<li id="fn85"><p>Zeiler, Matthew D., and Rob Fergus. “Visualizing and understanding convolutional networks.” European conference on computer vision. Springer, Cham, 2014.<a href="pixel-attribution.html#fnref85" class="footnote-back">↩︎</a></p></li>
<li id="fn86"><p>Smilkov, Daniel, et al. “Smoothgrad: removing noise by adding noise.” arXiv preprint arXiv:1706.03825 (2017).<a href="pixel-attribution.html#fnref86" class="footnote-back">↩︎</a></p></li>
<li id="fn87"><p>Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).<a href="pixel-attribution.html#fnref87" class="footnote-back">↩︎</a></p></li>
<li id="fn88"><p>Ghorbani, Amirata, Abubakar Abid, and James Zou. “Interpretation of neural networks is fragile.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.<a href="pixel-attribution.html#fnref88" class="footnote-back">↩︎</a></p></li>
<li id="fn89"><p>Kindermans, Pieter-Jan, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schütt, Sven Dähne, Dumitru Erhan, and Been Kim. “The (un) reliability of saliency methods.” In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267-280. Springer, Cham, (2019).<a href="pixel-attribution.html#fnref89" class="footnote-back">↩︎</a></p></li>
<li id="fn90"><p>Adebayo, Julius, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. “Sanity checks for saliency maps.” arXiv preprint arXiv:1810.03292 (2018).<a href="pixel-attribution.html#fnref90" class="footnote-back">↩︎</a></p></li>
<li id="fn91"><p>Tomsett, Richard, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun Preece. “Sanity checks for saliency metrics.” In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, pp. 6021-6029. 2020.<a href="pixel-attribution.html#fnref91" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cnn-features.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="detecting-concepts.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/manuscript/07.2-feature-attribution.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
