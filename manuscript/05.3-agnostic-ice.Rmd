```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
```

## Individual Conditional Expectation (ICE) Plot {#ice}

The partial dependence plot for visualising the average effect of a feature is a global method, because it does not focus on specific instances, but on an overall average.
The equivalent to a PDP for local expectations is called individual conditional expectation (ICE) plot [@goldstein2015peeking].
An ICE plot visualises the dependence of the predicted response on a feature for EACH instance separately, resulting in multiple lines, one for each instance, compared to one line in partial dependence plots.
A PDP is the average of the lines of an ICE plot.
The values for a line (and one instance) can be computed by leaving all other features the same, creating variants of this instance by replacing the feature's value with values from a grid and letting the black box make the predictions with these newly created instances.
The result is a set of points for an instance with the feature value from the grid and the respective predictions.

So, what do you gain by looking at individual expectations, instead of partial dependencies?
Partial dependence plots can obfuscate a heterogeneous relationship that comes from interactions.
PDPs can show you how the average relationship between feature $x_S$ and $\hat{y}$ looks like.
This works only well in cases where the interactions between $x_S$ and the remaining $x_C$ are weak.
In case of interactions, the ICE plot will give a lot more insight.

A more formal definition:
In ICE plots, for each instance in $\{(x_{S_i}, x_{C_i})\}_{i=1}^N$ the curve $\hat{f}_S^{(i)}$ is plotted against $x_{S_i}$, while $x_{C_i}$ is kept fixed.

### Example
Let's go back to the dataset about risk factors for cervical cancer from Chapter \@ref(cervical) and see how each instance's prediction is associated with the feature 'Age'.
The model we will analyse is a RandomForest that predicts the probability of cancer for a woman given risk factors.
In the partial dependence plot from Chapter \@ref(pdp) we have seen that the cancer probability increases around the age of 50, but does it hold true for each woman in the dataset?
The ICE plot (Figure \@ref(fig:ice-cervical)) reveals that the most women's predicted probability follows the average pattern of increase at 50, but there are a few exceptions:
For the few women that have a high predicted probability at a young age, the predicted cancer probability does not change much with increasing age.

Figure \@ref(fig:ice-bike) shows an ICE plot for the bike rental prediction (the underlying prediction model is a RandomForest).
The data is described in Chapter \@ref(bike-data).
All curves seem to follow the same course, so there seem to be no obvious interactions.
That means that the PDP is already a good summary of the relationships of the displayed features and the predicted bike rentals.
```{r ice-cervical, fig.cap="Individual conditional expectation plot of cervical cancer probability by age. Each line represents the conditional expectation for one woman. Most women with a low cancer probability in younger years see an increase in predicted cancer probability, given all other feature value stay the same. Interestingly for a few women that have a high estimated cancer probability bigger than 0.4, the estimated probability does not change much with higher age."}
library("mlr")
library("ggplot2")
data(cervical)
set.seed(43)
cervical_subset_index = sample(1:nrow(cervical), size = 300)
cervical_subset = cervical[cervical_subset_index, ]
cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
mod = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', id = 'cervical-rf', predict.type = 'prob'), cervical.task)
pd1 = mlr::generatePartialDependenceData(mod, cervical_subset, 'Age', individual = TRUE)
mlr::plotPartialDependence(pd1) + my_theme() + scale_color_discrete(guide='none') + scale_y_continuous('Predicted cancer probability')
```

```{r ice-bike, fig.cap='Individual conditional expectation plot of expected bike rentals and weather conditions. The same effects as in the partial dependence plots can be observed.'}
set.seed(42)
data("bike")
bike.subset.index = sample(1:nrow(bike), size = 300)
bike.subset = bike[bike.subset.index,]
bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.randomForest', id = 'bike-rf'), bike.task)
pd.data = mlr::generatePartialDependenceData(mod.bike, bike.subset, c('temp', 'hum', 'windspeed'), individual = TRUE)
mlr::plotPartialDependence(pd.data) + my_theme()+ scale_x_continuous('',limits = c(0, NA)) + scale_y_continuous('Predicted number of bike rentals', limits = c(0, NA))
```

#### Centred ICE Plot
There is one issue with ICE plots:
It can be hard to see if the individual conditional expectation curves differ between individuals, because they start at different $\hat{f}(x)$.
An easy fix is to centre the curves at a certain point in $x_S$ and only display the difference in the predicted response.
The resulting plot is called centred ICE plot (c-ICE).
Anchoring the curves at the lower end of $x_S$ is a good choice.
The new curves are defined as:
$$\hat{f}_{cent}^{(i)} = \hat{f}_i - \mathbf{1}\hat{f}(x^{\text{*}}, x_{C_i}), $$
where $\mathbf{1}$ is a vector of 1's with the appropriate number of dimensions (usually one- or two-dimensional), $\hat{f}$ the fitted model and $x^{\text{*}}$ the anchor point.

#### Example
Taking the plot in Figure \@ref(fig:ice-cervical) and centring the lines at the youngest observed age yields Figure \@ref(fig:ice-cervical-centered).
With the centred ICE plots it is easier to compare the curves of individual instances.
This can be useful when we are not interested in seeing the absolute change of a predicted value, but rather the difference in prediction compared to a fixed point of the feature range.

```{r ice-cervical-centered, fig.cap=sprintf("Centred ICE plot for predicted cervical cancer risk probability by age. The lines are fixed to 0 at age %i and each point shows the difference to the prediction with age %i. Compared to age 18, the predictions for most instances stay the same and see an increase up to 20 percent. A few cases show the opposite behaviour: The predicted probability decreases with increasing age.", min(cervical_subset$Age), min(cervical_subset$Age))}
pd1 = mlr::generatePartialDependenceData(mod, cervical_subset, 'Age', individual = TRUE, center = list(Age=min(cervical_subset$Age)))
mlr::plotPartialDependence(pd1) + my_theme() + scale_color_discrete(guide='none') +
    scale_y_continuous('Cancer probability difference to age 18')
```

```{r ice-bike-centered, fig.cap='Centred individual conditional expectation plots of expected bike rentals by weather condition. The lines were fixed at value 0 for each feature and instance. The lines show the difference in prediction compared to the prediction with the respective feature value at 0.'}
set.seed(43)
bike.subset.index = sample(1:nrow(bike), size = 100)
bike.subset = bike[bike.subset.index,]
pd.data = mlr::generatePartialDependenceData(mod.bike, bike.subset, c('temp', 'hum', 'windspeed'), individual = TRUE, center = list(temp = min(bike$temp), hum = min(bike$hum), windspeed = min(bike$windspeed)))
mlr::plotPartialDependence(pd.data) + my_theme() + scale_y_continuous('Difference in predicted rentals to at value zero') + scale_x_continuous('')
```

#### Derivative ICE Plot
Another way to make it visually easier to spot heterogeneity is to look at the individual derivatives of $\hat{f}$ with respect to $x_S$ instead of the predicted response $\hat{f}$.
The resulting plot is called derivative ICE plot (d-ICE).
The derivatives of a function (or curve) tell you in which direction changes occur and if any occur at all.
With the derivative ICE plot it is easy to spot value ranges in a feature where the black box's predicted values change for (at least some) instances.
If there is no interaction between $x_S$ and $x_C$, then $\hat{f}$ can be expressed as:
$$\hat{f}(x) = \hat{f}(x_S, x_C) = g(x_S) + h(x_C), \text{ so that } \frac{\delta\hat{f}(x)}{\delta x_S} = g'(x_S)$$
Without interactions, the individual partial derivatives should be the same for all instances.
If they differ, it is because of interactions and it will become visible in the d-ICE plot.
In addition to displaying the individual curves for derivative $\hat{f}$, showing the standard deviation of derivative $\hat{f}$ helps to highlight regions in $x_S$ with heterogeneity in the estimated derivatives.

#### Example
As we have seen, the most changes in estimated cancer probability happen around age 45.
This is confirmed by the derivative ICE plot in Figure \@ref(fig:ice-cervical-derivative).

```{r ice-cervical-derivative, fig.cap="Derivative ICE plot of predicted cancer probability by age. Between age 14 and the early forties, a few instance see changes in prediction both upwards and downwards, but the majorities derivatives are near zero. Between age 45 and 50, most women's prediction curves have a positive derivative, indicating an increase in predicted cancer probability."}
# takes to much time ... maybe precompute and save results.
pd1 = mlr::generatePartialDependenceData(mod, cervical_subset[1:60,], 'Age', individual = TRUE, derivative = TRUE, method = 'simple')
mlr::plotPartialDependence(pd1) + my_theme() + scale_y_continuous('Derivative cancer probability')+ scale_color_discrete(guide='none')
```

Figure \@ref(fig:ice-bike-derivative) shows the derivative ICE plot for the bike rental model.

```{r ice-bike-derivative, fig.cap = 'Derivative individual conditional expectation plot of expected bike rentals and weather conditions.'}
pd.data = mlr::generatePartialDependenceData(mod.bike, bike.subset[50:60,], c('temp', 'hum', 'windspeed'), individual = TRUE, derivative = TRUE, method = 'simple')
mlr::plotPartialDependence(pd.data) + my_theme() + scale_y_continuous('Derivative of predicted bike counts')
```
