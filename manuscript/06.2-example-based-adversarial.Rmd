## Adversarial examples

<!-- TODO: -->
<!-- - Wait for permission to use images (szegedy  left)-->
<!-- - Iterate over text (heavy edit of sentences) in editor -->
<!-- - extend the theory for all papers a bit -->
<!-- - Print, read and edit -->
<!-- - Double deepl translation -->
<!-- - Spell check -->
<!-- - Last read in HTML form -->

Adversarial examples are instances with small perturbations that are designed to fool a machine learning model into giving a wrong prediction.
I recommend reading the chapter about [counterfactual explanations](#counterfactual) first, since the concepts are very similar.
Adversarial examples are counterfactual examples with a focus on fooling instead of interpreting a system.

*Keywords: adversarial examples, adversarial machine learning, counterfactuals, evasion attacks, machine learning security*


Why do we even care about adversarial examples beyond explaining single predictions?
Are they not only some curious by-products of machine learning models, without practical relevance?
The answer is a clear "No!".
Adversarial examples make machine learning systems vulnerable to attacks, like in the following scenarios.
A self-driving car crashes into another car because it doesn't recognize the stop sign and assumes the right over left rule holds.
Someone had sticked an image over the sign, which looks like a stop sign with a little dirt to humans, but was designed to look like a no parking sign to machine learning models.
A spam detector missclassifies spam as valid mail.
The spam mail was designed to resemble normal mail, but of course trying to scam the recipient at the same time.
A machine-learning powered scanner scans packages at the airport for weapons.
A knife is designed to evade the detection by making the system think it is an umbrella.
There are many adversarial examples in image recognition, but also in text (One of the frist adversarial examples hit spam filtering models in the early 2000s).

Let's have a look at some ways of creating adversarial examples.

### Examples

There are many ways of creating these adversarial examples.
Most methods minimize the distance of adversarial example and instance to be manipulated, while moving the prediction to the desired (wrong) outcome.
Some methods require access to the gradients of the model (which only works for gradient based models) and some only work with input features and predictions (which makes those methods model-agnostic).
The examples in this section focus on image classifiers using deep neural networks, because there is a lot of research going on and they are easy to fool.
Adversarial examples for images are input samples that were intentionally perturbed (some pixels) aimed to fool the model during test time.
Through the examples you will see that deep convolutional neural networks for object recognition are vulnerable to adversarial examples, that look harmles to humans.
Adversarial examples are like optical illusions to us humans.
A possible source for deep neural networks being vulnerable to adversarial examples is that data points that are close in input space can be very different in the deep embedded space (think of the embedding vectors of an image or text).



<!-- WAITING FOR PERMISSION TO USE IMAGE -->
#### Something's Wrong With My Dog

Szegedy et. al (2013)[^szegedy] in their work "Intriguing Properties of Neural Networks" used box-constrained L-BFGS optimization to find adversarial examples for deep neural networks.

```{r adversarial-ostrich, fig.cap = "Adversarial examples for AlexNet by Szegedy et. al (2013). All images shown in the left column are correctly classified. The middle column shows the (magnified) perturbation that is added to the images to produce the images in the right column, which are all (wrongly) categorized as 'Ostrich'."}
knitr::include_graphics("images/adversarial-ostrich.png")
```

These adversarial examples were produced by minimizing the following function respective to r:

$$loss(\hat{f}(x+r),l)+c\cdot|r|$$

Where x is an image (represented as vector of pixels), r are the changes to the pixels to create an adversarial image (x+r results in a new image), l is the desired outcome class and parameter c for balancing distance of images and distance between predictions.
The first term is the the distance between the predicted outcome of the adversarial example and the desired class l, the second term measures the distance between the adversarial example and the original image.
This formulation is almost identical to the loss function optimized by [counterfactuals](#counterfactual): 
There are additional constraints on r, so that the pixel values stay between 0 and 1. 
The propose to solve this with a box-constrained L-BFGS, an optimizer algorithm that works with the gradients.

#### Perturbed Panda

Another method is called gradient sign method by Goodfellow et. al (2014)[^goodfellow].
In their method they use the the gradient of the underlying model to find adversarial examples.
```{r adversarial-panda, fig.cap = "Making a Panda look like a gibbon for a neural network by Goodfellow et. al (2014). By changing each of the original panda pixels (left image) in the direction of the gradient with respect to the input pixels towards the targeted outcome 'Gibbon' (middle image, which would be classified as nematode) the authors create an adversarial example that is classified as gibbon but looks like a panda to humans."}
knitr::include_graphics("images/adversarial-panda.png")
```

Adversarial examples are created with the following formula:

$$x'=x+\epsilon\cdot{}sign(\bigtriangledown_x{}J(\theta,x,y))$$

The original point x is changed pixel-wise a little bit by $\epsilon$ times the direction of the model gradient with respect to the input pixels.
J is the loss function of the model, $\theta$ the model parameters, which remain untouched.
The gradient of the loss J is calculated with respect to the input pixels towards the desired adversarial outcome y.
From the gradient vector (which is as long as the vector of input pixels) they take the sign.
This sign of the gradient is positive if an increase of the pixel intensity increases the score for the adversarial class y and negative if decreasing the pixel intensity increases the score.
I expected that these adversarial examples were rather specific for a single image classifier, given a particular neural network architecture.
But it turns out that you can take this same adversarial examples to a different architecture trained on the same task and it is likely to be an adversarial example for this network as well.
The vulnerability comes from the linear nature of neural networks.

In this paper they also propose to use adversarial examples and train networks with them to get more robust networks.



#### A Jellyfish ... Wait... No. A BATHTUB! 

There are even attacks that flip a classification by changing a single (!) pixel of an image as shown by Su et. al (2017) [^1pixel].

```{r adversarial-1pixel, fig.cap = "A neural network trained to classify the ImageNet dataset is fooled by adversarial examples. Instead of the original class (in black letters), the network predicts the wrong class (in blue letters) by changing a single pixel (marked with red circles). Work by Su et. al (2017)."}
knitr::include_graphics("images/adversarial-1pixel.png")
```

Their method uses an evolution based approach called differential evolution.

#### Everything is a toaster!

One of my favorite examples brings the adversarial examples into physical reality:
A sticker that you can print out and stick to objects to make them look like toasters to a machine. 
Briliant work by Brown et. al (2017)[^toaster].

```{r adversarial-toaster, fig.cap = "A sticker, when added to a scence, makes a VGG16 classifier trained on ImageNet categorize an image of the scence as toaster. Work by Brown et. al (2017)."}
knitr::include_graphics("images/adversarial-toaster.png")
```

The difference to other work on adversarial examples is that the constraint is lifted that the image has to be very close to the original image.
Given the missing constraint, they use a different approach.
They create an image by replacing a part of the image completely with the patch. 
The patch is allowed to take on any shape. 
The patch is optimized over various images, with different positions on the images, sometimes shifted, sometimes bigger or smaller and rotated to make it work for many situations.
But the authors also suggest a variant, where the generated adversarial example is very similar to the original image.


#### Never bring your 3D-printed fake-rifle turtle to a gun fight.

An idea similar idea to the toaster sticker is a 3D-printed turtle that looks like a riffle, an idea from Athalye et. al (2018)[^turtle]
Yes, you read that correctly. 
A riffle.
A physical object that looks like a turtle to humans looks like a damn riffle to the computer!
The missclassification works also when the turtle-riffle is rotated and in a different location of the image.

```{r adversarial-turtle, fig.cap = "A 3D-printed turtle that is recognized as a riffle by TensorFlow’s standard pretrained InceptionV3 classifier. Work by Athalye et. al (2018)"}
knitr::include_graphics("images/adversarial-turtle.png")
```

The difficult thing that the authors achieved that they found a way to create an adversarial example in 3D for a 2D classifier, that is adversarial over all possible transformations (all the ways of turning the turtle, zooming in etc.)


### The blindfolded thief and deceiver.

Most adversarial attacks work by using gradient-based optimization on functions defined by a DNN to find adversarial examples.
It is possible to even create adversarial examples without information about internal model information and without access to the training data as shown by Papernot and colleagues (2017)[^papernot].
It's called a black box attack, because the attacker has almost no knowledge.

Imagine the scenario:
I give you access to my great machine learning model via Web-API.
From the convenience of your couch, you can send data and my service sends you the prediction for it.


How to do it:

1. Start with a few data points, which don't have to be training samples, but should be some realistic data for the classifier (if digit classifier, start with images of digits.)
1. Get predictions via API
1. Train the surrogate model. 
1. Create synthetic inputs for the model using a heuristic that tells us in which feature direction the model's output varies the most around an initial set of data points.
1. Repeat steps 2 to 4 for a pre-defined number of epochs.
1. Create adversarial examples for surrogate model using the fast gradient method or 
1. Attack original model with adversarial examples

All this has to be done without knowing the original model and also keeping the number of queries to the black box model at a minimum.
Btw. it's not the goal to get a surrogate model that is as accurate as the original model, but rather to approximate the decision boundaries of the model.

The authors test this approach by attacking a mult-class deep neural network classifier.
They used MetaMind, which trains classifiers.
For this the authors uploaded the MNIST dataset with 50,000 samples of digit images.
Then MetaMind trains a neural network to classify the digits. 
The authors had no access or information about the trained model.
After training, the model prediction function can be accessed via API.
They successfully attack image classifier offered through API by MetaMind, Amazon, and Google. 
84.24% of the adversarial examples are indeed missclassified by MetaMind.

The method even works if the black box model to be fooled is some different machine learning model and not a neural network. 
Even a decision tree, which is not even differentiable.
And for the surrogate model trained on the synthetic data, they used a logistic regression model. 
The stuff still worked.


### The Cybersecurity Perspective

This section covers mainly insights from the paper that reviews ten years of research of adversarial machine learning by  Biggio et. al (2017)[^adversarial].
Since machine learning is getting more and more integrated into all kinds of system, they also become entry points for attacks.
Especially because we don't understand the machine learning models and what they learned, they are vulnerable to attacks and often the weakest link in a system. 
Even if a machine learning model is 100% correct on a dataset, there might still be adversarial examples to fool it.
Defending against such attacks is tough:
Machine learning deals with known unkowns (classifying data uknown data points from know distribution);
But defending against machine learning model attacks deals with unknown unknowns: classifying never seen data points that are weird;
Cyber security in general is a bit of an arms-race: spammer vs. a system. 
There are three golden rules for defending against attacks: 1) know your adversary agent 2) be proactive and 3) protect yourself.
Interpretability of the models (for the builder of the system), knowing about adversarial examples helps to protect them against these examples.
To protect a system, try to proactively find holes in it. 
Make the models interpretable and find their weaknesses.
You can be proactive or reactive to attacks.
Reactive means you wait until some attack happens, analyze it and create defenses.
Proactive means you actively think about what can happens and create attacks yourself and also some defenses.

The attacker can have different levels of knowledge about the attacked system:
The attacker might have perfect knowledge (white box attack), meaning he or she knows everything about the ml model like the weights of a linear model;
The attacker might have partial knowledge (gray box attack), meaning he or she only knows maybe the feature representation and the type of model that was used, but has no access to the training data or the parameters;
The attacker might have zero knowledge (black box attack), meaning that he or she can only query the model in a black box manner but doesn't have access to the data or information about the model.
Depending on the level of information, the attacker can employ different techniques to attack the model.
As we have seen in the examples, adversarial examples can even be created in the black box attack case, so hiding information about data and model is not enough.

How can we defend our machine learning systems?
A proactive approach to defend against adversarial attacks is to iteratively retrain the classifier on simulated attacks, also called adversarial training.
Other approaches are based on game theory, like learning invariant transformations of features.
A different approach is to defend against adversarial examples through robust optimization.
Adversarial data manipulation can be seen as a special type of noise, which turns protecting against those attacks into a problem of learning models that are robust to noise.
For example, regularizing the gradients through simulation of the correpsonding attacks to improve the security of deep neural networks against adversarial examples.
Another suggested method is to use multiple classifiers instead of only one and let them vote (ensemble), but this has no guarantee to work, since they might all suffer from similar adversarial examples.
Gradient masking constructs a model without useful gradients by using a nearest neighbor classifier instead of the original model.
But this method is still vulnerable to attacks.

Other ways ml algorithms can be attacked are poisoning attacks: 
These attacks happen at training time instead of at application time.
The model is "poisoned" by giving it wrongly labeled or otherwise misleading examples to train on.
An example is Microsoft's Twitter chatbot Tay, which was designed to talk to people on Twitter. 
It was shut down after 16 hours, because it had started crafting racist and other offensive tweets. 
It had started repeating what some malicious users were feeding it.
A (not so) great examples for a poisoning attack in the wild.


Proactively creating adversarial examples as the defender can teach you a great lot about your machine learning model.
How well does it generalize? 
How easy is it fooled?
Is it robust?
Knowing the boundaries and problems of a system let's the deployer more easily imagine possible vulnerabilities. 
Interpretability of machine learning models in general can help to improve the security of such systems. 
Looking at feature importance values gives a quick overview which features influence the model prediction the most. 
ICE curves simulate some uni-variate attacks, by trying out variations of single features and measuring the output.
Analysing miss-classifications with counterfactuals, LIME and Shapley value might reveal further weaknesses of the machine learning model. 
Looking at prototypes and criticisms gives the developer a better understanding of the underlying data distribution, and especially the odd cases as well. 
And this might give an idea where problems in the model might occur for those edge cases.
Do you, as a developer of a model, trust your model out into this dangerous world, without having ever looked into it and even tried to understand it?
See how it behave in different scenarios, get a feel for the most important input, get the explanations for a few selected predictions.
Interpretability of machine learning models has a great role to play in adversarial machine learning.

As always, this sword has to edges. 
Interpretable machine learning can also be used to attack machine learning models. 
Using surrogate models, I can steal a machine learning model and experiment in my garage with it to find out weaknesses.
When I learn an interpretable surrogate model, I might also learn something about the internals.
Like this you could steal the image recognition models from all the cloud services.
You can - also conveniently via API - create counterfactual explanations, which can be used as adversarial examples.
When you understand a model, you have a chance to find entry points to cheat it. 
When you know that a credit score application uses having some special insurance (e.g. for jewelry) as feature (as a proxy that you are reliable or rich maybe), then you can simply get such an insurance to fool the system that your credit risk score is better than it really is.
Does this mean that any deployed machine learning model has to be closed source?
Not necessarily.
The same discussions have taken place and are still ongoing about closed and open source software in general.
Is open source software more vulnerable to attacks because of its openess?
Yes, because attackers can read the code and exploit it. 
No, because many other programmers with good intentions see the code and can fix issues.
Is closed source software more vulnerable to attacks because of its closedness?
Yes, because attackers can't read the code. 
Security by obfuscation.
No, because only a very limited amount of programmers will see the code and are highly likely to overlook problems that can be exploited.



Given the nature of the cat-and-mouse game, we will see a lot of research in this area.
Attack methods against machine learning will be invented, defenses will be suggested to defend against the attacks.
Then new attacks will be designed that are not detected by the implemented defenses and the circle begins again.
I hope with this chapter to sensibilize you that the problem of adversarial examples exist and that only by proactively studying the machine learning models we are able to discover and fix vulnerabilities of the models.

[^szegedy]: Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2013). Intriguing properties of neural networks, 1–10. http://doi.org/10.1021/ct2009208

[^adversarial]: Biggio, B., & Roli, F. (2017). Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning, 32–37. Retrieved from http://arxiv.org/abs/1712.03141

[^turtle]: Athalye, A., Engstrom, L., Ilyas, A., & Kwok, K. (2017). Synthesizing Robust Adversarial Examples. Retrieved from http://arxiv.org/abs/1707.07397

[^goodfellow]: Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and Harnessing Adversarial Examples, 1–11. http://doi.org/10.1109/CVPR.2015.7298594

[^1pixel]: Su, J., Vargas, D. V., & Kouichi, S. (2017). One pixel attack for fooling deep neural networks. Retrieved from http://arxiv.org/abs/1710.08864

[^toaster]: Brown, T. B., Mané, D., Roy, A., Abadi, M., & Gilmer, J. (2017). Adversarial Patch, (Nips). Retrieved from http://arxiv.org/abs/1712.09665

[^papernot]: Papernot, Nicolas, et al. "Practical black-box attacks against machine learning." Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM, 2017.