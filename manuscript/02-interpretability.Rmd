```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
```

# Interpretability {#interpretability}

Tidak ada definisi matematis dari interpretabilitas.
Definisi (non-matematis) yang saya suka dari Miller (2017)[^Miller2017] adalah:
**Interpretabilitas adalah sejauh mana manusia dapat memahami penyebab keputusan.**
Satu lagi adalah:
**Interpretabilitas adalah sejauh mana manusia dapat secara konsisten memprediksi hasil model** [^critique].
Semakin tinggi interpretabilitas model machine learning, semakin mudah bagi seseorang untuk memahami mengapa keputusan atau prediksi tertentu telah dibuat.
Suatu model dapat diinterpretasikan lebih baik daripada model lain jika keputusannya lebih mudah dipahami manusia daripada keputusan dari model lain.
Saya akan menggunakan istilah interpretable dan explainable secara bergantian.
Seperti Miller (2017), saya pikir masuk akal untuk membedakan antara istilah interpretability/explainability dan explanation.
Saya akan menggunakan "explanation" untuk penjelasan prediksi individu.
Lihat [bagian tentang penjelasan](#explanation) untuk mempelajari apa yang kita manusia lihat sebagai penjelasan yang baik.

## Importance of Interpretability {#interpretability-importance}

Jika model machine learning berkinerja baik, **mengapa kita tidak mempercayai model tersebut** dan mengabaikan **mengapa** model tersebut membuat keputusan tertentu?
"Masalahnya adalah bahwa metrik tunggal, seperti akurasi klasifikasi, adalah deskripsi yang tidak lengkap dari sebagian besar tugas dunia nyata." (Doshi-Velez dan Kim 2017 [^Doshi2017])

Mari kita selami lebih dalam alasan mengapa interpretabilitas sangat penting.
Ketika datang ke pemodelan prediktif, Anda harus melakukan trade-off:
Apakah Anda hanya ingin tahu **apa** yang diprediksi?
Misalnya, kemungkinan pelanggan akan berhenti atau seberapa efektif suatu obat bagi pasien.
Atau apakah Anda ingin tahu **mengapa** prediksi dibuat dan mungkin membayar interpretasi dengan penurunan kinerja prediktif?
Dalam beberapa kasus, Anda tidak peduli mengapa keputusan dibuat, cukup untuk mengetahui bahwa kinerja prediktif pada kumpulan data uji baik.
Namun dalam kasus lain, mengetahui 'mengapa' dapat membantu Anda mempelajari lebih lanjut tentang masalah, data, dan alasan mengapa sebuah model mungkin gagal.
Beberapa model mungkin tidak memerlukan penjelasan karena digunakan di lingkungan berisiko rendah, artinya kesalahan tidak akan memiliki konsekuensi serius, (misalnya sistem rekomendasi film) atau metode telah dipelajari dan dievaluasi secara ekstensif (misalnya optical character recognition).
Kebutuhan interpretasi muncul dari ketidaklengkapan dalam formalisasi masalah (Doshi-Velez dan Kim 2017), yang berarti bahwa untuk masalah atau tugas tertentu tidak cukup untuk mendapatkan prediksi (**what**).
Model juga harus menjelaskan bagaimana prediksi tersebut (**mengapa**), karena prediksi yang benar hanya menyelesaikan sebagian masalah awal Anda.
Alasan berikut mendorong permintaan untuk interpretasi dan penjelasan (Doshi-Velez dan Kim 2017 dan Miller 2017).

**Keingintahuan dan pembelajaran manusia**: Manusia memiliki model mental dari lingkungannya yang diperbarui ketika sesuatu yang tidak terduga terjadi.
Pembaruan ini dilakukan dengan mencari penjelasan untuk kejadian tak terduga.
Misalnya, seorang manusia tiba-tiba merasa sakit dan bertanya, "Mengapa saya merasa sangat sakit?".
Dia belajar bahwa dia sakit setiap kali dia makan buah beri merah itu.
Dia memperbarui model mentalnya dan memutuskan bahwa buah beri menyebabkan penyakit dan karenanya harus dihindari.
Ketika model machine learning buram digunakan dalam penelitian, temuan ilmiah tetap sepenuhnya tersembunyi jika model hanya memberikan prediksi tanpa penjelasan.
Untuk memfasilitasi pembelajaran dan memuaskan rasa ingin tahu tentang mengapa prediksi atau perilaku tertentu diciptakan oleh mesin, interpretasi dan penjelasan sangat penting.
Tentu saja, manusia tidak membutuhkan penjelasan atas semua yang terjadi.
Bagi kebanyakan orang tidak apa-apa jika mereka tidak mengerti cara kerja komputer.
Kejadian tak terduga membuat kita penasaran.
Misalnya: Mengapa komputer saya mati tiba-tiba?

Berkaitan erat dengan belajar adalah keinginan manusia untuk **menemukan makna di dunia**.
Kami ingin menyelaraskan kontradiksi atau inkonsistensi antara elemen struktur pengetahuan kami.
"Mengapa anjing saya menggigit saya meskipun tidak pernah melakukannya sebelumnya?" manusia mungkin bertanya.
Ada kontradiksi antara pengetahuan tentang perilaku anjing di masa lalu dan pengalaman gigitan yang baru dan tidak menyenangkan.
Penjelasan dokter hewan mendamaikan kontradiksi pemilik anjing:
"Anjing itu sedang stres dan menggigit."
Semakin keputusan mesin mempengaruhi kehidupan seseorang, semakin penting bagi mesin untuk menjelaskan perilakunya.
Jika model machine learning menolak aplikasi pinjaman, ini mungkin sama sekali tidak terduga bagi pelamar.
Mereka hanya dapat mendamaikan ketidakkonsistenan antara harapan dan kenyataan ini dengan semacam penjelasan.
Penjelasan sebenarnya tidak harus sepenuhnya menjelaskan situasi, tetapi harus membahas penyebab utama.
Contoh lain adalah rekomendasi produk algoritmik.
Secara pribadi, saya selalu memikirkan mengapa produk atau film tertentu telah direkomendasikan secara algoritmik kepada saya.
Seringkali cukup jelas:
Iklan mengikuti saya di Internet karena saya baru saja membeli mesin cuci, dan saya tahu bahwa di hari-hari berikutnya saya akan diikuti oleh iklan mesin cuci.
Ya, masuk akal untuk menyarankan sarung tangan jika saya sudah memiliki topi musim dingin di keranjang belanja saya.
Algoritme merekomendasikan film ini, karena pengguna yang menyukai film lain yang saya sukai juga menikmati film yang direkomendasikan.
Semakin banyak, perusahaan Internet menambahkan penjelasan pada rekomendasi mereka.
Contoh yang baik adalah rekomendasi produk, yang didasarkan pada kombinasi produk yang sering dibeli:

```{r amazon-recommendation, fig.cap='Recommended products that are frequently bought together.', out.width=500}
knitr::include_graphics("images/amazon-freq-bought-together.png")
```


Dalam banyak disiplin ilmu ada perubahan dari metode kualitatif ke kuantitatif (misalnya sosiologi, psikologi), dan juga menuju machine learning (biologi, genomik).
**Tujuan sains** adalah untuk mendapatkan pengetahuan, tetapi banyak masalah diselesaikan dengan kumpulan data besar dan model machine learning black box.
Model itu sendiri menjadi sumber pengetahuan, bukan data.
Interpretabilitas memungkinkan untuk mengekstrak pengetahuan tambahan yang ditangkap oleh model.

Model machine learning mengambil tugas dunia nyata yang memerlukan **langkah-langkah keamanan** dan pengujian.
Bayangkan sebuah mobil self-driving secara otomatis mendeteksi pengendara sepeda berdasarkan sistem pembelajaran yang mendalam.
Anda ingin 100% yakin bahwa abstraksi yang telah dipelajari sistem bebas dari kesalahan, karena menabrak pengendara sepeda cukup buruk.
Penjelasan mungkin mengungkapkan bahwa fitur yang paling penting dipelajari adalah mengenali dua roda sepeda, dan penjelasan ini membantu Anda memikirkan tentang kasus tepi seperti sepeda dengan tas samping yang menutupi sebagian roda.

Secara default, model machine learning mengambil bias dari data pelatihan.
Ini dapat mengubah model machine learning Anda menjadi rasis yang mendiskriminasi kelompok yang kurang terwakili.
Interpretabilitas adalah alat debug yang berguna untuk **mendeteksi bias** dalam model machine learning.
Mungkin saja model machine learning yang telah Anda latih untuk persetujuan otomatis atau penolakan aplikasi kredit mendiskriminasi minoritas yang secara historis kehilangan haknya.
Tujuan utama Anda adalah memberikan pinjaman hanya kepada orang-orang yang pada akhirnya akan membayarnya kembali.
Ketidaklengkapan rumusan masalah dalam hal ini terletak pada kenyataan bahwa Anda tidak hanya ingin meminimalkan default pinjaman, tetapi juga berkewajiban untuk tidak melakukan diskriminasi atas dasar demografi tertentu.
Ini adalah kendala tambahan yang merupakan bagian dari perumusan masalah Anda (memberikan pinjaman dengan cara yang berisiko rendah dan compliant) yang tidak tercakup oleh loss function yang dioptimalkan untuk model machine learning.

Proses mengintegrasikan mesin dan algoritme ke dalam kehidupan kita sehari-hari membutuhkan kemampuan interpretasi untuk meningkatkan **penerimaan sosial**.
Orang mengaitkan kepercayaan, keinginan, niat, dan sebagainya dengan objek.
Dalam sebuah eksperimen terkenal, Heider dan Simmel (1944) [^Heider] menunjukkan kepada para peserta video bentuk-bentuk di mana sebuah lingkaran membuka "pintu" untuk memasuki "ruangan" (yang sebenarnya hanya persegi panjang).
Para peserta menggambarkan tindakan bentuk-bentuk itu seperti mereka menggambarkan tindakan agen manusia, menetapkan niat dan bahkan emosi dan ciri-ciri kepribadian ke bentuk-bentuk itu.
Robot adalah contoh yang baik, seperti penyedot debu saya, yang saya beri nama "Doge".
Jika Doge macet, saya pikir:
"Doge ingin terus membersihkan, tetapi meminta bantuanku karena macet."
Kemudian, ketika Doge selesai membersihkan dan mencari pangkalan untuk mengisi ulang, saya pikir:
"Doge memiliki keinginan untuk mengisi ulang dan berniat untuk menemukan markas."
Saya juga mengaitkan ciri-ciri kepribadian:
"Doge agak bodoh, tapi dengan cara yang lucu."
Ini adalah pemikiran saya, terutama ketika saya mengetahui bahwa Doge telah merobohkan tanaman saat dengan patuh menyedot debu rumah.
Mesin atau algoritma yang menjelaskan prediksinya akan lebih diterima.
Lihat juga [bab tentang penjelasan](#explanation), yang menyatakan bahwa penjelasan adalah proses sosial.

Penjelasan digunakan untuk **mengelola interaksi sosial**.
Dengan menciptakan makna bersama tentang sesuatu, si penjelas memengaruhi tindakan, emosi, dan keyakinan si penerima penjelasan.
Agar mesin dapat berinteraksi dengan kita, mungkin perlu membentuk emosi dan keyakinan kita.
Mesin harus "membujuk" kita, sehingga mereka dapat mencapai tujuan yang diinginkan.
Saya tidak akan sepenuhnya menerima penyedot debu robot saya jika tidak menjelaskan perilakunya sampai tingkat tertentu.
Penyedot debu menciptakan makna bersama, misalnya, "kecelakaan" (seperti tersangkut di karpet kamar mandi ... lagi) dengan menjelaskan bahwa itu macet alih-alih berhenti bekerja tanpa komentar.
Menariknya, mungkin ada ketidakselarasan antara tujuan mesin penjelas (menciptakan kepercayaan) dan tujuan penerima (memahami prediksi atau perilaku).
Mungkin penjelasan lengkap mengapa Doge macet bisa jadi karena baterainya sangat lemah, salah satu rodanya tidak berfungsi dengan baik, dan ada bug yang membuat robot itu berulang kali ke tempat yang sama meskipun ada sebuah halangan.
Alasan-alasan ini (dan beberapa lagi) menyebabkan robot macet, tetapi itu hanya menjelaskan bahwa ada sesuatu yang menghalangi, dan itu cukup bagi saya untuk memercayai perilakunya dan mendapatkan makna bersama dari kecelakaan itu.
Ngomong-ngomong, Doge terjebak di kamar mandi lagi.
Kami harus melepas karpet setiap kali sebelum kami membiarkan Doge menyedot debu.

```{r doge-stuck, fig.cap="Doge, our vacuum cleaner, got stuck. As an explanation for the accident, Doge told us that it needs to be on an even surface.", out.width=800}
knitr::include_graphics("images/doge-stuck.jpg")
```

Model machine learning hanya dapat **di-debug dan diaudit** jika dapat ditafsirkan.
Bahkan di lingkungan berisiko rendah, seperti rekomendasi film, kemampuan untuk menafsirkan sangat berharga dalam fase penelitian dan pengembangan serta setelah penerapan.
Kemudian, ketika sebuah model digunakan dalam suatu produk, hal-hal bisa salah.
Interpretasi untuk prediksi yang salah membantu untuk memahami penyebab kesalahan.
Ini memberikan arahan tentang cara memperbaiki sistem.
Pertimbangkan contoh pengklasifikasi husky versus serigala yang salah mengklasifikasikan beberapa husky sebagai serigala.
Menggunakan metode interpretable machine learning, Anda akan menemukan bahwa kesalahan klasifikasi disebabkan oleh salju pada gambar.
Pengklasifikasi belajar menggunakan salju sebagai fitur untuk mengklasifikasikan gambar sebagai "serigala", yang mungkin masuk akal dalam hal memisahkan serigala dari husky dalam kumpulan data pelatihan, tetapi tidak dalam penggunaan di dunia nyata.

Jika Anda dapat memastikan bahwa model machine learning dapat menjelaskan keputusan, Anda juga dapat memeriksa ciri-ciri berikut dengan lebih mudah (Doshi-Velez dan Kim 2017):

- Keadilan: Memastikan bahwa prediksi tidak bias dan tidak secara implisit atau eksplisit mendiskriminasi kelompok yang kurang terwakili.
Model yang dapat ditafsirkan dapat memberi tahu Anda mengapa telah memutuskan bahwa orang tertentu tidak boleh mendapatkan pinjaman, dan menjadi lebih mudah bagi manusia untuk menilai apakah keputusan tersebut didasarkan pada bias demografis (misalnya ras) yang dipelajari.
- Privasi: Memastikan bahwa informasi sensitif dalam data dilindungi.
- Keandalan atau Kekokohan: Memastikan bahwa perubahan kecil pada input tidak menyebabkan perubahan besar dalam prediksi.
- Kausalitas: Periksa bahwa hanya hubungan sebab akibat yang diambil.
- Kepercayaan: Lebih mudah bagi manusia untuk mempercayai sistem yang menjelaskan keputusannya dibandingkan dengan black box.

**Saat kita tidak membutuhkan interpretabilitas.**

Skenario berikut mengilustrasikan ketika kita tidak membutuhkan atau bahkan tidak menginginkan interpretabilitas model machine learning.

Interpretabilitas tidak diperlukan jika model **tidak memiliki dampak signifikan**.
Bayangkan seseorang bernama Mike mengerjakan proyek sampingan machine learning untuk memprediksi ke mana teman-temannya akan pergi untuk liburan berikutnya berdasarkan data Facebook.
Mike hanya suka mengejutkan teman-temannya dengan tebak-tebakan ke mana mereka akan pergi berlibur.
Tidak ada masalah sebenarnya jika modelnya salah (paling buruk hanya sedikit memalukan bagi Mike), juga tidak ada masalah jika Mike tidak dapat menjelaskan output dari modelnya.
Tidak apa-apa untuk tidak memiliki interpretasi dalam kasus ini.
Situasi akan berubah jika Mike mulai membangun bisnis di sekitar prediksi tujuan liburan ini.
Jika modelnya salah, bisnis bisa merugi, atau modelnya mungkin bekerja lebih buruk bagi sebagian orang karena bias rasial yang dipelajari.
Segera setelah model memiliki dampak yang signifikan, baik finansial maupun sosial, interpretabilitas menjadi relevan.

Penafsiran tidak diperlukan saat **masalah dipelajari dengan baik**.
Beberapa aplikasi telah dipelajari dengan cukup baik sehingga ada pengalaman praktis yang cukup dengan model dan masalah dengan model telah dipecahkan dari waktu ke waktu.
Contoh yang baik adalah model machine learning untuk pengenalan karakter optik yang memproses gambar dari amplop dan mengekstrak alamat.
Ada tahun pengalaman dengan sistem ini dan jelas bahwa mereka bekerja.
Selain itu, kami tidak terlalu tertarik untuk mendapatkan wawasan tambahan tentang tugas yang ada.

Interpretabilitas mungkin memungkinkan orang atau program untuk **memanipulasi sistem**.
Masalah dengan pengguna yang menipu sistem hasil dari ketidaksesuaian antara tujuan pencipta dan pengguna model.
Penilaian kredit adalah sistem seperti itu karena bank ingin memastikan bahwa pinjaman hanya diberikan kepada pemohon yang kemungkinan besar akan mengembalikannya, dan pemohon bertujuan untuk mendapatkan pinjaman meskipun bank tidak mau memberikannya.
Ketidaksesuaian antara tujuan ini memperkenalkan insentif bagi pelamar untuk mempermainkan sistem guna meningkatkan peluang mereka mendapatkan pinjaman.
Jika pemohon mengetahui bahwa memiliki lebih dari dua kartu kredit berdampak negatif terhadap skornya, dia hanya mengembalikan kartu kredit ketiganya untuk meningkatkan skornya, dan mengatur kartu baru setelah pinjaman disetujui.
Sementara skornya meningkat, kemungkinan sebenarnya untuk membayar kembali pinjaman tetap tidak berubah.
Sistem hanya dapat dimainkan jika inputnya adalah proxy untuk fitur kausal, tetapi tidak benar-benar menyebabkan hasilnya.
Jika memungkinkan, fitur proxy harus dihindari karena membuat model dapat dimainkan.
Misalnya, Google mengembangkan sistem yang disebut Google Pantau Flu Dunia untuk memprediksi wabah flu.
Sistem ini menghubungkan pencarian Google dengan wabah flu -- dan kinerjanya buruk.
Distribusi permintaan pencarian berubah dan Google Pantau Flu Dunia melewatkan banyak wabah flu.
Pencarian Google tidak menyebabkan flu.
Ketika orang mencari gejala seperti "demam" itu hanya korelasi dengan wabah flu yang sebenarnya.
Idealnya, model hanya akan menggunakan fitur kausal karena tidak dapat dimainkan.

<!--{pagebreak}-->

## Taxonomy of Interpretability Methods

Metode untuk interpretasi machine learning dapat diklasifikasikan menurut berbagai kriteria.

**Intrinsik atau post hoc?**
Kriteria ini membedakan apakah interpretabilitas dicapai dengan membatasi kompleksitas model machine learning (intrinsik) atau dengan menerapkan metode yang menganalisis model setelah pelatihan (post hoc).
Interpretabilitas intrinsik mengacu pada model machine learning yang dianggap dapat ditafsirkan karena strukturnya yang sederhana, seperti short decision trees atau sparse linear models.
Interpretasi post hoc mengacu pada penerapan metode interpretasi setelah pelatihan model.
Misalnya, permutation feature importance adalah metode interpretasi post hoc.
Metode post hoc juga dapat diterapkan pada interpretable models secara intrinsik.
Misalnya, permutation feature importance dapat dihitung untuk decision trees.
Organisasi bab-bab dalam buku ini ditentukan oleh perbedaan antara [interpretable models secara intrinsik](#simple) dan [metode interpretasi post hoc (dan model-agnostic)](#agnostic).

**Hasil dari metode interpretasi**
Berbagai metode interpretasi dapat secara kasar dibedakan menurut hasilnya.

- **Statistik ringkasan fitur**:
Banyak metode interpretasi menyediakan statistik ringkasan untuk setiap fitur.
Beberapa metode mengembalikan satu nomor per fitur, seperti feature importance, atau hasil yang lebih kompleks, seperti pairwise feature interaction strengths, yang terdiri dari nomor untuk setiap pasangan fitur.
- **Visualisasi ringkasan fitur**:
Sebagian besar statistik ringkasan fitur juga dapat divisualisasikan.
Beberapa ringkasan fitur sebenarnya hanya bermakna jika divisualisasikan dan tabel akan menjadi pilihan yang salah.
Ketergantungan sebagian dari fitur adalah kasus seperti itu.
Partial dependence plot adalah kurva yang menunjukkan fitur dan hasil prediksi rata-rata.
Cara terbaik untuk menyajikan ketergantungan parsial adalah dengan benar-benar menggambar kurva daripada mencetak koordinat.
- **Internal model (mis. bobot yang dipelajari)**:
Penafsiran model yang dapat ditafsirkan secara intrinsik termasuk dalam kategori ini.
Contohnya adalah bobot dalam model linier atau struktur pohon yang dipelajari (fitur dan ambang batas yang digunakan untuk pemisahan) dari decision trees.
Garis dikaburkan antara internal model dan statistik ringkasan fitur dalam, misalnya, model linier, karena bobotnya merupakan internal model dan statistik ringkasan untuk fitur pada saat yang bersamaan.
Metode lain yang mengeluarkan model internal adalah visualisasi dari detektor fitur yang dipelajari dalam convolutional neural networks.
Metode interpretasi yang menghasilkan model internal menurut definisi model-spesifik (lihat kriteria berikutnya).
- **Titik data**:
Kategori ini mencakup semua metode yang mengembalikan titik data (sudah ada atau baru dibuat) untuk membuat model dapat diinterpretasikan.
Salah satu metode disebut counterfactual explanations.
Untuk menjelaskan prediksi data instance, metode ini menemukan titik data yang serupa dengan mengubah beberapa fitur yang hasil prediksinya berubah dengan cara yang relevan (misalnya, flip di kelas yang diprediksi).
Contoh lain adalah identifikasi prototypes kelas yang diprediksi.
Agar berguna, metode interpretasi yang menghasilkan titik data baru mengharuskan titik data itu sendiri dapat diinterpretasikan.
Ini berfungsi dengan baik untuk gambar dan teks, tetapi kurang berguna untuk data tabular dengan ratusan fitur.
- **Model yang dapat ditafsirkan secara intrinsik**:
Salah satu solusi untuk menginterpretasikan model black box adalah dengan memperkirakannya (baik secara global maupun lokal) dengan interpretable models.
Model interpretable itu sendiri diinterpretasikan dengan melihat parameter model internal atau statistik ringkasan fitur.

**Model-specific atau model-agnostic?**
Alat interpretasi model khusus terbatas pada kelas model tertentu.
Interpretasi bobot regresi dalam model linier adalah interpretasi model-spesifik, karena - menurut definisi - interpretasi model intrinsik ditafsirkan selalu model-spesifik.
Alat yang hanya berfungsi untuk interpretasi mis. jaringan saraf adalah model-spesifik.
Alat model-agnostic dapat digunakan pada model machine learning apa pun dan diterapkan setelah model dilatih (post hoc).
Metode agnostik ini biasanya bekerja dengan menganalisis pasangan input dan output fitur.
Menurut definisi, metode ini tidak dapat memiliki akses ke model internal seperti bobot atau informasi struktural.

**Lokal atau global?**
Apakah metode interpretasi menjelaskan prediksi individu atau perilaku model secara keseluruhan?
Atau apakah ruang lingkupnya ada di antaranya?
Baca lebih lanjut tentang kriteria ruang lingkup di bagian selanjutnya.

<!--{pagebreak}-->

## Scope of Interpretability
Sebuah algoritma melatih model yang menghasilkan prediksi.
Setiap langkah dapat dievaluasi dalam hal transparansi atau interpretasi.

###  Algorithm Transparency
*Bagaimana algoritma membuat model?*

Transparansi algoritma adalah tentang bagaimana algoritma mempelajari model dari data dan jenis hubungan apa yang dapat dipelajarinya.
Jika Anda menggunakan convolutional neural networks untuk mengklasifikasikan gambar, Anda dapat menjelaskan bahwa algoritme mempelajari detektor tepi dan filter pada lapisan terendah.
Ini adalah pemahaman tentang cara kerja algoritme, tetapi tidak untuk model spesifik yang dipelajari pada akhirnya, dan bukan untuk bagaimana prediksi individu dibuat.
Transparansi algoritma hanya membutuhkan pengetahuan tentang algoritma dan bukan data atau model yang dipelajari.
Buku ini berfokus pada interpretasi model dan bukan transparansi algoritma.
Algoritma seperti metode kuadrat terkecil untuk model linier dipelajari dan dipahami dengan baik.
Mereka dicirikan oleh transparansi yang tinggi.
Pendekatan deep learning (mendorong gradien melalui jaringan dengan jutaan bobot) kurang dipahami dengan baik dan cara kerja bagian dalam adalah fokus penelitian yang sedang berlangsung.
Mereka dianggap kurang transparan.


### Global, Holistic Model Interpretability
*Bagaimana model terlatih membuat prediksi?*

Anda dapat menggambarkan model sebagai dapat diinterpretasikan jika Anda dapat memahami seluruh model sekaligus (Lipton 2016[^Lipton2016]).
Untuk menjelaskan output model global, Anda memerlukan model terlatih, pengetahuan tentang algoritma dan data.
Tingkat interpretasi ini adalah tentang memahami bagaimana model membuat keputusan, berdasarkan pandangan holistik dari fitur-fiturnya dan masing-masing komponen yang dipelajari seperti bobot, parameter lain, dan struktur.
Fitur mana yang penting dan interaksi seperti apa yang terjadi di antara mereka?
Penafsiran model global membantu untuk memahami distribusi hasil target Anda berdasarkan fitur.
Penafsiran model global sangat sulit dicapai dalam praktik.
Model apa pun yang melebihi beberapa parameter atau bobot tidak mungkin cocok dengan memori jangka pendek rata-rata manusia.
Saya berpendapat bahwa Anda tidak dapat benar-benar membayangkan model linier dengan 5 fitur, karena itu berarti menggambar perkiraan hyperplane secara mental dalam ruang 5 dimensi.
Ruang fitur apa pun dengan lebih dari 3 dimensi tidak terbayangkan bagi manusia.
Biasanya, ketika orang mencoba memahami suatu model, mereka hanya mempertimbangkan sebagian saja, seperti bobot dalam model linier.

### Global Model Interpretability on a Modular Level
*Bagaimana bagian dari model mempengaruhi prediksi?*

Model Naive Bayes dengan ratusan fitur akan terlalu besar untuk saya dan Anda simpan dalam memori kerja kami.
Dan bahkan jika kami berhasil menghafal semua bobot, kami tidak akan dapat dengan cepat membuat prediksi untuk titik data baru.
Selain itu, Anda perlu memiliki distribusi bersama dari semua fitur di kepala Anda untuk memperkirakan pentingnya setiap fitur dan bagaimana fitur tersebut mempengaruhi prediksi secara rata-rata.
Sebuah tugas yang mustahil.
Tetapi Anda dapat dengan mudah memahami satu bobot.
Sementara interpretabilitas model global biasanya di luar jangkauan, ada peluang bagus untuk memahami setidaknya beberapa model pada tingkat modular.
Tidak semua model dapat diinterpretasikan pada tingkat parameter.
Untuk model linier, bagian yang dapat diinterpretasikan adalah bobot, untuk pohon itu akan menjadi perpecahan (fitur yang dipilih ditambah titik potong) dan prediksi simpul daun.
Model linier, misalnya, terlihat seolah-olah mereka dapat diinterpretasikan dengan sempurna pada tingkat modular, tetapi interpretasi dari satu bobot saling terkait dengan semua bobot lainnya.
Interpretasi bobot tunggal selalu disertai dengan catatan kaki bahwa fitur input lainnya tetap pada nilai yang sama, yang tidak terjadi pada banyak aplikasi nyata.
Model linier yang memprediksi nilai sebuah rumah, yang memperhitungkan ukuran rumah dan jumlah kamar, dapat memiliki bobot negatif untuk fitur ruangan.
Itu bisa terjadi karena sudah ada fitur ukuran rumah yang sangat berkorelasi.
Di pasar di mana orang lebih suka kamar yang lebih besar, rumah dengan kamar lebih sedikit bisa bernilai lebih dari rumah dengan lebih banyak kamar jika keduanya memiliki ukuran yang sama.
Bobot hanya masuk akal dalam konteks fitur lain dalam model.
Namun bobot dalam model linier masih dapat diinterpretasikan lebih baik daripada bobot deep neural network.

### Local Interpretability for a Single Prediction 
*Mengapa model membuat prediksi tertentu untuk sebuah instance?*

Anda dapat memperbesar satu contoh dan memeriksa apa yang diprediksi model untuk input ini, dan menjelaskan alasannya.
Jika Anda melihat prediksi individu, perilaku model yang kompleks mungkin berperilaku lebih menyenangkan.
Secara lokal, prediksi mungkin hanya bergantung secara linier atau monoton pada beberapa fitur, daripada memiliki ketergantungan yang kompleks pada fitur tersebut.
Misalnya, nilai sebuah rumah mungkin bergantung secara nonlinier pada ukurannya.
Tetapi jika Anda hanya melihat satu rumah seluas 100 meter persegi, ada kemungkinan bahwa untuk subset data tersebut, prediksi model Anda bergantung secara linier pada ukurannya.
Anda dapat mengetahuinya dengan mensimulasikan bagaimana prediksi harga berubah saat Anda menambah atau mengurangi ukuran sebesar 10 meter persegi.
Oleh karena itu, penjelasan lokal dapat lebih akurat daripada penjelasan global.
Buku ini menyajikan metode yang dapat membuat prediksi individu lebih dapat diinterpretasikan di [bagian tentang metode model-agnostic](#agnostic).

### Local Interpretability for a Group of Predictions
*Mengapa model membuat prediksi khusus untuk sekelompok instance?*

Prediksi model untuk beberapa contoh dapat dijelaskan baik dengan metode interpretasi model global (pada tingkat modular) atau dengan penjelasan masing-masing contoh.
Metode global dapat diterapkan dengan mengambil grup instance, memperlakukannya seolah-olah grup tersebut adalah kumpulan data lengkap, dan menggunakan metode global dengan subset ini.
Metode penjelasan individu dapat digunakan pada setiap contoh dan kemudian terdaftar atau dikumpulkan untuk seluruh grup.

<!--{pagebreak}-->

## Evaluation of Interpretability

Tidak ada konsensus nyata tentang apa yang dimaksud dengan interpretabilitas dalam machine learning.
Juga tidak jelas bagaimana mengukurnya.
Tetapi ada beberapa penelitian awal tentang ini dan upaya untuk merumuskan beberapa pendekatan untuk evaluasi, seperti yang dijelaskan pada bagian berikut.

Doshi-Velez dan Kim (2017) mengusulkan tiga tingkat utama untuk evaluasi interpretabilitas:

**Evaluasi tingkat aplikasi (tugas nyata)**:
Masukkan penjelasan ke dalam produk dan uji oleh pengguna akhir.
Bayangkan perangkat lunak pendeteksi patah tulang dengan komponen machine learning yang menemukan dan menandai patah tulang dalam sinar-X.
Pada tingkat aplikasi, ahli radiologi akan menguji perangkat lunak pendeteksi fraktur secara langsung untuk mengevaluasi model.
Ini membutuhkan pengaturan eksperimental yang baik dan pemahaman tentang bagaimana menilai kualitas.
Dasar yang baik untuk ini adalah selalu seberapa baik manusia dalam menjelaskan keputusan yang sama.

**Evaluasi tingkat manusia (tugas sederhana)** adalah evaluasi tingkat aplikasi yang disederhanakan.
Bedanya, eksperimen ini tidak dilakukan dengan pakar domain, tetapi dengan orang awam.
Hal ini membuat eksperimen lebih murah (terutama jika pakar domain adalah ahli radiologi) dan lebih mudah untuk menemukan lebih banyak penguji.
Contohnya adalah menunjukkan kepada pengguna penjelasan yang berbeda dan pengguna akan memilih yang terbaik.

**Evaluasi tingkat fungsi (tugas proxy)** tidak memerlukan manusia.
Ini bekerja paling baik ketika kelas model yang digunakan telah dievaluasi oleh orang lain dalam evaluasi tingkat manusia.
Misalnya, mungkin diketahui bahwa pengguna akhir memahami decision trees.
Dalam hal ini, proxy untuk kualitas penjelasan mungkin kedalaman pohon.
Pohon yang lebih pendek akan mendapatkan skor kemampuan menjelaskan yang lebih baik.
Masuk akal untuk menambahkan batasan bahwa kinerja prediktif pohon tetap baik dan tidak berkurang terlalu banyak dibandingkan dengan pohon yang lebih besar.

Bab berikutnya berfokus pada evaluasi penjelasan untuk prediksi individu pada tingkat fungsi.
Apa sifat-sifat relevan dari penjelasan yang akan kita pertimbangkan untuk evaluasi mereka?

<!--{pagebreak}-->

## Properties of Explanations {#properties}

Kami ingin menjelaskan prediksi model machine learning.
Untuk mencapai ini, kami mengandalkan beberapa metode penjelasan, yang merupakan algoritma yang menghasilkan penjelasan.
**Penjelasan biasanya menghubungkan nilai fitur dari sebuah instance dengan prediksi modelnya dengan cara yang dapat dipahami secara manusiawi.**
Jenis penjelasan lainnya terdiri dari satu set contoh data (misalnya dalam kasus model k-nearest neighbor).
Misalnya, kami dapat memprediksi risiko kanker menggunakan support vector machine dan menjelaskan prediksi menggunakan [local surrogate method](#lime), yang menghasilkan decision trees sebagai penjelasan.
Atau kita bisa menggunakan model linear regression alih-alih support vector machine.
Model linear regression sudah dilengkapi dengan metode penjelasan (interpretasi bobot).

Kami melihat lebih dekat sifat-sifat metode penjelasan dan penjelasan (Robnik-Sikonja dan Bohanec, 2018[^human-ml]).
Sifat-sifat ini dapat digunakan untuk menilai seberapa baik suatu metode atau penjelasan penjelasan.
Tidak jelas untuk semua sifat ini bagaimana mengukurnya dengan benar, jadi salah satu tantangannya adalah memformalkan cara menghitungnya.

**Sifat Metode Penjelasan**

- **Kekuatan Ekspresif** adalah "bahasa" atau struktur penjelasan yang dapat dihasilkan oleh metode.
Metode penjelasan dapat menghasilkan aturan IF-THEN, decision trees, weighted sum, bahasa alami atau yang lainnya.
- **Translusensi** menjelaskan seberapa besar ketergantungan metode penjelasan dalam melihat model machine learning, seperti parameternya.
Misalnya, metode penjelasan yang mengandalkan model yang dapat ditafsirkan secara intrinsik seperti model linear regression (Model-specific) sangat tembus cahaya.
Metode yang hanya mengandalkan manipulasi input dan mengamati prediksi tidak memiliki translusensi.
Tergantung pada skenario, tingkat transparansi yang berbeda mungkin diinginkan.
Keuntungan dari translusensi tinggi adalah metode ini dapat mengandalkan lebih banyak informasi untuk menghasilkan penjelasan.
Keuntungan dari translusensi rendah adalah metode penjelasan lebih portabel.
- **Portabilitas** menjelaskan berbagai model machine learning yang dapat digunakan metode penjelasan.
Metode dengan transparansi rendah memiliki portabilitas yang lebih tinggi karena mereka memperlakukan model machine learning sebagai black box.
Surrogate models mungkin merupakan metode penjelasan dengan portabilitas tertinggi.
Metode yang hanya berfungsi untuk mis. recurrent neural networks memiliki portabilitas rendah.
- **Kompleksitas Algoritma** menjelaskan kompleksitas komputasi dari metode yang menghasilkan penjelasan.
Properti ini penting untuk dipertimbangkan ketika waktu komputasi menjadi hambatan dalam menghasilkan penjelasan.

**Sifat Penjelasan Individu**

- **Akurasi**: Seberapa baik penjelasan memprediksi data yang tidak terlihat?
Akurasi tinggi sangat penting jika penjelasan digunakan untuk prediksi menggantikan model machine learning.
Akurasi rendah bisa baik-baik saja jika akurasi model machine learning juga rendah, dan jika tujuannya adalah untuk menjelaskan apa yang dilakukan model black box.
Dalam hal ini, hanya fidelity yang penting.
- **Fidelity**: Seberapa baik penjelasan tersebut mendekati prediksi model black box?
Fidelitas tinggi adalah salah satu properti terpenting dari sebuah penjelasan, karena penjelasan dengan fidelitas rendah tidak berguna untuk menjelaskan model machine learning.
Akurasi dan fidelity sangat erat hubungannya.
Jika model black box memiliki akurasi yang tinggi dan penjelasannya memiliki fidelitas yang tinggi, maka penjelasannya juga memiliki akurasi yang tinggi.
Beberapa penjelasan hanya menawarkan fidelitas lokal, artinya penjelasan tersebut hanya mendekati prediksi model untuk subset data (mis. [local surrogate models](#lime)) atau bahkan hanya untuk instance data individual (mis. [Nilai Shapley]( #shapley)).
- **Konsistensi**: Seberapa besar perbedaan penjelasan antara model yang telah dilatih pada tugas yang sama dan yang menghasilkan prediksi serupa?
Sebagai contoh, saya melatih support vector machine dan model linear regression pada tugas yang sama dan keduanya menghasilkan prediksi yang sangat mirip.
Saya menghitung penjelasan menggunakan metode pilihan saya dan menganalisis betapa berbedanya penjelasan tersebut.
Jika penjelasannya sangat mirip, penjelasannya sangat konsisten.
Saya menemukan properti ini agak rumit, karena kedua model dapat menggunakan fitur yang berbeda, tetapi mendapatkan prediksi serupa (juga disebut ["Efek Rashomon"](https://en.wikipedia.org/wiki/Rashomon_effect)).
Dalam hal ini konsistensi yang tinggi tidak diinginkan karena penjelasannya harus sangat berbeda.
Konsistensi tinggi diinginkan jika model benar-benar bergantung pada hubungan serupa.
- **Stabilitas**: Seberapa mirip penjelasan untuk kejadian serupa?
Sementara konsistensi membandingkan penjelasan antar model, stabilitas membandingkan penjelasan antara contoh serupa untuk model tetap.
Stabilitas tinggi berarti bahwa sedikit variasi dalam fitur suatu instans tidak secara substansial mengubah penjelasan (kecuali variasi kecil ini juga sangat mengubah prediksi).
Kurangnya stabilitas dapat menjadi hasil dari varians yang tinggi dari metode penjelasan.
Dengan kata lain, metode penjelasan sangat dipengaruhi oleh sedikit perubahan nilai fitur dari instance yang akan dijelaskan.
Kurangnya stabilitas juga dapat disebabkan oleh komponen non-deterministik dari metode penjelasan, seperti langkah pengambilan sampel data, seperti yang digunakan [local surrogate method](#lime).
Stabilitas tinggi selalu diinginkan.
- **Comprehensibility**: Seberapa baik manusia memahami penjelasannya?
Ini terlihat seperti satu properti lagi di antara banyak properti, tetapi itu adalah gajah di dalam ruangan.
Sulit untuk didefinisikan dan diukur, tetapi sangat penting untuk dilakukan dengan benar.
Banyak orang setuju bahwa pemahaman tergantung pada audiens.
Gagasan untuk mengukur keterpahaman termasuk mengukur ukuran penjelasan (jumlah fitur dengan bobot bukan nol dalam model linier, jumlah decision rules, ...) atau menguji seberapa baik orang dapat memprediksi perilaku model machine learning dari penjelasan.
Kelengkapan fitur yang digunakan dalam penjelasan juga harus diperhatikan.
Transformasi fitur yang kompleks mungkin kurang dipahami daripada fitur aslinya.
- **Kepastian**: Apakah penjelasannya mencerminkan kepastian model machine learning?
Banyak model machine learning hanya memberikan prediksi tanpa pernyataan tentang keyakinan model bahwa prediksi itu benar.
Jika model memprediksi 4% kemungkinan kanker untuk satu pasien, apakah sama pasti dengan 4% kemungkinan yang diterima pasien lain, dengan nilai fitur yang berbeda?
Penjelasan yang mencakup kepastian model sangat berguna.
- **Tingkat Kepentingan**: Seberapa baik penjelasan mencerminkan feature importance atau bagian dari penjelasan?
Misalnya, jika decision rules dihasilkan sebagai penjelasan untuk prediksi individu, apakah jelas kondisi aturan mana yang paling penting?
- **Kebaruan**: Apakah penjelasan mencerminkan apakah instance data yang akan dijelaskan berasal dari wilayah "baru" yang jauh dari distribusi data pelatihan?
Dalam kasus seperti itu, modelnya mungkin tidak akurat dan penjelasannya mungkin tidak berguna.
Konsep kebaruan berkaitan dengan konsep kepastian.
Semakin tinggi kebaruan, semakin besar kemungkinan model akan memiliki kepastian yang rendah karena kurangnya data.
- **Keterwakilan**: Berapa banyak contoh yang dicakup oleh penjelasan?
Penjelasan dapat mencakup keseluruhan model (misalnya interpretasi bobot dalam model linear regression) atau hanya mewakili prediksi individu (misalnya [Nilai Shapley](#shapley)).

<!--{pagebreak}-->

## Human-friendly Explanations {#explanation}

Mari kita menggali lebih dalam dan menemukan apa yang kita manusia lihat sebagai penjelasan "baik" dan apa implikasinya untuk interpretable machine learning.
Penelitian humaniora dapat membantu kita mencari tahu.
Miller (2017) telah melakukan survei besar-besaran atas publikasi tentang penjelasan, dan bab ini didasarkan pada ringkasannya.

Dalam bab ini, saya ingin meyakinkan Anda tentang hal-hal berikut:
Sebagai penjelasan atas suatu peristiwa, manusia lebih menyukai penjelasan singkat (hanya 1 atau 2 penyebab) yang mengkontraskan situasi saat ini dengan situasi di mana peristiwa itu tidak akan terjadi.
Terutama penyebab abnormal memberikan penjelasan yang baik.
Eksplanasi adalah interaksi sosial antara si penjelas dan si penjelas (penerima penjelasan) dan oleh karena itu konteks sosial memiliki pengaruh yang besar terhadap isi penjelasan yang sebenarnya.

Ketika Anda membutuhkan penjelasan dengan SEMUA faktor untuk prediksi atau perilaku tertentu, Anda tidak menginginkan penjelasan yang ramah manusia, tetapi atribusi kausal yang lengkap.
Anda mungkin menginginkan atribusi kausal jika Anda diwajibkan secara hukum untuk menentukan semua fitur yang memengaruhi atau jika Anda men-debug model machine learning.
Dalam hal ini, abaikan poin-poin berikut.
Dalam semua kasus lain, di mana orang awam atau orang dengan sedikit waktu adalah penerima penjelasan, bagian berikut akan menarik bagi Anda.


### What Is an Explanation?

Penjelasan adalah **jawaban atas pertanyaan mengapa** (Miller 2017).

- Mengapa pengobatan tidak berhasil pada pasien?
- Mengapa pinjaman saya ditolak?
- Mengapa kita belum dihubungi oleh kehidupan alien?

Dua pertanyaan pertama dapat dijawab dengan penjelasan "sehari-hari", sedangkan yang ketiga berasal dari kategori "fenomena ilmiah yang lebih umum dan pertanyaan filosofis".
Kami fokus pada jenis penjelasan "sehari-hari", karena itu relevan dengan interpretable machine learning.
Pertanyaan yang dimulai dengan "bagaimana" biasanya dapat diubah menjadi pertanyaan "mengapa":
"Bagaimana pinjaman saya ditolak?" dapat berubah menjadi "Mengapa pinjaman saya ditolak?".

Berikut ini, istilah "penjelasan" mengacu pada proses sosial dan kognitif menjelaskan, tetapi juga produk dari proses ini.
Penjelajah bisa berupa manusia atau mesin.


### What Is a Good Explanation? {#good-explanation}

Bagian ini lebih lanjut memadatkan ringkasan Miller tentang penjelasan "baik" dan menambahkan implikasi konkret untuk interpretable machine learning.

**Penjelasan yang kontras** (Lipton 1990[^lipton2]).
Manusia biasanya tidak bertanya mengapa prediksi tertentu dibuat, tetapi mengapa prediksi ini dibuat *bukan prediksi lain*.
Kita cenderung berpikir dalam kasus kontrafaktual, yaitu "Bagaimana prediksi jika input X berbeda?".
Untuk prediksi harga rumah, pemilik rumah mungkin akan tertarik mengapa harga yang diprediksi lebih tinggi dibandingkan dengan harga yang lebih rendah yang mereka harapkan.
Jika aplikasi pinjaman saya ditolak, saya tidak peduli untuk mendengar semua faktor yang secara umum mendukung atau menentang penolakan.
Saya tertarik dengan faktor-faktor dalam aplikasi saya yang perlu diubah untuk mendapatkan pinjaman.
Saya ingin mengetahui perbedaan antara aplikasi saya dan versi aplikasi saya yang akan diterima.
Pengakuan bahwa penjelasan yang kontras penting adalah temuan penting untuk machine learning yang dapat dijelaskan.
Dari sebagian besar interpretable models, Anda dapat mengekstrak penjelasan yang secara implisit membedakan prediksi instance dengan prediksi instance data buatan atau rata-rata instance.
Dokter mungkin bertanya: "Mengapa obat itu tidak bekerja untuk pasien saya?".
Dan mereka mungkin menginginkan penjelasan yang membedakan pasien mereka dengan pasien yang obatnya bekerja dan yang mirip dengan pasien yang tidak merespons.
Penjelasan kontrastif lebih mudah dipahami daripada penjelasan lengkap.
Penjelasan lengkap tentang pertanyaan dokter mengapa obat tidak bekerja mungkin termasuk:
Pasien telah menderita penyakit selama 10 tahun, 11 gen diekspresikan secara berlebihan, tubuh pasien sangat cepat dalam memecah obat menjadi bahan kimia yang tidak efektif, ...
Penjelasan yang kontras mungkin jauh lebih sederhana:
Berbeda dengan pasien yang merespons, pasien yang tidak merespons memiliki kombinasi gen tertentu yang membuat obat menjadi kurang efektif.
Penjelasan terbaik adalah penjelasan yang menyoroti perbedaan terbesar antara objek yang menarik dan objek referensi.
**Apa artinya untuk interpretable machine learning**:
Manusia tidak menginginkan penjelasan yang lengkap untuk suatu prediksi, tetapi ingin membandingkan apa perbedaannya dengan prediksi contoh lain (bisa jadi buatan).
Membuat penjelasan yang kontras bergantung pada aplikasi karena memerlukan titik referensi untuk perbandingan.
Dan ini mungkin tergantung pada titik data yang akan dijelaskan, tetapi juga pada pengguna yang menerima penjelasan.
Seorang pengguna situs web prediksi harga rumah mungkin ingin mendapatkan penjelasan tentang prediksi harga rumah yang kontras dengan rumahnya sendiri atau mungkin dengan rumah lain di situs web tersebut atau mungkin dengan rumah rata-rata di lingkungan tersebut.
Solusi untuk pembuatan otomatis penjelasan kontrastif mungkin juga melibatkan pencarian prototypes atau arketipe dalam data.

**Penjelasan dipilih**.
Orang tidak mengharapkan penjelasan yang mencakup daftar penyebab yang sebenarnya dan lengkap dari suatu peristiwa.
Kita terbiasa memilih satu atau dua penyebab dari berbagai kemungkinan penyebab sebagai penjelasannya.
Sebagai buktinya, nyalakan berita TV:
"Penurunan harga saham disalahkan pada reaksi yang berkembang terhadap produk perusahaan karena masalah dengan pembaruan perangkat lunak terbaru."
"Tsubasa dan timnya kalah dalam pertandingan karena pertahanan yang lemah: mereka memberi lawan mereka terlalu banyak ruang untuk memainkan strategi mereka."
"Meningkatnya ketidakpercayaan terhadap institusi mapan dan pemerintah kita adalah faktor utama yang mengurangi jumlah pemilih."
Fakta bahwa suatu peristiwa dapat dijelaskan oleh berbagai penyebab disebut Efek Rashomon.
Rashomon adalah film Jepang yang menceritakan kisah (penjelasan) alternatif yang kontradiktif tentang kematian seorang samurai.
Untuk model machine learning, akan menguntungkan jika prediksi yang baik dapat dibuat dari fitur yang berbeda.
Metode ensemble yang menggabungkan beberapa model dengan fitur yang berbeda (penjelasan yang berbeda) biasanya berkinerja baik karena rata-rata dari "cerita" tersebut membuat prediksi lebih kuat dan akurat.
Tapi itu juga berarti bahwa ada lebih dari satu penjelasan selektif mengapa prediksi tertentu dibuat.
**Apa artinya untuk interpretable machine learning**:
Buat penjelasannya sangat singkat, berikan hanya 1 sampai 3 alasan, bahkan jika dunia ini lebih kompleks.
[Metode LIME](#lime) bekerja dengan baik dengan ini.

**Penjelasan bersifat sosial**.
Mereka adalah bagian dari percakapan atau interaksi antara penjelas dan penerima penjelasan.
Konteks sosial menentukan isi dan sifat penjelasan.
Jika saya ingin menjelaskan kepada orang teknis mengapa cryptocurrency digital sangat berharga, saya akan mengatakan hal-hal seperti:
"Buku besar yang terdesentralisasi, terdistribusi, berbasis blockchain, yang tidak dapat dikendalikan oleh entitas pusat, beresonansi dengan orang-orang yang ingin mengamankan kekayaan mereka, yang menjelaskan tingginya permintaan dan harga."
Tetapi kepada nenek saya, saya akan mengatakan:
"Dengar, Nenek: Cryptocurrency sedikit mirip dengan emas komputer. Orang suka dan membayar banyak untuk emas, dan anak muda suka dan membayar banyak untuk emas komputer."
**Apa artinya untuk interpretable machine learning**:
Perhatikan lingkungan sosial aplikasi machine learning Anda dan audiens target.
Mendapatkan bagian sosial dari model machine learning dengan benar bergantung sepenuhnya pada aplikasi spesifik Anda.
Temukan ahli dari humaniora (misalnya psikolog dan sosiolog) untuk membantu Anda.

**Penjelasan berfokus pada yang tidak normal**.
Orang lebih fokus pada penyebab abnormal untuk menjelaskan peristiwa (Kahnemann dan Tversky, 1981[^Kahnemann]).
Ini adalah penyebab yang memiliki kemungkinan kecil tetapi tetap terjadi.
Penghapusan penyebab abnormal ini akan sangat mengubah hasilnya (counterfactual explanations).
Manusia menganggap penyebab "abnormal" semacam ini sebagai penjelasan yang bagus.
Contoh dari trumbelj dan Kononenko (2011)[^Strumbelj2011] adalah:
Asumsikan kita memiliki kumpulan data situasi pengujian antara guru dan siswa.
Siswa menghadiri kursus dan lulus kursus langsung setelah berhasil memberikan presentasi.
Guru memiliki pilihan untuk mengajukan pertanyaan tambahan kepada siswa untuk menguji pengetahuan mereka.
Siswa yang tidak dapat menjawab pertanyaan-pertanyaan ini akan gagal dalam kursus.
Siswa dapat memiliki tingkat persiapan yang berbeda, yang diterjemahkan ke dalam probabilitas yang berbeda untuk menjawab pertanyaan guru dengan benar (jika mereka memutuskan untuk menguji siswa).
Kami ingin memprediksi apakah seorang siswa akan lulus kursus dan menjelaskan prediksi kami.
Peluang lulus adalah 100% jika guru tidak mengajukan pertanyaan tambahan, jika tidak, kemungkinan lulus tergantung pada tingkat persiapan siswa dan probabilitas yang dihasilkan untuk menjawab pertanyaan dengan benar.
Skenario 1:
Guru biasanya mengajukan pertanyaan tambahan kepada siswa (misalnya 95 dari 100 kali).
Seorang siswa yang tidak belajar (10% kesempatan untuk lulus bagian pertanyaan) bukanlah salah satu yang beruntung dan mendapat tambahan pertanyaan yang gagal dia jawab dengan benar.
Mengapa siswa gagal dalam kursus?
Saya akan mengatakan bahwa itu adalah kesalahan siswa untuk tidak belajar.
Skenario 2:
Guru jarang mengajukan pertanyaan tambahan (misalnya 2 dari 100 kali).
Untuk siswa yang belum mempelajari soal, kami akan memprediksi kemungkinan lulus mata pelajaran yang tinggi karena soal tidak mungkin.
Tentu saja, salah satu siswa tidak mempersiapkan pertanyaan, yang memberinya kesempatan 10% untuk lulus pertanyaan.
Dia tidak beruntung dan guru mengajukan pertanyaan tambahan yang tidak dapat dijawab oleh siswa dan dia gagal dalam pelajaran.
Apa alasan kegagalannya?
Saya berpendapat bahwa sekarang, penjelasan yang lebih baik adalah "karena guru menguji siswa".
Tidak mungkin guru akan menguji, sehingga guru berperilaku tidak normal.
**Apa artinya untuk interpretable machine learning**:
Jika salah satu fitur masukan untuk prediksi tidak normal dalam arti apa pun (seperti kategori langka dari fitur kategoris) dan fitur tersebut memengaruhi prediksi, fitur tersebut harus disertakan dalam penjelasan, meskipun fitur 'normal' lainnya memiliki pengaruh yang sama pada prediksi sebagai yang tidak normal.
Sebuah fitur abnormal dalam contoh prediksi harga rumah kami mungkin bahwa rumah yang agak mahal memiliki dua balkon.
Bahkan jika beberapa metode atribusi menemukan bahwa dua balkon berkontribusi sama besarnya dengan perbedaan harga seperti ukuran rumah di atas rata-rata, lingkungan yang baik atau renovasi baru-baru ini, fitur abnormal "dua balkon" mungkin merupakan penjelasan terbaik mengapa rumah itu begitu mahal.

**Penjelasan adalah jujur**.
Penjelasan yang baik terbukti benar dalam kenyataan (yaitu dalam situasi lain).
Tapi mengganggu, ini bukan faktor yang paling penting untuk penjelasan yang "baik".
Misalnya, selektivitas tampaknya lebih penting daripada kejujuran.
Penjelasan yang hanya memilih satu atau dua kemungkinan penyebab jarang mencakup seluruh daftar penyebab yang relevan.
Selektivitas menghilangkan sebagian dari kebenaran.
Tidak benar bahwa hanya satu atau dua faktor saja yang menyebabkan crash pasar saham, tetapi kenyataannya ada jutaan penyebab yang mempengaruhi jutaan orang untuk bertindak sedemikian rupa sehingga pada akhirnya menyebabkan crash. .
**Apa artinya untuk interpretable machine learning**:
Penjelasannya harus memprediksi peristiwa sejujur ​​mungkin, yang dalam machine learning terkadang disebut **fidelity**.
Jadi jika kita mengatakan bahwa balkon kedua meningkatkan harga sebuah rumah, maka itu juga harus berlaku untuk rumah lain (atau setidaknya untuk rumah serupa).
Bagi manusia, kesetiaan suatu penjelasan tidak sepenting selektivitas, kontras, dan aspek sosialnya.

**Penjelasan yang baik konsisten dengan keyakinan sebelumnya dari pihak yang menjelaskan**.
Manusia cenderung mengabaikan informasi yang tidak sesuai dengan keyakinan mereka sebelumnya.
Efek ini disebut bias konfirmasi (Nickerson 1998[^Nickerson]).
Penjelasan tidak luput dari bias semacam ini.
Orang akan cenderung meremehkan atau mengabaikan penjelasan yang tidak sesuai dengan keyakinan mereka.
Set keyakinan bervariasi dari orang ke orang, tetapi ada juga keyakinan sebelumnya berbasis kelompok seperti pandangan dunia politik.
**Apa artinya untuk interpretable machine learning**:
Penjelasan yang baik konsisten dengan keyakinan sebelumnya.
Ini sulit untuk diintegrasikan ke dalam machine learning dan mungkin akan secara drastis membahayakan kinerja prediktif.
Keyakinan kami sebelumnya untuk pengaruh ukuran rumah pada harga yang diprediksi adalah bahwa semakin besar rumah, semakin tinggi harganya.
Mari kita asumsikan bahwa sebuah model juga menunjukkan efek negatif dari ukuran rumah pada prediksi harga untuk beberapa rumah.
Model telah mempelajari ini karena meningkatkan kinerja prediktif (karena beberapa interaksi yang kompleks), tetapi perilaku ini sangat bertentangan dengan keyakinan kami sebelumnya.
Anda dapat menerapkan batasan monoton (fitur hanya dapat memengaruhi prediksi dalam satu arah) atau menggunakan sesuatu seperti model linier yang memiliki properti ini.

**Penjelasan yang baik bersifat umum dan mungkin**.
Suatu penyebab yang dapat menjelaskan banyak peristiwa adalah sangat umum dan dapat dianggap sebagai penjelasan yang baik.
Perhatikan bahwa ini bertentangan dengan klaim bahwa penyebab abnormal memberikan penjelasan yang baik.
Seperti yang saya lihat, penyebab abnormal mengalahkan penyebab umum.
Penyebab abnormal menurut definisi jarang terjadi dalam skenario yang diberikan.
Dengan tidak adanya kejadian abnormal, penjelasan umum dianggap sebagai penjelasan yang baik.
Juga ingat bahwa orang cenderung salah menilai probabilitas kejadian bersama.
(Joe adalah seorang pustakawan. Apakah dia lebih cenderung pemalu atau pemalu yang suka membaca buku?)
Contoh yang baik adalah "Rumah itu mahal karena besar", yang merupakan penjelasan yang sangat umum dan bagus tentang mengapa rumah itu mahal atau murah.
**Apa artinya untuk interpretable machine learning**:
Umum dapat dengan mudah diukur dengan dukungan fitur, yang merupakan jumlah contoh yang penjelasannya berlaku dibagi dengan jumlah total contoh.

[^Miller2017]: Miller, Tim. "Explanation in artificial intelligence: Insights from the social sciences." arXiv Preprint arXiv:1706.07269. (2017).

[^Doshi2017]: Doshi-Velez, Finale, and Been Kim. "Towards a rigorous science of interpretable machine learning," no. Ml: 1–13. http://arxiv.org/abs/1702.08608 ( 2017).

[^Heider]: Heider, Fritz, and Marianne Simmel. "An experimental study of apparent behavior." The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944).

[^Lipton2016]: Lipton, Zachary C. "The mythos of model interpretability." arXiv preprint arXiv:1606.03490, (2016).

[^Kahnemann]: Kahneman, Daniel, and Amos Tversky. "The Simulation Heuristic." Stanford Univ CA Dept of Psychology. (1981).

[^Strumbelj2011]: Štrumbelj, Erik, and Igor Kononenko. "A general method for visualizing and explaining black-box regression models." In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).

[^Nickerson]: Nickerson, Raymond S. "Confirmation Bias: A ubiquitous phenomenon in many guises." Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998).

[^critique]: Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. "Examples are not enough, learn to criticize! Criticism for interpretability." Advances in Neural Information Processing Systems (2016).

[^human-ml]: Robnik-Sikonja, Marko, and Marko Bohanec. "Perturbation-based explanations of prediction models." Human and Machine Learning. Springer, Cham. 159-175. (2018).

[^lipton2]: Lipton, Peter. "Contrastive explanation." Royal Institute of Philosophy Supplements 27 (1990): 247-266.
