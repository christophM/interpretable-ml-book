```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

## Accumulated Local Effects (ALE) {#ale}

Accumulated Local Effects show the effect of a single feature or of two features on the predicted outcome.
The Accumulated Local Effects (ALE) plot [^ALE] is an alternative to the PDP and fixes the problem that PDPs have when features are correlated.

*Keywords: partial dependence plots, PDP, PD plot, marginal means, predictive margins, marginal effects, ALE plot*


There is a problem with partial dependence plots, and that's correlated features.
When two input features are strongly correlated, the partial dependence plot for one of those features will not make sense.
How come? 
When features are correlated and we compute the partial dependence, we use grid values in combination with the correlated feature that don't make sense. 
For example we want to predict the worth of a house using number of rooms and the size of the living area.
Then we analyse the partial dependence of the predicted price on the living area.
For the first grid values - let's say a living area of 30 square meters - we use all data points, even flats with 10 rooms, which don't yield a realistic data instance. 
But in PD plots we use those to compute the average effect. 
So what can we do?
The answer is to only use instance for which the grid values that are forced as feature values make realistic instances.
One solution is to use only the neighbours, meaning that for the 30 square meters, we only compute the partial dependence with houses that are between let's say 25 and 35 square meters (wait, are those real houses? maybe houses for ants). 
The solution also has a name: ALE plots, which is short for [Accumulated Local Effects (ALE) Plots](#ale).


ALEplots are partial dependence plots that solve the problem the PDPs have with correlated features.
Both types solve the problem of computing the average marginal effects: 

$$E_{x_C}\left[\hat{f}(x_S,x_C)\right]$$

The difference is that PDPs assume that the feature are not strongly correlated for computing the average effects. 
The ALEplot also works for correlated features, by only computing the partial effects locally.
So how do ALEplots solve this problem?
First, the method divides a feature into intervals instead of grid points.
This is almost the same, but the difference is important. 
Because ALEplots now use all the data points in each interval to compute the partial effects.


```{r aleplot-motivation, fig.cap = "Two strongly correlated features x1 and x2. To compute the partial dependence at x1 = 0.75, the PDP averages the prediction over the whole range of x2 (left plot), even if unrealistic combinations of x1 and x2 are used. ALE plots average only over the conditional distribution at x1 = 0.75 (right plot). "}

set.seed(1)
n = 100
intercept = 0.75

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +  
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank())

p.int = p + geom_vline(xintercept = intercept)
x1.dens = density(x1)
x1.dens.df = data.frame(dens = x1.dens$y, x = x1.dens$x)


p1 = p.int + geom_path(data = x1.dens.df, aes(x = intercept - dens/10, y = x)) + 
  ggtitle(sprintf("Marginal distribution p(x2)", intercept)) + 
  scale_y_continuous(limits = c(-0.2, 1.2))

x1.dens.ale = density(x1[(x1 > (intercept - 0.1)) & (x1 < (intercept + 0.1))])
x1.dens.ale.df = data.frame(dens = x1.dens.ale$y, x = x1.dens.ale$x)

p2 = p.int + geom_path(data = x1.dens.ale.df, aes(x = intercept - dens/10, y = x)) + 
  ggtitle(sprintf("Conditional distribution p(x2|x1=%.2f)", intercept)) + 
  scale_y_continuous(limits = c(-0.2, 1.2))

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

The following graphic show how the ALEplots are calculated. 


```{r aleplot-computation, fig.cap = "For the computation of ALE plots, we first divide the feature into intervals (vertical lines). In each interval, we select the data instances (points) and compute the difference in prediction when we replace the feature with the upper and lower limit of the interval (horizontal lines). Not shown in the plot: These difference are later cumulated and centered, which yields the ALE curve."}

set.seed(12)
n = 25

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank()) 


grid.df = data.frame(x1 = seq(from = 0,  to = 1, length.out = 6)[1:6], x2 = NA)
label.df = grid.df[1:5,]
label.df$x1 = label.df$x1 + 0.1
label.df$x2 = 0.95
label.df$label = sprintf("N1(%i)", 1:5)

break.labels = c(expression(z[0~","~1]),  expression(z[1~","~1]), expression(z[2~","~1]), expression(z[3~","~1]),
  expression(z[4~","~1]), expression(z[5~","~1]))

diff.df = df[df$x1 <= 0.8 & df$x1 > 0.6, ]

p + geom_vline(data = grid.df, aes(xintercept = x1), linetype = 3) + 
  scale_x_continuous(breaks = seq(from = 0,  to = 1, length.out = 6), limits = c(0, 1), labels = break.labels) + 
  geom_label(data = label.df, aes(x = x1, y = x2, label = label)) + 
  geom_segment(data = diff.df, aes(x = 0.6, xend = 0.8, y = x2, yend  = x2), arrow = arrow(ends = "both", angle = 90, length = unit(0.07, "inches")))
```

ALE plots are plots for estimates of the first order (1D) or second order (2D) effects of features on the prediction.
An ALE plot for feature $x_j$ is the accumulated integral of the expected gradient of the prediction over values of $x_j$.
We are interested in the partial function of the prediction dependent on only a single (or two) features.
Instead of directly computing the average of the function, the authors calculate the derivative and integrate again over the derivative to get the value of the function. 
Now, that sound stupid. 
Derivation and integration cancel each other out, like first subtracting, then adding the same number. 
Why does this make sense?
Because the derivative is computed at different grid values and averages the effects locally, avoiding the mistake of partial dependence plots of averaging over the marginal distribution which is so problematic when the features are correlated.

$$\tilde{f}_{j,ALE}(x_j)=\int^{x_j}_{z_{0,j}}E\left[f^j(X_j,X_{\setminus{}j})|X_j=z_j\right]dz_j$$
This is the theoretic construct, also it is also centered as you will see in the estimator.
You might notice that this would require that we f has a derivative.
But for the actual estimation, we use a sample estimator and divide the feature into many intervals, so it is not a problem.
First we have to estimate the uncentered effect: 

$$\hat{\tilde{f}}_{j,ALE}(x)=\sum_{k=1}^{k_j(x)}\frac{1}{n_j(k)}\sum_{i:x_{i,j}\in{}N_j(k)}\left[f(z_{k,j},x_{i\setminus{}j})-f(z_{k-1,j},x_{i\setminus{}j})\right]$$


This effect is then centered, so that the mean effect is zero:

$$\hat{f}_{j,ALE}(x)=\hat{\tilde{f}}_{j,ALE}(x)-\frac{1}{n}\sum_{i=1}^{n}\hat{\tilde{f}}_{j,ALE}(x_{i,j})$$

The value of the ALE can be interpreted as the effect of the feature at a certain feature value compared to the mean. 
For example if the curve at $x_j=3$ is -2, it means that when $x_j$ has value 2, then the prediction is lower by 2 compared to the average prediction.

Let's see this thing now in action. 
I create a data situation in which the partial dependence plot fails. 
Namely a situation in which the features are strongly correlated and the prediction model does something weird at a combination of the two features for which we never observed instances.


```{r correlation-problem, fig.cap = "Two features and a predicted outcome. The model simply predicts for y (shaded background) the sum of the two features, but there is also some artifact where the model always predicts 2, when x1 is larger than 0.7 and x2 smaller than 0.3. Since the distribution doesn't touch this area, there shouldn't be a problem. Shoudl the distribution shift, then there would be one."}

set.seed(12)
n = 25

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  
df$y = x1 + x2

mod  = lm(y ~ ., data = df)

y.fun = function(X.model, newdata) {
  pred = predict(X.model, newdata)
  pred[newdata$x1 > 0.7 & newdata$x2 < 0.3] = 2
  pred
}

grid.dat = expand.grid(x1 = seq(from = 0, to = 1, length.out = 20), x2 = seq(from = 0, to = 1, length.out = 20))
grid.dat$predicted = y.fun(mod, grid.dat)

ggplot(df) + geom_tile(data = grid.dat, aes(x = x1, y = x2, fill = predicted)) + 
  geom_point(aes(x = x1, y = x2), size = 3) + 
  scale_fill_gradient(low = "white", high = "blue")
```

"This is a completely made up example", I here some people yelling.
"What kind of model would do such a thing?! Unrealistic!", other join. 
Yes, this is a toy example. 
BUT, remember that, when you train a model, the model only fits the existing data points. 
Any area outside of the distributions, depending on the type of model anything can happen, because the model will not be penalized for doing weird stuff abroad.
"Where there is no judge, there will be no judgement".
It's like dancing like nobodies watching.
Linear models for example have extreme behaviour outside the are of the data, because they are linear functions. 
Imagine a linear regression model for predicting the worth of a house which has a postive weight for the influence of the living area.
Going outside the data means going beyond certain levels of square meters (let's say 3000) will make your predictions grow extreme.
The phenomenon is called extrapolation and that's also what's happening when you have correlated features with partial dependence plots.
Extrapolation of values to values that don't even exist.
See what happens with the PD plots and how they compare to the ALE plots in our little example:


```{r correlation-pdp-ale-compute, plot = FALSE}
library(ALEPlot)
pred = Predictor$new(mod, data = df, predict.fun = y.fun)
pdp = Partial$new(pred, feature = "x1", ice = FALSE)
pdp1 = pdp$plot() + ggtitle("PDP")
pdp = Partial$new(pred, feature = "x2", ice = FALSE)
pdp2 = pdp$plot() + ggtitle("PDP")

ale1.df = data.frame(ALEPlot(df, mod, J = "x1", pred.fun = y.fun))
ale1 = ggplot(ale1.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("x1") + scale_y_continuous(expression(hat(y))) + 
  ggtitle("ALE plot")
ale2.df = data.frame(ALEPlot(df, mod, J = "x2", pred.fun = y.fun))
ale2 = ggplot(ale2.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("x2") + scale_y_continuous(expression(hat(y))) + 
  ggtitle("ALE plot")
```


```{r correlation-pdp-ale-plot, fig.cap = "Comparing the marginal feature effects computed with PDP and ALE. The PDP picks up on the artifact in the data, which is not part of the data distribution, which shows as a jump in the plot. The ALE plot correctly identifies that the machine learning model has a linear relationship between features and prediction, ignoring areas where no data points are."}
gridExtra::grid.arrange(pdp1, pdp2, ale1, ale2)
```


ALE plot can also be used with more features, but, as with PDP,  only makes sense with two at maximum.
Two feature works as one feature, with the difference that we don't build intervals, but rectangles and use for the computation of the marginal effects the  data instances in each rectangle.
I'll spare you with the formula for 2D plots. 
If you are interested, I refer to the paper, formulas (13) - (16) for the computation.
Here is some intuition how it is computed:

```{r aleplot-computation-2d, fig.cap = "Illustration on how to calculate the second-order Accumulated Local Effects. We lay a grid over the two features we are interested in. In each cell of that grid (one is exemplary highlighted), we compute the second-order differences, which is the joint effect (indicated by arrow from cell bottom left to top right) minus the two first order effects, as indicated by the vertical and horizontal arrows next to the cell. In each cell the effects are averaged for the points within that cell."}


p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank()) 


grid.df1 = data.frame(x1 = seq(from = min(df$x1),  to = max(df$x1), length.out = 6)[1:6], x2 = NA)
grid.df2 = data.frame(x2 = seq(from = min(df$x2),  to = max(df$x2), length.out = 6)[1:6], x1 = NA)

chosen.tile = data.frame(x = grid.df1$x1[4], xend = grid.df1$x1[5], y = grid.df2$x2[4], yend = grid.df2$x2[5])

mv.arr = 0.02
p + geom_vline(data = grid.df1, aes(xintercept = x1), linetype = 3) + 
  geom_hline(data = grid.df2, aes(yintercept = x2), linetype = 3) + 
  geom_rect(aes(xmin = x, xmax = xend,  ymin = y, ymax = yend), data = chosen.tile, alpha = 0, color = "black", size = 1.1) + 
  geom_segment(data = chosen.tile, aes(x = x, xend = xend, y = y, yend = yend), arrow = arrow(angle = 10)) + 
    geom_segment(data = chosen.tile, aes(x = x - mv.arr, xend = x - mv.arr, y = y, yend = yend), arrow = arrow(angle = 10)) + 
    geom_segment(data = chosen.tile, aes(x = x, xend = xend, y = y - mv.arr, yend = y - mv.arr), arrow = arrow(angle = 10))  
```


When spanning the grid, which is the cartesian product of the quantiles of both features, some cells might remain empty, especially when the features are correlated. 
In the example in the figure before, many cells were empty because of the correlation.
In this case, the ALE can either be omitted and visualized with a greyed out box. 
Alternatively, you can can replace the average local effect for an empty cell by the corresponding value for the
nearest nonempty cell.
How is this problem distinct from the extrapolation problem that PDPs have?
Well, for PDPs these empty areas are involved in estimating first-order univariate effects, which biases the estimates.
For ALE plots the empty cells only play a role in second-order effects and when a cell is empty it can also be indicated by plotting points where data lie into the plot to make it clear.


Interpretation of the second-order ALE:
The second-order effect is the additional interaction effect of the features, when we account for the main effects of the features.
Let's say two features don't interact, but have each a linear main-effect on the predicted. 
In the 1D ALE plot for each feature we will see a line as effect curve. 
But when plotting the 2D ALE plot it should remain zero, because the second-order effect is only the additional effect of the interaction.
The same as the first-order effect is centered at zero and does not evovle around the mean prediction like partial dependence plots do.



The Accumulated Local Effects can also be computed for arbitrary higher orders (combinations of more than two features), but as argued in the [PDP chapter](#pdp), only up to 2D makes sense, because higher interactions can't be visualized meaningfully.



*What happens with categorical features?*

Accumulated Local Effects need - by definition - an order within a feature to work.
That means that they also work for categorical features once you can decide on an order of the categories.
Some categorical features have a natural order, for example grades in school or hierarchy ranks in a company.
For those it might make sense to keep that order, but for other features like the season or color there is no obvious order.
Then there are some features inbetween like day of the week (Monday, Tuesday, ...) which look like they have an order, but where do we start (monday? sunday?) and for some prediction model Mondays might be more similar to Thursdays than to Tuesdays.


TODO: Proofread PDP chapter


### Examples

TODO: Add comparisons with partial dependence plots 
TODO: Intro text for ALE example and some interpretation

Let's return to the regression example, in which we predict [bike rentals](#bike-data).
We first fit a machine learning model on the dataset, for which we want to analyse the partial dependencies.
In this case, we fitted a RandomForest to predict the bike rentals and use the partial dependence plot to visualize the relationships the model learned.
The influence of the weather features on the predicted bike counts:

```{r pdp-bike, fig.cap = 'Partial dependence plots for the rental bike prediction model and different weather measurements (Temperature, Humidity, Windspeed). The biggest differences can be seen in the temperature: On average, the hotter the more bikes are rented, until 20C degrees, where it stays the same also for hotter temperatures and drops a bit again towards 30C degrees. The marks on the x-axis indicate the distribution of the feature in the data.'}
data(bike)
library("mlr")
library("ggplot2")

bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.ctree'), bike.task)

pred.bike = Predictor$new(mod.bike, data = bike)
```

```{r ale-bike-prep, fig.keep = FALSE}
y.fun = function(X.model, newdata) {
 X.model$predict(newdata)[[1]]
}

limits = c(-1100, 700)

ale1.df = data.frame(ALEPlot(bike, pred.bike, J = "temp", pred.fun = y.fun))
ale1 = ggplot(ale1.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("temp") + scale_y_continuous(expression(hat(y)), limits = limits)
ale2.df = data.frame(ALEPlot(bike, pred.bike, J = "hum", pred.fun = y.fun))
ale2 = ggplot(ale2.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("hum") + scale_y_continuous(expression(hat(y)), limits = limits)

ale3.df = data.frame(ALEPlot(bike, pred.bike, J = "windspeed", pred.fun = y.fun))
ale3 = ggplot(ale3.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("windspeed") + scale_y_continuous(expression(hat(y)), limits = limits)

```

```{r ale-bike, fig.cap = "ALE  plots for the rental bike prediction model and different weather measurements (Temperature, Humidity, Windspeed). The overall tendencies are similar to partial dependence plots. One difference is that the effects are centered at 0. For very high temperature or humidity, the ALE and PD plot differ. The ALE plot detects much larger negative effects on the predicted bike rentals when it's very hot or very humid."}
gridExtra::grid.arrange(ale1, ale2, ale3, ncol = 3)
```


```{r ale-bike, fig.cap = ""}
kable(cor(bike[c("temp", "hum", "windspeed")]))
```

```{r pdp-bike-compare, fig.cap = 'PDP fails to identify for humidity and temperature the big drop in prediction when at high values. That is humid weather or hot weather.'}
pdp = Partial$new(pred.bike, "temp", ice = FALSE) 
p1 = pdp$plot() +  scale_x_continuous('Temperature', limits = c(0, NA)) + scale_y_continuous('Predicted number of bike rentals', limits = c(3000, 5500))
pdp$set.feature("hum")
p2 = pdp$plot() +  scale_x_continuous('Humidity', limits = c(0, NA)) + scale_y_continuous('', limits = c(3000, 5500))
pdp$set.feature("windspeed")
p3 = pdp$plot() + scale_x_continuous('Windspeed', limits = c(0, NA)) + scale_y_continuous('', limits = c(3000, 5500))

gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

Next we look at the second-order effect of humidity and temperature on the predicted number of bike rentals. 
Remember that the second-order effect is the additional interaction effect of the two features and does not cover the first-order effects. 
This means that you won't see the for example the trend that high humidity causes less rented bikes.

```{r ale-bike-2d, fig.cap = 'Accumulated Local Effects Plot for the second-order effect of humidity and temperature on the predicted number of bike rentals. Yellow color indicates above average and red color below average number of predicted bike rentals. The plot reveals an interaction between temperature and humidity: Hot and humid weather additionally decreases the number of bike rentals (keep in mind that both first-order effects of humidity and temperature also say that the number of bike rentals are decreased when hot and humid), but hot and dry or humid and cold weather actually increases the expected number of bike rentals. TODO: Adapt '}
x = ALEPlot::ALEPlot(bike, pred.bike, J = c("hum", "temp"), pred.fun = y.fun, K = 100)
```



```{r ale-cervical-prep, fig.keep = FALSE, include = FALSE}
data(cervical)
cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
mod = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', id = 'cervical-rf', predict.type = 'prob'), cervical.task)

pred.cervical = Predictor$new(mod, data = cervical, class = "Cancer")
ale1.df = data.frame(ALEPlot(cervical, pred.cervical, J = "Age", pred.fun = y.fun))
ale1 = ggplot(ale1.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("Age") + scale_y_continuous(expression(hat(y))) 

ale2.df = data.frame(ALEPlot(cervical, pred.cervical, J = "IUD..years.", pred.fun = y.fun))
ale2 = ggplot(ale2.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous(".IUD..years.") + scale_y_continuous(expression(hat(y))) 

```

```{r ale-cervical, fig.cap = ""}
gridExtra::grid.arrange(ale1, ale2, ncol = 2)
```

```{r ale-cervical-cor, fig.cap = ''}
kable(cor(cervical[c("Age", "IUD..years.")]))
```

```{r pdp-cervical-compare, fig.cap = ''}
pdp = Partial$new(pred.cervical, "Age", ice = FALSE) 
p1 = pdp$plot() + 
  scale_x_continuous(limits = c(0, NA)) + 
  scale_y_continuous('Predicted cancer probability', limits = c(0.03, 0.15))
pdp$set.feature("IUD..years.")
p2 = pdp$plot() + 
  scale_x_continuous(limits = c(0, NA)) + 
  scale_y_continuous('', limits = c(0.03, 0.15))

gridExtra::grid.arrange(p1, p2, ncol = 2)
```


```{r ale-cervical-2d, fig.cap = 'Accumulated Local Effects Plot for the second-order effect of number of pregnancies and the age on the predicted risk of cancer.'}
ALEPlot(cervical, pred.cervical, J = which(names(cervical) %in% c("Num.of.pregnancies", "Age")), pred.fun = y.fun, K = 50)
```


### Advantages
- ALE plots also work when features are correlated.
Partial dependence plots fail in this scenario because they marginalize over unlikely or even physically impossible combinations of feature values.
- ALE plots are faster to compute than PDPs and they scale with O(n), since the maximum number of intervals is the number of instances, amounting to one interval per data instance.
The PDP needs n times number of grid points estimations. 
For, let's say, 20 grid points, this would be 20x more predictions the model has to make.
- The interpretation of ALE plots is also clear: Conditional on a given x value, the effect of the feature on the prediction can be read from the ALE plot.

### Disadvantages
- ALE plots can become a bit wobbly (many small ups and downs) with a high number of intervals
- No perfect solution for setting the number of intervals. 
If the number is too small, than the ALE plots will not be very accurate. 
If the number is too high, it can become wobbly.
- ALE plots are centered at zero.
This makes their interpretation nice, because the value at each point of the ALE curve is the difference to the mean prediction.
- Unlike PDPs, ALE plots don't have anything equivalent like ICE curves.
For PDPs, ICE curves are greate because they can reveal effect heterogeneity in the data, meaning that the effect of a feature looks different for subsets of the data.
For ALE plot you could only check per interval if the effect differs between the instances, but each interval has different instances so it is not equivalent to ICE curves.
- Each estimation of a local effect in an interval uses a different number of data instances.
This means that not all estimates are equally accurate (but still the best possible estimate).
- The choice to use the second-order effect for the 2D ALE computation is a bit annoying to interpret, since you always have to hold in your mind the first-order effects.
It's tempting to read the heat maps as the total effect of the two features, but it is only the additional effect of the interaction.
The pure second-order effect is interesting for discovering and exploring interactions, but for the interpretation of how the effect looks like, I think it more useful when the first-order effect is integrated into the heatmap.





[^ALE]: Apley, D. W. (n.d.). Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models, 1â€“36. Retrieved from https://arxiv.org/ftp/arxiv/papers/1612/1612.08468.pdf