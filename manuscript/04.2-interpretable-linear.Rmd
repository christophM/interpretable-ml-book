```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->

## Linear Regression {#limo}

Model linear regression memprediksi target sebagai jumlah bobot dari input fitur.
Linearitas dari hubungan yang dipelajari membuat interpretasi menjadi mudah.
Model linear regression telah lama digunakan oleh ahli statistik, ilmuwan komputer, dan orang lain yang menangani masalah kuantitatif.

Model linier dapat digunakan untuk memodelkan ketergantungan target regresi y pada beberapa fitur x.
Hubungan yang dipelajari adalah linier dan dapat ditulis untuk satu contoh i sebagai berikut:

$$y=\beta_{0}+\beta_{1}x_{1}+\ldots+\beta_{p}x_{p}+\epsilon$$

Hasil yang diprediksi dari sebuah instance adalah jumlah tertimbang dari fitur p-nya.
Beta ($\beta_{j}$) mewakili bobot atau koefisien fitur yang dipelajari.
Bobot pertama dalam penjumlahan ($\beta_0$) disebut intersep dan tidak dikalikan dengan fitur.
Epsilon ($\epsilon$) adalah kesalahan yang masih kita buat, yaitu perbedaan antara prediksi dan hasil aktual.
Kesalahan ini diasumsikan mengikuti distribusi Gaussian, yang berarti bahwa kita membuat kesalahan dalam arah negatif dan positif dan membuat banyak kesalahan kecil dan sedikit kesalahan besar.

Berbagai metode dapat digunakan untuk memperkirakan bobot optimal.
Metode least squares biasa biasanya digunakan untuk menemukan bobot yang meminimalkan perbedaan kuadrat antara hasil yang sebenarnya dan yang diperkirakan:

$$\hat{\boldsymbol{\beta}}=\arg\!\min_{\beta_0,\ldots,\beta_p}\sum_{i=1}^n\left(y^{(i)}-\left(\beta_0+\sum_{j=1}^p\beta_jx^{(i)}_{j}\right)\right)^{2}$$

Kami tidak akan membahas secara rinci bagaimana bobot optimal dapat ditemukan, tetapi jika Anda tertarik, Anda dapat membaca bab 3.2 dari buku "The Elements of Statistical Learning" (Friedman, Hastie and Tibshirani 2009)[^Hastie] atau salah satu dari sumber daya online lainnya tentang model linear regression.

Keuntungan terbesar dari model linear regression adalah linieritas:
Itu membuat prosedur estimasi sederhana dan, yang paling penting, persamaan linier ini memiliki interpretasi yang mudah dipahami pada tingkat modular (yaitu bobot).
Inilah salah satu alasan utama mengapa model linier dan semua model serupa begitu tersebar luas di bidang akademik seperti kedokteran, sosiologi, psikologi, dan banyak bidang penelitian kuantitatif lainnya.
Misalnya, di bidang medis, tidak hanya penting untuk memprediksi hasil klinis pasien, tetapi juga untuk mengukur pengaruh obat dan pada saat yang sama mempertimbangkan jenis kelamin, usia, dan fitur lainnya dengan cara yang dapat ditafsirkan. .

Bobot yang diperkirakan datang dengan confidence intervals.
Confidence intervals adalah rentang untuk perkiraan bobot yang mencakup bobot "sebenarnya" dengan keyakinan tertentu.
Misalnya, confidence intervals 95% untuk bobot 2 dapat berkisar dari 1 hingga 3.
Interpretasi dari interval ini adalah:
Jika kita mengulangi estimasi 100 kali dengan data sampel baru, confidence intervals akan mencakup bobot sebenarnya dalam 95 dari 100 kasus, mengingat model linear regression adalah model yang benar untuk data.

Apakah model tersebut merupakan model yang “benar” tergantung pada apakah hubungan dalam data memenuhi asumsi tertentu, yaitu linieritas, normalitas, homoskedastisitas, independensi, fitur tetap, dan tidak adanya multikolinearitas.

**Linieritas**
Model linear regression memaksa prediksi menjadi kombinasi linier fitur, yang merupakan kekuatan terbesar dan keterbatasan terbesarnya.
Linearitas mengarah pada interpretable models.
Efek linier mudah diukur dan dijelaskan.
Mereka adalah aditif, sehingga mudah untuk memisahkan efeknya.
Jika Anda mencurigai interaksi fitur atau asosiasi nonlinier dari fitur dengan nilai target, Anda dapat menambahkan istilah interaksi atau menggunakan regression splines.

**Normal**
Diasumsikan bahwa hasil target yang diberikan fitur mengikuti distribusi normal.
Jika asumsi ini dilanggar, perkiraan confidence intervals dari bobot fitur tidak valid.

**Homoscedasticity** (varian konstan)
Varians dari istilah kesalahan diasumsikan konstan di seluruh ruang fitur.
Misalkan Anda ingin memprediksi nilai sebuah rumah dengan luas ruang tamu dalam meter persegi.
Anda memperkirakan model linier yang mengasumsikan bahwa, terlepas dari ukuran rumah, kesalahan di sekitar respons yang diprediksi memiliki varians yang sama.
Asumsi ini sering dilanggar dalam kenyataan.
Dalam contoh rumah, masuk akal bahwa varian istilah kesalahan di sekitar harga yang diprediksi lebih tinggi untuk rumah yang lebih besar, karena harga lebih tinggi dan ada lebih banyak ruang untuk fluktuasi harga.
Misalkan kesalahan rata-rata (selisih antara prediksi dan harga aktual) dalam model linear regression Anda adalah 50.000 Euro.
Jika Anda mengasumsikan homoskedastisitas, Anda berasumsi bahwa kesalahan rata-rata 50.000 adalah sama untuk rumah yang harganya 1 juta dan untuk rumah yang harganya hanya 40.000.
Ini tidak masuk akal karena itu berarti kita bisa mengharapkan harga rumah negatif.

**Kemerdekaan**
Diasumsikan bahwa setiap instance tidak tergantung pada instance lainnya.
Jika Anda melakukan pengukuran berulang, seperti beberapa tes darah per pasien, titik data tidak independen.
Untuk data dependen, Anda memerlukan model linear regression khusus, seperti model efek campuran atau GEE.
Jika Anda menggunakan model linear regression "normal", Anda mungkin menarik kesimpulan yang salah dari model tersebut.

**Fitur tetap**
Fitur input dianggap "tetap".
Tetap berarti bahwa mereka diperlakukan sebagai "konstanta yang diberikan" dan bukan sebagai variabel statistik.
Ini berarti bahwa mereka bebas dari kesalahan pengukuran.
Ini adalah asumsi yang agak tidak realistis.
Namun, tanpa asumsi itu, Anda harus menyesuaikan model kesalahan pengukuran yang sangat kompleks yang menjelaskan kesalahan pengukuran fitur input Anda.
Dan biasanya Anda tidak ingin melakukan itu.

**Tidak adanya multikolinearitas**
Anda tidak ingin fitur berkorelasi kuat, karena ini mengacaukan estimasi bobot.
Dalam situasi di mana dua fitur berkorelasi kuat, menjadi masalah untuk memperkirakan bobot karena efek fitur bersifat aditif dan menjadi tidak dapat ditentukan fitur mana yang berkorelasi untuk mengatribusikan efek.


### Interpretation
Interpretasi bobot dalam model linear regression tergantung pada jenis fitur yang sesuai.

- Fitur numerik: Meningkatkan fitur numerik satu unit mengubah hasil yang diperkirakan berdasarkan bobotnya.
Contoh fitur numerik adalah ukuran rumah.
- Fitur biner: Fitur yang mengambil salah satu dari dua kemungkinan nilai untuk setiap instance.
Contohnya adalah fitur "Rumah dilengkapi dengan taman".
Salah satu nilai dihitung sebagai kategori referensi (dalam beberapa bahasa pemrograman yang dikodekan dengan 0), seperti "Tanpa taman".
Mengubah fitur dari kategori referensi ke kategori lain akan mengubah perkiraan hasil berdasarkan bobot fitur.
- Fitur kategoris dengan beberapa kategori:
Fitur dengan jumlah nilai yang mungkin tetap.
Contohnya adalah fitur "tipe lantai", dengan kemungkinan kategori "karpet", "laminasi" dan "parket".
Solusi untuk menangani banyak kategori adalah one-hot-encoding, artinya setiap kategori memiliki kolom binernya sendiri.
Untuk fitur kategoris dengan kategori L, Anda hanya memerlukan kolom L-1, karena kolom ke-L akan memiliki informasi yang berlebihan (misalnya ketika kolom 1 hingga L-1 semuanya memiliki nilai 0 untuk satu contoh, kita tahu bahwa fitur kategoris dari contoh ini mengambil kategori L).
Interpretasi untuk setiap kategori kemudian sama dengan interpretasi untuk fitur biner.
Beberapa bahasa, seperti R, memungkinkan Anda untuk mengkodekan fitur kategoris dengan berbagai cara, seperti [dijelaskan nanti dalam bab ini](#cat-code).
- Intersep $\beta_0$:
Intersep adalah bobot fitur untuk "fitur konstan", yang selalu 1 untuk semua instance.
Sebagian besar paket perangkat lunak secara otomatis menambahkan fitur "1" ini untuk memperkirakan intersep.
Interpretasinya adalah:
Untuk contoh dengan semua nilai fitur numerik nol dan nilai fitur kategoris pada kategori referensi, prediksi model adalah bobot intersep.
Interpretasi intersep biasanya tidak relevan karena instance dengan semua nilai fitur nol sering kali tidak masuk akal.
Interpretasi hanya bermakna ketika fitur telah distandarisasi (rata-rata nol, standar deviasi satu).
Kemudian intersep mencerminkan hasil yang diprediksi dari sebuah instance di mana semua fitur berada pada nilai rata-ratanya.

Interpretasi fitur dalam model linear regression dapat diotomatisasi dengan menggunakan templat teks berikut.

**Interpretasi Fitur Numerik**
Peningkatan fitur $x_{k}$ sebesar satu unit meningkatkan prediksi untuk y sebesar $\beta_k$ unit ketika semua nilai fitur lainnya tetap.

**Interpretasi Fitur Kategoris**

Mengubah fitur $x_{k}$ dari kategori referensi ke kategori lain meningkatkan prediksi untuk y sebesar $\beta_{k}$ ketika semua fitur lainnya tetap.

Pengukuran penting lainnya untuk menafsirkan model linier adalah pengukuran R-kuadrat.
R-kuadrat memberitahu Anda berapa banyak varians total hasil target Anda dijelaskan oleh model.
Semakin tinggi R-kuadrat, semakin baik model Anda menjelaskan data.
Rumus untuk menghitung R-kuadrat adalah:

$$R^2=1-SSE/SST$$

SSE adalah jumlah kuadrat dari istilah kesalahan:

$$SSE=\sum_{i=1}^n(y^{(i)}-\hat{y}^{(i)})^2$$ 

SST adalah jumlah kuadrat dari varians data:

$$SST=\sum_{i=1}^n(y^{(i)}-\bar{y})^2$$

SSE memberi tahu Anda berapa banyak varians yang tersisa setelah menyesuaikan model linier, yang diukur dengan perbedaan kuadrat antara nilai target yang diprediksi dan aktual.
SST adalah varians total dari hasil target.
R-kuadrat memberi tahu Anda seberapa banyak varians Anda dapat dijelaskan oleh model linier.
R-kuadrat berkisar antara 0 untuk model di mana model tidak menjelaskan data sama sekali dan 1 untuk model yang menjelaskan semua varian dalam data Anda.

Ada tangkapan, karena R-kuadrat meningkat dengan jumlah fitur dalam model, bahkan jika mereka tidak mengandung informasi tentang nilai target sama sekali.
Oleh karena itu, lebih baik menggunakan R-kuadrat yang disesuaikan, yang memperhitungkan jumlah fitur yang digunakan dalam model.
Perhitungannya adalah:

$$\bar{R}^2=1-(1-R^2)\frac{n-1}{n-p-1}$$

di mana p adalah jumlah fitur dan n jumlah instance.

Tidaklah berarti untuk menginterpretasikan model dengan R-kuadrat yang sangat rendah (disesuaikan), karena model seperti itu pada dasarnya tidak menjelaskan banyak varians.
Setiap interpretasi bobot tidak akan berarti.

**Fitur Penting**

Pentingnya suatu fitur dalam model linear regression dapat diukur dengan nilai absolut dari t-statistiknya.
T-statistik adalah perkiraan bobot yang diskalakan dengan kesalahan standarnya.

$$t_{\hat{\beta}_j}=\frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}$$

Mari kita periksa apa yang dikatakan rumus ini kepada kita:
feature importance meningkat dengan meningkatnya bobot.
Ini masuk akal.
Semakin banyak varians yang dimiliki bobot yang diperkirakan (= semakin tidak yakin kita tentang nilai yang benar), semakin tidak penting fitur tersebut.
Ini juga masuk akal.

###  Example

Dalam contoh ini, kami menggunakan model linear regression untuk memprediksi [jumlah sepeda sewaan](#bike-data) pada hari tertentu, berdasarkan informasi cuaca dan kalender.
Untuk interpretasi, kami menguji bobot regresi yang diperkirakan.
Fitur terdiri dari fitur numerik dan kategoris.
Untuk setiap fitur, tabel menunjukkan bobot estimasi, standar error estimasi (SE), dan nilai absolut t-statistik (|t|).

```{r linear_model}

data(bike)
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)

mod = lm(y ~ ., data = dat, x = TRUE)
lm_summary = summary(mod)$coefficients

lm_summary_print = lm_summary
lm_summary_print[,'t value'] = abs(lm_summary_print[,'t value'])
rownames(lm_summary_print) = pretty_rownames(rownames(lm_summary_print))

kable(lm_summary_print[,c('Estimate', 'Std. Error', 't value')], digits = 1, col.names = c('Weight', 'SE', "|t|"))
```

Interpretasi fitur numerik (suhu):
Peningkatan suhu sebesar 1 derajat Celcius meningkatkan perkiraan jumlah sepeda sebesar `r sprintf('%.1f', lm_summary_print['temp', 'Estimate'])`, ketika semua fitur lainnya tetap.

Interpretasi fitur kategoris ("cuaca"):
Perkiraan jumlah sepeda adalah `r sprintf('%.1f', lm_summary_print['weathersitRAIN/SNOW/STORM', 'Estimate'])` lebih rendah saat hujan, bersalju, atau badai, dibandingkan dengan cuaca baik -- sekali lagi dengan asumsi bahwa semua fitur lainnya tidak berubah.
Saat cuaca berkabut, jumlah sepeda yang diprediksi adalah `r sprintf('%.1f', lm_summary_print['weathersitMISTY', 'Estimate'])` lebih rendah dibandingkan dengan cuaca baik, mengingat semua fitur lainnya tetap sama.

Semua interpretasi selalu datang dengan catatan kaki bahwa "semua fitur lainnya tetap sama".
Hal ini karena sifat model linear regression.
Target yang diprediksi adalah kombinasi linier dari fitur berbobot.
Estimasi persamaan linier adalah hyperplane dalam fitur/ruang target (garis sederhana dalam kasus fitur tunggal).
Bobot menentukan kemiringan (gradien) hyperplane di setiap arah.
Sisi baiknya adalah bahwa aditif mengisolasi interpretasi efek fitur individual dari semua fitur lainnya.
Itu dimungkinkan karena semua efek fitur (= bobot kali nilai fitur) dalam persamaan digabungkan dengan plus.
Di sisi buruk, interpretasi mengabaikan distribusi fitur bersama.
Meningkatkan satu fitur, tetapi tidak mengubah yang lain, dapat menyebabkan titik data yang tidak realistis atau setidaknya tidak mungkin.
Misalnya menambah jumlah kamar mungkin tidak realistis tanpa juga menambah ukuran rumah.

### Visual Interpretation
Berbagai visualisasi membuat model linear regression mudah dan cepat dipahami oleh manusia.
40

#### Weight Plot
Informasi tabel bobot (perkiraan bobot dan varians) dapat divisualisasikan dalam weight plot.
Plot berikut menunjukkan hasil dari model linear regression sebelumnya.

```{r linear-weights-plot, fig.cap="Weights are displayed as points and the 95% confidence intervals as lines."}
coef_plot(mod) + scale_y_discrete("")
```

Weight plot menunjukkan bahwa cuaca hujan/bersalju/badai memiliki efek negatif yang kuat pada prediksi jumlah sepeda.
Bobot fitur hari kerja mendekati nol dan nol termasuk dalam interval 95%, yang berarti pengaruhnya tidak signifikan secara statistik.
Beberapa confidence intervals sangat pendek dan perkiraan mendekati nol, namun efek fitur signifikan secara statistik.
Suhu adalah salah satu kandidat tersebut.
Masalah dengan weight plot adalah bahwa fitur diukur pada skala yang berbeda.
Sedangkan untuk cuaca perkiraan berat mencerminkan perbedaan antara cuaca baik dan hujan/badai/salju, untuk suhu hanya mencerminkan kenaikan 1 derajat Celcius.
Anda dapat membuat perkiraan bobot lebih sebanding dengan menskalakan fitur (rata-rata nol dan simpangan baku satu) sebelum memasang model linier.

#### Effect Plot
Bobot model linear regression dapat dianalisis secara lebih bermakna ketika bobot tersebut dikalikan dengan nilai fitur yang sebenarnya.
Bobot tergantung pada skala fitur dan akan berbeda jika Anda memiliki fitur yang mengukur mis. tinggi seseorang dan Anda beralih dari meter ke sentimeter.
Bobotnya akan berubah, tetapi efek aktual dalam data Anda tidak akan berubah.
Penting juga untuk mengetahui distribusi fitur Anda dalam data, karena jika Anda memiliki varians yang sangat rendah, berarti hampir semua instance memiliki kontribusi yang sama dari fitur ini.
Effect plot dapat membantu Anda memahami seberapa besar kontribusi kombinasi bobot dan fitur terhadap prediksi dalam data Anda.
Mulailah dengan menghitung efek, yang merupakan bobot per fitur dikalikan nilai fitur dari sebuah instance:

$$\text{effect}_{j}^{(i)}=w_{j}x_{j}^{(i)}$$

Efeknya dapat divisualisasikan dengan boxplot.
Sebuah kotak di boxplot berisi rentang efek untuk setengah dari data Anda (25% hingga 75% kuantil efek).
Garis vertikal di dalam kotak adalah efek median, yaitu 50% dari contoh memiliki efek yang lebih rendah dan setengah lainnya lebih tinggi pada prediksi.
Garis horizontal meluas ke $\pm1.5\text{IQR}/\sqrt{n}$, dengan IQR menjadi rentang antar kuartil (75% kuantil dikurangi 25% kuantil).
Titik-titik adalah outlier.
Efek fitur kategoris dapat diringkas dalam satu boxplot, dibandingkan dengan weight plot, di mana setiap kategori memiliki barisnya sendiri.

```{r linear-effects, fig.cap="The feature effect plot shows the distribution of effects (= feature value times feature weight) across the data per feature."}
effect_plot(mod, dat) + scale_x_discrete("")
```

Kontribusi terbesar terhadap perkiraan jumlah sepeda sewaan berasal dari fitur suhu dan fitur hari, yang menangkap tren penyewaan sepeda dari waktu ke waktu.
Suhu memiliki rentang yang luas seberapa besar kontribusinya terhadap prediksi.
Fitur tren hari berubah dari nol menjadi kontribusi positif yang besar, karena hari pertama dalam kumpulan data (01.01.2011) memiliki efek tren yang sangat kecil dan perkiraan bobot untuk fitur ini adalah positif (`r sprintf('%.2f', lm_summary_print['days_since_2011', 'Estimate'])`).
Ini berarti bahwa efeknya meningkat setiap hari dan tertinggi untuk hari terakhir dalam kumpulan data (31/12/2012).
Perhatikan bahwa untuk efek dengan bobot negatif, instance dengan efek positif adalah yang memiliki nilai fitur negatif.
Misalnya, hari-hari dengan efek negatif kecepatan angin yang tinggi adalah hari-hari dengan kecepatan angin yang tinggi.

### Explain Individual Predictions

```{r linear-effects-single-preparation}
i = 6
effects = get_effects(mod, dat)
predictions = predict(mod)

effects_i = tidyr::gather(effects[i, ])
predictions_mean = mean(predictions)
# For proper indexing, names have to be removed
names(predictions) = NULL
pred_i = predictions[i]
```

Seberapa besar kontribusi setiap fitur dari sebuah instans terhadap prediksi?
Ini dapat dijawab dengan menghitung efek untuk contoh ini.
Interpretasi dari efek spesifik-instance hanya masuk akal dibandingkan dengan distribusi efek untuk setiap fitur.
Kami ingin menjelaskan prediksi model linier untuk instance ke-'r ke-i dari dataset sepeda.
Instance memiliki nilai fitur berikut.

```{r linear-effects-single-table}
df = data.frame(feature = colnames(bike), value = t(bike[i,]))
colnames(df) = c("feature", "value")
kable(df, col.names = c("Feature", "Value"), row.names = FALSE)
```

Untuk mendapatkan efek fitur dari instance ini, kita harus mengalikan nilai fiturnya dengan bobot yang sesuai dari model linear regression.
Untuk nilai "`r df["workingday", "value"]`" of feature "`r df["workingday", "feature"]`", efeknya adalah, `r round(lm_summary_print[paste(df["workingday", "feature"], df["workingday", "value"], sep = ""), "Estimate"], 1)`.
Untuk suhu `r round(as.numeric(as.character(df["temp", "value"])), 1)` derajat Celcius, efeknya adalah `r round(as.numeric(as.character(df["temp", "value"])) * lm_summary_print[as.character(df["temp", "feature"]), "Estimate"], 1)`.
Kami menambahkan efek individu ini sebagai persilangan ke effect plot, yang menunjukkan kepada kami distribusi efek dalam data.
Hal ini memungkinkan kita untuk membandingkan efek individu dengan distribusi efek dalam data.

```{r linear-effects-single, fig.cap="The effect plot for one instance shows the effect distribution and highlights the effects of the instance of interest."}
i = 6
effects = get_effects(mod, dat)
predictions = predict(mod)

effects_i = tidyr::gather(effects[i, ])
predictions_mean = mean(predictions)
# For proper indexing, names have to be removed
names(predictions) = NULL
pred_i = predictions[i]

effect_plot(mod, dat) +
  geom_point(aes(x=key, y=value), color = 'red', data = effects_i, shape = 4, size=4) +
  scale_x_discrete("") +
  ggtitle(sprintf('Predicted value for instance: %.0f\nAverage predicted value: %.0f\nActual value: %.0f', pred_i, predictions_mean, y[i]))
```

Jika kita merata-ratakan prediksi untuk instance data pelatihan, kita mendapatkan rata-rata `r round(predictions_mean, 0)`.
Sebagai perbandingan, prediksi instance `r i`-th kecil, karena hanya sewa sepeda `r round(pred_i, 0)` yang diprediksi.
Effect plot mengungkapkan alasannya.
Boxplots menunjukkan distribusi efek untuk semua instance dari kumpulan data, persilangan menunjukkan efek untuk instance `r i`-th.
Instans `ri`-th memiliki efek suhu rendah karena pada hari ini suhunya `r round(X[i, 'temp'],0)` derajat, yang lebih rendah dibandingkan hari-hari lainnya (dan ingat bahwa berat fitur suhu positif).
Selain itu, efek fitur tren "hari_sejak_2011" kecil dibandingkan dengan instans data lainnya karena instans ini berasal dari awal 2011 (`r  X[i, 'days_since_2011']` hari) dan fitur tren juga memiliki bobot positif.


### Encoding of Categorical Features {#cat-code}

Ada beberapa cara untuk mengkodekan fitur kategoris, dan pilihan mempengaruhi interpretasi bobot.

Standar dalam model linear regression adalah treatment coding, yang cukup dalam banyak kasus.
Menggunakan pengkodean yang berbeda bermuara pada pembuatan matriks (desain) yang berbeda dari satu kolom dengan fitur kategoris.
Bagian ini menyajikan tiga pengkodean yang berbeda, tetapi masih banyak lagi.
Contoh yang digunakan memiliki enam contoh dan fitur kategoris dengan tiga kategori.
Untuk dua contoh pertama, fitur mengambil kategori A;
untuk contoh tiga dan empat, kategori B;
dan untuk dua contoh terakhir, kategori C.

**Treatment coding**

Dalam treatment coding, bobot per kategori adalah perkiraan perbedaan prediksi antara kategori yang sesuai dan kategori referensi.
Intersep dari model linier adalah rata-rata dari kategori referensi (ketika semua fitur lainnya tetap sama).
Kolom pertama dari matriks desain adalah intersep, yang selalu 1.
Kolom dua menunjukkan apakah instance i termasuk dalam kategori B, kolom tiga menunjukkan apakah instance i termasuk dalam kategori C.
Tidak diperlukan kolom untuk kategori A, karena persamaan linier akan menjadi terlalu ditentukan dan tidak ada solusi unik untuk bobot yang dapat ditemukan.
Cukup untuk mengetahui bahwa sebuah instance tidak termasuk dalam kategori B atau C.

Matriks fitur: $$\begin{pmatrix}1&0&0\\1&0&0\\1&1&0\\1&1&0\\1&0&1\\1&0&1\\\end{pmatrix}$$

**Effect coding**

Bobot per kategori adalah perkiraan perbedaan y dari kategori yang sesuai dengan rata-rata keseluruhan (mengingat semua fitur lain adalah nol atau kategori referensi).
Kolom pertama digunakan untuk memperkirakan intersep.
Bobot $\beta_{0}$ yang terkait dengan intersep mewakili rata-rata keseluruhan dan $\beta_{1}$, bobot untuk kolom dua, adalah perbedaan antara rata-rata keseluruhan dan kategori B.
Efek total kategori B adalah $\beta_{0}+\beta_{1}$.
Interpretasi untuk kategori C setara.
Untuk kategori referensi A, $-(\beta_{1}+\beta_{2})$ adalah selisih dari rata-rata keseluruhan dan $\beta_{0}-(\beta_{1}+\beta_{2})$ efek keseluruhan.

Matriks fitur: $$\begin{pmatrix}1&-1&-1\\1&-1&-1\\1&1&0\\1&1&0\\1&0&1\\1&0&1\\\end{pmatrix}$$

**Dummy coding**

$\beta$ per kategori adalah perkiraan nilai rata-rata y untuk setiap kategori (diberikan semua nilai fitur lainnya adalah nol atau kategori referensi).
Perhatikan bahwa intersep telah dihilangkan di sini sehingga solusi unik dapat ditemukan untuk bobot model linier.
Cara lain untuk mengurangi masalah multikolinearitas ini adalah dengan mengabaikan salah satu kategori.

Matriks fitur: $$\begin{pmatrix}1&0&0\\1&0&0\\0&1&0\\0&1&0\\0&0&1\\0&0&1\\\end{pmatrix}$$

Jika Anda ingin menyelami lebih dalam tentang berbagai pengkodean fitur kategoris, periksa [halaman web ikhtisar ini](http://stats.idre.ucla.edu/r/library/r-library-contrast-coding -sistem-untuk-variabel-kategoris/) dan
[posting blog ini](http://heidiseibold.github.io/page7/).


### Do Linear Models Create Good Explanations?

Dilihat dari atribut yang merupakan penjelasan yang baik, seperti yang disajikan [dalam bab Penjelasan Ramah Manusia](#good-explanation), model linier tidak membuat penjelasan terbaik.
Mereka kontras, tetapi contoh referensi adalah titik data di mana semua fitur numerik adalah nol dan fitur kategoris berada pada kategori referensi mereka.
Ini biasanya merupakan contoh artifisial dan tidak berarti yang tidak mungkin terjadi dalam data atau kenyataan Anda.
Ada pengecualian:
Jika semua fitur numerik dipusatkan rata-rata (fitur dikurangi rata-rata fitur) dan semua fitur kategoris diberi kode efek, instance referensi adalah titik data di mana semua fitur mengambil nilai fitur rata-rata.
Ini mungkin juga merupakan titik data yang tidak ada, tetapi setidaknya mungkin lebih mungkin atau lebih bermakna.
Dalam hal ini, bobot kali nilai fitur (efek fitur) menjelaskan kontribusi terhadap hasil prediksi yang kontras dengan "contoh rata-rata".
Aspek lain dari penjelasan yang baik adalah selektivitas, yang dapat dicapai dalam model linier dengan menggunakan lebih sedikit fitur atau dengan melatih sparse linear models.
Tetapi secara default, model linier tidak membuat penjelasan selektif.
Model linier menciptakan penjelasan yang benar, selama persamaan linier adalah model yang sesuai untuk hubungan antara fitur dan hasil.
Semakin banyak non-linearitas dan interaksinya, semakin tidak akurat model liniernya dan semakin tidak jujur ​​penjelasannya.
Linearitas membuat penjelasan lebih umum dan sederhana.
Sifat linier dari model, saya percaya, adalah faktor utama mengapa orang menggunakan model linier untuk menjelaskan hubungan.


###  Sparse Linear Models {#sparse-linear}

Contoh model linier yang saya pilih semuanya terlihat bagus dan rapi, bukan?
Namun pada kenyataannya Anda mungkin tidak hanya memiliki beberapa fitur, tetapi ratusan atau ribuan.
Dan model linear regression Anda?
Interpretabilitas menurun.
Anda bahkan mungkin menemukan diri Anda dalam situasi di mana ada lebih banyak fitur daripada instance, dan Anda tidak dapat menyesuaikan model linier standar sama sekali.
Kabar baiknya adalah ada cara untuk memperkenalkan sparsity (= sedikit fitur) ke dalam model linier.

#### Lasso {#lasso}

Lasso adalah cara otomatis dan nyaman untuk memperkenalkan sparsity ke dalam model linear regression.
Lasso adalah singkatan dari "least absolute shrinkage and selection operator" dan, ketika diterapkan dalam model linear regression, melakukan seleksi fitur dan regularisasi bobot fitur yang dipilih.
Mari kita pertimbangkan masalah minimalisasi yang bobotnya dioptimalkan:

$$min_{\boldsymbol{\beta}}\left(\frac{1}{n}\sum_{i=1}^n(y^{(i)}-x_i^T\boldsymbol{\beta})^2\right)$$

Lasso menambahkan istilah untuk masalah optimasi ini.

$$min_{\boldsymbol{\beta}}\left(\frac{1}{n}\sum_{i=1}^n(y^{(i)}-x_{i}^T\boldsymbol{\beta})^2+\lambda||\boldsymbol{\beta}||_1\right)$$

Istilah $||\boldsymbol{\beta}||_1$, norma-L1 dari vektor fitur, mengarah ke penalty bobot besar.
Karena norma-L1 digunakan, banyak bobot menerima perkiraan 0 dan yang lainnya menyusut.
Parameter lambda ($\lambda$) mengontrol kekuatan efek regularisasi dan biasanya disetel dengan validasi silang.
Terutama ketika lambda besar, banyak bobot menjadi 0.
Bobot fitur dapat divisualisasikan sebagai fungsi dari istilah penalty lambda.
Setiap bobot fitur diwakili oleh kurva pada gambar berikut.

```{r lasso-path, fig.cap="With increasing penalty of the weights, fewer and fewer features receive a non-zero weight estimate. These curves are also called regularization paths. The number above the plot is the number of non-zero weights."}
library("glmnet")
X.d = model.matrix(y ~ . -1, data = X)
l.mod = glmnet(X.d, y)
plot(l.mod,  xvar = "lambda", ylab="Weights")
```

Nilai apa yang harus kita pilih untuk lambda?
Jika Anda melihat istilah penalty sebagai parameter penyetelan, maka Anda dapat menemukan lambda yang meminimalkan kesalahan model dengan validasi silang.
Anda juga dapat mempertimbangkan lambda sebagai parameter untuk mengontrol interpretasi model.
Semakin besar penaltynya, semakin sedikit fitur yang ada dalam model (karena bobotnya nol) dan semakin baik model tersebut dapat diinterpretasikan.

**Example with Lasso**

Kami akan memprediksi persewaan sepeda menggunakan Lasso.
Kami mengatur jumlah fitur yang ingin kami miliki dalam model sebelumnya.
Mari kita atur dulu angkanya menjadi 2 fitur:

```{r lasso_effects}
extract.glmnet.effects = function(betas, best.index) {
  data.frame(beta = betas[, best.index])
}
n.features = apply(l.mod$beta, 2, function(x){sum(x!=0)})
kable(extract.glmnet.effects(l.mod$beta, max(which(n.features == 2))), col.names = "Weight", digits = 2)
```

Dua fitur pertama dengan bobot bukan nol di jalur Lasso adalah suhu ("temp") dan tren waktu ("days_since_2011").

Sekarang, mari kita pilih 5 fitur:

```{r lasso_effects2}
kable(extract.glmnet.effects(l.mod$beta, max(which(n.features == 5))), col.names = "Weight", digits = 2)
```


Perhatikan bahwa bobot untuk "temp" dan "days_since_2011" berbeda dari model dengan dua fitur.
Alasan untuk ini adalah bahwa dengan mengurangi lambda bahkan fitur yang sudah "dalam" model dihukum lebih sedikit dan mungkin mendapatkan bobot absolut yang lebih besar.
Interpretasi bobot Lasso sesuai dengan interpretasi bobot dalam model linear regression.
Anda hanya perlu memperhatikan apakah fiturnya terstandarisasi atau tidak, karena ini mempengaruhi bobotnya.
Dalam contoh ini, fitur distandarisasi oleh perangkat lunak, tetapi bobot secara otomatis diubah kembali agar kami sesuai dengan skala fitur asli.

**Metode lain untuk sparsity dalam model linier**

Spektrum metode yang luas dapat digunakan untuk mengurangi jumlah fitur dalam model linier.

Metode pra-pemrosesan:

- Fitur yang dipilih secara manual:
Anda selalu dapat menggunakan pengetahuan ahli untuk memilih atau membuang beberapa fitur.
Kelemahan besar adalah tidak dapat diotomatisasi dan Anda harus memiliki akses ke seseorang yang memahami data.
- Pilihan univariat:
Contohnya adalah correlation coefficient.
Anda hanya mempertimbangkan fitur yang melebihi ambang batas korelasi tertentu antara fitur dan target.
Kerugiannya adalah hanya mempertimbangkan fitur secara individual.
Beberapa fitur mungkin tidak menunjukkan korelasi sampai model linier memperhitungkan beberapa fitur lainnya.
Yang akan Anda lewatkan dengan metode univariate selection.

Metode step-wise:

- forward selection:
Sesuaikan model linier dengan satu fitur.
Lakukan ini dengan setiap fitur.
Pilih model yang bekerja paling baik (misalnya R-kuadrat tertinggi).
Sekarang sekali lagi, untuk fitur yang tersisa, sesuaikan versi berbeda dari model Anda dengan menambahkan setiap fitur ke model terbaik Anda saat ini.
Pilih salah satu yang berkinerja terbaik.
Lanjutkan hingga beberapa kriteria tercapai, seperti jumlah fitur maksimum dalam model.
- backward selection:
Mirip dengan seleksi maju.
Tetapi alih-alih menambahkan fitur, mulailah dengan model yang berisi semua fitur dan coba fitur mana yang harus Anda hapus untuk mendapatkan peningkatan kinerja tertinggi.
Ulangi ini sampai beberapa kriteria berhenti tercapai.

Saya sarankan menggunakan Lasso, karena dapat diotomatisasi, mempertimbangkan semua fitur secara bersamaan, dan dapat dikontrol melalui lambda.
Ini juga berfungsi untuk [regression models logistik](#logistic) untuk klasifikasi.

### Advantages

Pemodelan prediksi sebagai **weighted sum** membuatnya transparan bagaimana prediksi dihasilkan.
Dan dengan Lasso kami dapat memastikan bahwa jumlah fitur yang digunakan tetap sedikit.

Banyak orang menggunakan model linear regression.
Ini berarti bahwa di banyak tempat **diterima** untuk pemodelan prediktif dan melakukan inferensi.
Ada **pengalaman dan keahlian kolektif tingkat tinggi**, termasuk materi pengajaran tentang model linear regression dan implementasi perangkat lunak.
linear regression dapat ditemukan di R, Python, Java, Julia, Scala, Javascript, ...

Secara matematis, sangat mudah untuk memperkirakan bobot dan Anda memiliki **jaminan untuk menemukan bobot optimal** (mengingat semua asumsi model linear regression dipenuhi oleh data).

Bersama dengan bobot, Anda mendapatkan confidence intervals, tes, dan teori statistik yang solid.
Ada juga banyak perluasan dari model linear regression (lihat [bab tentang GLM, GAM, dan lainnya](#extend-lm)).


### Disadvantages

Model linear regression hanya dapat mewakili hubungan linier, yaitu jumlah bobot dari fitur input.
Setiap **nonlinier atau interaksi harus dibuat dengan tangan** dan diberikan secara eksplisit ke model sebagai fitur masukan.

Model linier juga sering **tidak terlalu bagus dalam hal kinerja prediktif**, karena hubungan yang dapat dipelajari sangat terbatas dan biasanya terlalu menyederhanakan realitas yang kompleks.

Penafsiran bobot **bisa jadi tidak intuitif** karena bergantung pada semua fitur lainnya.
Sebuah fitur dengan korelasi positif tinggi dengan hasil y dan fitur lain mungkin mendapatkan bobot negatif dalam model linier, karena, mengingat fitur berkorelasi lainnya, itu berkorelasi negatif dengan y dalam ruang dimensi tinggi.
Fitur yang sepenuhnya berkorelasi membuatnya bahkan tidak mungkin untuk menemukan solusi unik untuk persamaan linier.
Sebuah contoh:
Anda memiliki model untuk memprediksi nilai rumah dan memiliki fitur seperti jumlah kamar dan ukuran rumah.
Ukuran rumah dan jumlah kamar sangat berkorelasi: semakin besar sebuah rumah, semakin banyak kamar yang dimilikinya.
Jika Anda mengambil kedua fitur ke dalam model linier, itu mungkin terjadi, bahwa ukuran rumah adalah prediktor yang lebih baik dan mendapat bobot positif yang besar.
Jumlah kamar mungkin berakhir dengan bobot negatif, karena, mengingat bahwa sebuah rumah memiliki ukuran yang sama, menambah jumlah kamar dapat membuatnya kurang berharga atau persamaan linier menjadi kurang stabil, ketika korelasinya terlalu kuat.
