# Data and Models for Examples {#data}

{{< include _setup.qmd >}}


```{r}
compute_and_plot_mi <- function(data, bins = 10) {
  # Input validation
  if (!is.data.frame(data)) stop("Input must be a data frame")
  if (ncol(data) < 2) stop("Data must have at least 2 columns")
  if (bins < 2) stop("Number of bins must be at least 2")
  
  require("infotheo")
  require("ggplot2")
  require("reshape2")
  
  # Discretize data
  data_discretized <- data
  for (col in colnames(data)) {
    if (is.numeric(data[[col]])) {
      data_discretized[[col]] <- discretize(data[[col]], disc = "equalfreq", nbins = bins)
    }
  }
  
  # Compute MI matrix
  mi_matrix <- matrix(0, ncol = ncol(data), nrow = ncol(data),
                     dimnames = list(colnames(data), colnames(data)))
  entropy_vec <- sapply(data_discretized, function(col) entropy(col))
  
  for (i in 1:ncol(data)) {
    for (j in 1:ncol(data)) {
      if (j < i) {  # Only compute for lower triangle
        mi_value <- mutinformation(data_discretized[[i]], data_discretized[[j]])
        # Ensure no division by zero
        if (entropy_vec[i] == 0 || entropy_vec[j] == 0) {
          mi_matrix[i, j] <- 0
        } else {
          mi_matrix[i, j] <- mi_value / sqrt(entropy_vec[i] * entropy_vec[j])  # Normalized MI
        }
      } else {
        mi_matrix[i, j] <- NA  # Set upper triangle including diagonal to NA
      }
    }
  }
  
  # Convert MI matrix to a long format for plotting
  mi_long <- melt(mi_matrix)
  colnames(mi_long) <- c("Feature1", "Feature2", "MutualInformation")
  
  # Remove NA values (upper triangle)
  mi_long <- mi_long[!is.na(mi_long$MutualInformation), ]
  
  # Plot MI heatmap
  p <- ggplot(mi_long, aes(x = Feature1, y = Feature2, fill = MutualInformation)) +
    geom_tile() +
    geom_label(aes(label = sprintf("%.2f", MutualInformation)), 
               size = 3,
               fill = "white",
               alpha = 0.7) +
    scale_fill_viridis_c(
      limits = c(0, 1),  # Fix scale limits
      oob = scales::squish  # Handle any out-of-bounds values
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Mutual Information Heatmap", 
         fill = "Normalized MI")  # Updated legend title to be more specific
  
  print(p)
  
  # Optionally return the MI matrix invisibly for further analysis
  invisible(mi_matrix)
}
```

Throughout the book, there's two datasets that you will encounter often.
One about bikes, the other about penguins.
I love them both.
This chapter presents these datasets, the associated prediction tasks and the models I trained and that we will interpret through the book.

## Bike Rentals (Regression) {#bike-data}

This dataset contains daily counts of rented bicycles from the bicycle rental company [Capital-Bikeshare](https://www.capitalbikeshare.com/) in Washington D.C., along with weather and seasonal information.
The data was kindly made openly available by Capital-Bikeshare.
[@fanaeet2014event] added weather data and season information.
The goal is to predict how many bikes will be rented depending on the weather and the day, and the dataset contains `r nrow(bike)` days.
The data can be downloaded from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).

I didn't use all the features and did some feature engineering myself, so here is the list of features I used for training models:

- Count of bicycles including both casual and registered users. The count is used as the target in the regression task.
- The season, either spring, summer, fall or winter.
- Indicator whether the day was a holiday or not.
- Indicator whether the day was a working day or weekend.
- The weather situation on that day. One of:
    - clear, few clouds, partly cloudy, cloudy
    - mist + clouds, mist + broken clouds, mist + few clouds, mist
    - light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds
    - heavy rain + ice pallets + thunderstorm + mist, snow + mist
- Temperature in degrees Celsius.
- Relative humidity in percent (0 to 100).
- Wind speed in km per hour.
- Count of rented bikes two days before.


### Predicting Bike Rentals

Since this example is just for showcasing the interpretability methods, I'm taking some liberties.
I pretend that the weather features are forecasts (they are not).
That means our prediction task has the following shape:
We predict tomorrows number of rented bikes based on weather forecasts, seasonal information, and how many bikes were rented yesterday.

## Bike count regression models

I trained all regression models using a simple holdout strategy: 2/3 of the data for training, and 1/3 for testing.
The ML algorithms were: random forest, CART decision tree, support vector machine, and linear regression.

```{r}
#| label: fig-performance-bike
library(Metrics)
rf_preds = predict(bike_rf, bike_test)
rf_rmse = rmse(bike_test$cnt, rf_preds)
rf_mae = mae(bike_test$cnt, rf_preds)

tree_preds = predict(bike_tree, bike_test)
tree_rmse = rmse(bike_test$cnt, tree_preds)
tree_mae = mae(bike_test$cnt, tree_preds)


svm_preds = predict(bike_svm, bike_test)
svm_rmse = rmse(bike_test$cnt, svm_preds)
svm_mae = mae(bike_test$cnt, svm_preds)

lm_preds = predict(bike_lm, bike_test)
lm_rmse = rmse(bike_test$cnt, lm_preds)
lm_mae = mae(bike_test$cnt, lm_preds)

# Create a markdown table
results = data.frame(
  Model = c("Random Forest", "Decision Tree", "SVM", "Linear Regression"),
  RMSE = c(rf_rmse, tree_rmse, svm_rmse, lm_rmse),
  MAE = c(rf_mae, tree_mae, svm_mae, lm_mae)
)

results = results[order(results$RMSE), ]
kable(results, caption = "Comparison of Model Performance")
```

@fig-performance-bike shows that the support vector machine performed best, since it had the lowest root mean squared error (RMSE) and the lowest mean absolute error (MAE).
The random forest was slightly worse, and the linear regression model even more.
Trailing very far behind is the decision tree, which didn't work out so well at all.


### Correlations

For many interpretation methods, it's important to understand how the features are correlated.
Therefore, let's have a look at the Pearson correlation for the numerical features.

```{r}
#| label: fig-bike-correlations
#| fig.cap: Pearson correlation between the bike data features.
# Select numerical columns and remove rows with NA values
bike_clean <- bike[, sapply(bike, is.numeric)]
bike_clean$cnt = NULL
cor_matrix <- cor(bike_clean)
cor_melt <- melt(cor_matrix)
# Visualize the correlations using ggplot2 with labels
ggplot(cor_melt, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_label(aes(label = round(value, 2)), color = "black", size = 5, fill="white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1)) +
  my_theme() +
  labs(
    title = "Correlation Matrix of Bike Rental Data",
    x = "",
    y = "",
    fill = "Correlation"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

As the correlation plot in @fig-bike-correlations shows, the only linear correlation that is a bit bigger is between count 2 days before and the temperature.

But what about the categorical features?
And what about non-linear correlation?
Let's be a bit creative and use "variance explained".
The "variance-explained" measure lies always between 0 (no association) and 1 (temperature can be perfectly predicted from the other feature).
We calculate the explained variance of each feature, by predicting it from the other features (using the test data).[^problem-with-test-data]
The higher the explained variance (correlation), the more potential problems with interpretation.
@fig-bike-cor visualizes how strongly the weather features are correlated with other features.

```{r}
#| label: fig-bike-cor
#| eval: true 
#| fig.cap: "The strength of the correlation between temperature, humidity and wind speed with all features, measured as the amount of variance explained, when we train a linear model with e.g. temperature to predict and season as feature. For temperature we observe -- not surprisingly -- a high correlation with season and month. Humidity correlates with weather situation."
compute_variance_explained <- function(train_data, test_data) {
  results <- data.frame(Feature = character(), Variance_Explained = numeric(), stringsAsFactors = FALSE)
  for (feature in colnames(train_data)) {
    y <- train_data[[feature]]
    X <- train_data[, setdiff(colnames(train_data), feature), drop = FALSE]
    y_test <- test_data[[feature]]
    X_test <- test_data[, setdiff(colnames(test_data), feature), drop = FALSE]

    if (is.factor(y) || length(unique(y)) < 10) { # Categorical
      model <- randomForest(X, y, importance = TRUE)
      y_pred <- predict(model, X_test)
      accuracy <- sum(y_pred == y_test) / length(y_test)
      variance_explained <- accuracy
    } else { # Numerical
      model <- randomForest(X, y, importance = TRUE)
      y_pred <- predict(model, X_test)
      r2 <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)
      variance_explained <- r2
    }
    results <- rbind(results, data.frame(Feature = feature, Variance_Explained = variance_explained))
  }
  return(results[order(-results$Variance_Explained), ])
}
results_df <- compute_variance_explained(bike_train %>% select(-cnt), bike_test %>% select(-cnt))

# Visualize results
ggplot(results_df, aes(x = reorder(Feature, Variance_Explained), y = Variance_Explained)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Variance Explained by Random Forest", x = "Features", y = "Variance Explained (R² or Accuracy)") +
  theme_minimal()
```

```{r}
compute_and_plot_mi(bike)
```



## Palmer Penguins (Classification) {#penguins}

For classification, we will use the Palmer penguin data.
This cute dataset contains measurements from `r nrow(penguins)` of penguins from the Palmer Archipelago in Antarctica.
The dataset was collected and published by Gorman et al. (2014) [@gorman2014ecological] and the Palmer Station in Antarctica, which is part of the Long Term Ecological Research Network.
The paper studies difference in appearance between male and female, among other things.
That's why we'll use as prediction target a penguin's sex and predict it based on body measurements. 

![The 3 penguin species in the data: Chinstrap, Gentoo, and Adelie. Artwork by \@allison_horst.](./images/lter_penguins.png){width=80%}


Each row represents a penguin and contains the following information:

- Sex of the penguin (male/female).
- Species of penguin (Chinstrap/Gentoo/Adelie).
- Body mass of the penguin, measured in grams.
- Length of the bill (the beak) measured in millimeters (see also the following @fig-bill-measurements).
- Depth of the bill, measured in millimeters.
- Length of the flipper (the "tail"), measured in millimeters.

![Bill measurements. Artwork by \@allison_horst.](./images/culmen_depth.png){#fig-bill-measurements width=70%}


11 penguins had missing data.
Since the purpose of this data is to demonstrate interpretable machine learning methods and not an in-depth study of penguins, I simply dropped the missing values.
The dataset is loaded using the palmerpenguins R package [@horst2020allisonhorst].

### Penguin Models

For the data examples, I trained the following models, using a simple split into training (2/3) and holdout test data (1/3):

```{r}
library(Metrics)
# Predictions for random forest 
rf_probs = predict(pengu_rf, penguins_test, type = "prob")[,"female"]
rf_preds = predict(pengu_rf, penguins_test)
rf_log_loss = logLoss(predicted = rf_probs, actual = penguins_test$sex == "female")
rf_accuracy = sum(rf_preds == penguins_test$sex) / nrow(penguins_test)

# Predictions for decision tree
tree_probs = predict(pengu_tree, penguins_test, type = "prob")
tree_preds = predict(pengu_tree, penguins_test, type = "class")
tree_log_loss = logLoss(predicted = tree_probs, actual = penguins_test$sex == "female")
tree_accuracy = sum(tree_preds == penguins_test$sex) / nrow(penguins_test)

# Predictions for multinomial logistic regression
logreg_probs = predict(pengu_logreg, penguins_test)
logreg_preds = ifelse(logreg_probs > 0.5, "female", "male")  
logreg_log_loss = logLoss(predicted = logreg_probs, actual = penguins_test$sex == "female")
logreg_accuracy = sum(logreg_preds == penguins_test$sex) / nrow(penguins_test)

results = data.frame(
  Model = c("Random Forest", "Decision Tree", "Logistic Regression (by Species)"),
  Log_Loss = c(rf_log_loss, tree_log_loss, logreg_log_loss),
  Accuracy = c(rf_accuracy, tree_accuracy, logreg_accuracy)
)

results = results[order(results$Log_Loss), ]

kable(results, caption = "Comparison of Model Performance")
```

### Correlations

Let's have a look at how the features in the data are correlated (Pearson correlation) with @fig-penguins-correlations.

```{r}
#| label: fig-penguins-correlations
#| fig.cap: Pearson correlation between the penguin features.

# Select numerical columns and remove rows with NA values
penguins_clean <- na.omit(penguins[, sapply(penguins, is.numeric)])

# Compute the correlation matrix
cor_matrix <- cor(penguins_clean)

# Melt the correlation matrix for visualization
cor_melt <- melt(cor_matrix)

# Keep only lower triangle of the correlation matrix
cor_melt <- cor_melt[which(as.numeric(cor_melt$Var1) >= as.numeric(cor_melt$Var2)), ]

# Visualize the correlations using ggplot2 with labels
ggplot(cor_melt, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_label(aes(label = round(value, 2)), 
             color = "black", 
             size = 5, 
             fill = "white") +
  scale_fill_gradient2(low = "blue", 
                      high = "red", 
                      mid = "white", 
                      midpoint = 0, 
                      limit = c(-1, 1)) +
  my_theme() +
  labs(
    title = "Correlation Matrix of Palmer Penguins Data",
    x = "",
    y = "",
    fill = "Correlation"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_fixed()  # Make the plot square
```



@fig-penguins-correlations shows that especially the body mass and the flipper length are strongly correlated.
But also other features are strongly  correlated, like flipper length and bill length or flipper length and bill depth.
Again, we look at variance explained:

```{r}
#| label: variance explained penguins
#| fig-cap: Variance explained by predicting each of the features from the other features. The species is almost fully determined by the other features.
results_df <- compute_variance_explained(penguins_train %>% select(-sex), penguins_test %>% select(-sex))

# Visualize results
ggplot(results_df, aes(x = reorder(Feature, Variance_Explained), y = Variance_Explained)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Variance Explained by Random Forest", x = "Features", y = "Variance Explained (R² or Accuracy)") +
  theme_minimal()
```



```{r}
compute_and_plot_mi(penguins)
```



[^problem-with-test-data]: In a perfect setting we wouldn't use the test data for this, but keep it perfectly untouched for the test evaluation.
