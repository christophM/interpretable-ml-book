```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->

## Decision Tree {#tree}

Linear regression dan logistic regression model gagal dalam situasi di mana hubungan antara fitur dan hasil tidak linier atau di mana fitur berinteraksi satu sama lain.
Saatnya bersinar untuk decision trees!
Model berbasis pohon membagi data beberapa kali sesuai dengan nilai batas tertentu dalam fitur.
Melalui pemisahan, subset yang berbeda dari dataset dibuat, dengan setiap instance milik satu subset.
Subset terakhir disebut node terminal atau daun dan subset perantara disebut node internal atau node split.
Untuk memprediksi hasil di setiap simpul daun, hasil rata-rata dari data pelatihan di simpul ini digunakan.
Pohon dapat digunakan untuk klasifikasi dan regresi.

Ada berbagai algoritma yang dapat menumbuhkan pohon.
Mereka berbeda dalam kemungkinan struktur pohon (misalnya jumlah perpecahan per node), kriteria bagaimana menemukan perpecahan, kapan harus berhenti membelah dan bagaimana memperkirakan model sederhana dalam node daun.
Algoritma classification and regression trees (CART) mungkin merupakan algoritma yang paling populer untuk induksi pohon.
Kami akan fokus pada CART, tetapi interpretasinya serupa untuk sebagian besar jenis pohon lainnya.
Saya merekomendasikan buku 'The Elements of Statistical Learning' (Friedman, Hastie dan Tibshirani 2009)[^Hastie] untuk pengenalan yang lebih rinci tentang CART.

```{r tree-artificial, fig.cap="Decision tree with artificial data. Instances with a value greater than 3 for feature x1 end up in node 5. All other instances are assigned to node 3 or node 4, depending on whether values of feature x2  exceed 1.", dev.args = list(pointsize = 15)}
library("partykit")
set.seed(42)
n = 100
dat_sim = data.frame(feature_x1 = rep(c(3,3,4,4), times = n), feature_x2 = rep(c(1,2,2,2), times = n), y = rep(c(1, 2, 3, 4), times = n))
dat_sim = dat_sim[sample(1:nrow(dat_sim), size = 0.9 * nrow(dat_sim)), ]
dat_sim$y = dat_sim$y + rnorm(nrow(dat_sim), sd = 0.2)
ct = ctree(y ~ feature_x1 + feature_x2, dat_sim)
plot(ct, inner_panel = node_inner(ct, pval = FALSE, id = FALSE), 
  terminal_panel = node_boxplot(ct, id = FALSE))
```

Rumus berikut menjelaskan hubungan antara hasil y dan fitur x.

$$\hat{y}=\hat{f}(x)=\sum_{m=1}^Mc_m{}I\{x\in{}R_m\}$$

Setiap instance jatuh ke dalam satu simpul daun (=subset $R_m$).
$I_{\{x\in{}R_m\}}$ adalah fungsi identitas yang mengembalikan 1 jika $x$ ada dalam subset $R_m$ dan 0 sebaliknya.
Jika sebuah instance jatuh ke dalam simpul daun $R_l$, hasil yang diprediksi adalah $\hat{y}=c_l$, di mana $c_l$ adalah rata-rata dari semua contoh pelatihan di simpul daun $R_l$.

Tapi dari mana subset itu berasal?
Ini cukup sederhana:
CART mengambil fitur dan menentukan titik potong mana yang meminimalkan varians y untuk tugas regresi atau indeks Gini dari distribusi kelas y untuk tugas klasifikasi.
Varians memberi tahu kita berapa banyak nilai y dalam sebuah simpul tersebar di sekitar nilai rata-ratanya.
Indeks Gini memberi tahu kita seberapa "tidak murni" sebuah node, mis. jika semua kelas memiliki frekuensi yang sama, simpul tersebut tidak murni, jika hanya satu kelas yang hadir, maka simpul tersebut murni maksimal.
Varians dan indeks Gini diminimalkan ketika titik data dalam node memiliki nilai yang sangat mirip untuk y.
Akibatnya, titik potong terbaik membuat dua himpunan bagian yang dihasilkan menjadi berbeda mungkin sehubungan dengan hasil target.
Untuk fitur kategoris, algoritme mencoba membuat himpunan bagian dengan mencoba pengelompokan kategori yang berbeda.
Setelah cutoff terbaik per fitur telah ditentukan, algoritme memilih fitur untuk pemisahan yang akan menghasilkan partisi terbaik dalam hal varians atau indeks Gini dan menambahkan pemisahan ini ke pohon.
Algoritme melanjutkan pencarian dan pemisahan ini secara rekursif di kedua node baru hingga kriteria berhenti tercapai.
Kriteria yang mungkin adalah:
Jumlah minimum instance yang harus berada di sebuah node sebelum pemisahan, atau jumlah minimum instance yang harus berada di terminal node.

### Interpretation

Interpretasinya sederhana:
Mulai dari simpul akar, Anda pergi ke simpul berikutnya dan ujung-ujungnya memberi tahu Anda himpunan bagian mana yang Anda lihat.
Setelah Anda mencapai simpul daun, simpul tersebut memberi tahu Anda hasil yang diprediksi.
Semua tepi dihubungkan oleh 'DAN'.

Templat: Jika fitur x [lebih kecil/lebih besar] dari ambang c AND ... maka hasil yang diprediksi adalah nilai rata-rata y dari instance di simpul itu.

**Feature importance**

Pentingnya keseluruhan fitur dalam decision trees dapat dihitung dengan cara berikut:
Telusuri semua pemisahan yang menggunakan fitur tersebut dan ukur seberapa besar penurunan varians atau indeks Gini dibandingkan dengan simpul induk.
Jumlah semua kepentingan diskalakan menjadi 100.
Ini berarti bahwa setiap kepentingan dapat diinterpretasikan sebagai bagian dari kepentingan model secara keseluruhan.

**Dekomposisi pohon**

Prediksi individu dari decision trees dapat dijelaskan dengan menguraikan jalur keputusan menjadi satu komponen per fitur.
Kita dapat melacak keputusan melalui pohon dan menjelaskan prediksi dengan kontribusi yang ditambahkan pada setiap simpul keputusan.

Node akar dalam decision trees adalah titik awal kami.
Jika kita menggunakan simpul akar untuk membuat prediksi, itu akan memprediksi rata-rata dari hasil data pelatihan.
Dengan pembagian berikutnya, kami mengurangi atau menambahkan istilah ke jumlah ini, tergantung pada simpul berikutnya di jalur.
Untuk sampai ke prediksi akhir, kita harus mengikuti jalur instance data yang ingin kita jelaskan dan terus menambahkan ke rumus.

$$\hat{f}(x)=\bar{y}+\sum_{d=1}^D\text{split.contrib(d,x)}=\bar{y}+\sum_{j=1}^p\text{feat.contrib(j,x)}$$

Prediksi instance individu adalah rata-rata dari hasil target ditambah jumlah semua kontribusi dari D split yang terjadi antara node root dan node terminal tempat instance berakhir.
Kami tidak tertarik pada kontribusi split, tetapi pada feature contribution.
Sebuah fitur dapat digunakan untuk lebih dari satu split atau tidak sama sekali.
Kita dapat menambahkan kontribusi untuk setiap p fitur dan mendapatkan interpretasi seberapa besar kontribusi setiap fitur terhadap prediksi.

### Example
Mari kita lihat lagi [data sewa sepeda](#bike-data).
Kami ingin memprediksi jumlah sepeda yang disewa pada hari tertentu dengan decision trees.
Pohon yang dipelajari terlihat seperti ini:

```{r tree-example, fig.cap="Regression tree fitted on the bike rental data. The maximum allowed depth for the tree was set to 2. The trend feature (days since 2011) and the temperature (temp) have been selected for the splits. The boxplots show the distribution of bicycle counts in the terminal node.", dev.args = list(pointsize = 13)}
data(bike)
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)
# increases readability of tree
x = rpart(y ~ ., data = na.omit(dat), method = 'anova', control = rpart.control(cp = 0, maxdepth = 2))
xp = as.party(x)
plot(xp, digits = 0, id = FALSE, terminal_panel = node_boxplot(xp, id = FALSE),
  inner_panel = node_inner(xp, id = FALSE, pval = FALSE)
  )
```
Split pertama dan split kedua dilakukan dengan fitur tren, yang menghitung hari sejak pengumpulan data dimulai dan mencakup tren bahwa layanan penyewaan sepeda menjadi lebih populer dari waktu ke waktu.
Untuk hari sebelum hari ke 105, prediksi jumlah sepeda sekitar 1800, antara hari ke 106 dan 430 sekitar 3900.
Untuk hari setelah hari ke 430, prediksinya adalah 4600 (jika suhu di bawah 12 derajat) atau 6600 (jika suhu di atas 12 derajat).

Feature importance memberi tahu kita seberapa banyak fitur membantu meningkatkan kemurnian semua node.
Di sini, varians digunakan, karena memprediksi persewaan sepeda adalah tugas regresi.

Pohon yang divisualisasikan menunjukkan bahwa tren suhu dan waktu digunakan untuk pemisahan, tetapi tidak mengukur fitur mana yang lebih penting.
Ukuran feature importance menunjukkan bahwa tren waktu jauh lebih penting daripada suhu.

```{r tree-importance, fig.cap = "Importance of the features measured by how much the node purity is improved on average."}
imp = round(100 * x$variable.importance / sum(x$variable.importance),0)
imp.df = data.frame(feature = names(imp), importance = imp)
imp.df$feature = factor(imp.df$feature, levels = as.character(imp.df$feature)[order(imp.df$importance)])
ggplot(imp.df) + geom_point(aes(x = importance, y = feature)) + 
  scale_y_discrete("")
```

### Advantages

Struktur pohon ideal untuk **menangkap interaksi** antar fitur dalam data.

Data berakhir dalam **grup berbeda** yang seringkali lebih mudah dipahami daripada titik pada hyperplane multidimensi seperti pada linear regression.
Penafsirannya bisa dibilang cukup sederhana.

Struktur pohon juga memiliki **visualisasi alami**, dengan simpul dan tepinya.

Pohon **membuat penjelasan yang baik** seperti yang didefinisikan dalam [bab tentang "Penjelasan yang Ramah Manusia"](#good-explanation).
Struktur pohon secara otomatis mengundang untuk berpikir tentang nilai yang diprediksi untuk masing-masing contoh sebagai kontrafaktual:
"Jika fitur lebih besar / lebih kecil dari titik split, prediksinya akan menjadi y1 bukan y2."
Penjelasan pohon bersifat kontras, karena Anda selalu dapat membandingkan prediksi instance dengan skenario "bagaimana jika" yang relevan (sebagaimana didefinisikan oleh pohon) yang hanya merupakan simpul daun lain dari pohon.
Jika pohonnya pendek, seperti satu hingga tiga terbelah, penjelasan yang dihasilkan bersifat selektif.
Pohon dengan kedalaman tiga membutuhkan maksimal tiga fitur dan titik pisah untuk membuat penjelasan untuk prediksi masing-masing instance.
Kebenaran prediksi tergantung pada kinerja prediksi pohon.
Penjelasan untuk pohon pendek sangat sederhana dan umum, karena untuk setiap pemisahan, instance jatuh ke salah satu atau daun lainnya, dan keputusan biner mudah dipahami.

Tidak perlu mengubah fitur.
Dalam model linier, terkadang perlu untuk mengambil logaritma dari sebuah fitur.
Sebuah decision trees bekerja sama baiknya dengan transformasi monoton dari sebuah fitur.



### Disadvantages

**Trees gagal menangani hubungan linier**.
Setiap hubungan linier antara fitur input dan hasilnya harus didekati dengan pemisahan, membuat fungsi langkah.
Ini tidak efisien.

Ini sejalan dengan **kurangnya kelancaran**.
Perubahan kecil pada fitur input dapat berdampak besar pada hasil yang diprediksi, yang biasanya tidak diinginkan.
Bayangkan sebuah pohon yang memprediksi nilai sebuah rumah dan pohon tersebut menggunakan ukuran rumah sebagai salah satu fitur split.
Perpecahan terjadi pada 100,5 meter persegi.
Bayangkan pengguna penaksir harga rumah menggunakan model decision trees Anda:
Mereka mengukur rumah mereka, sampai pada kesimpulan bahwa rumah itu memiliki luas 99 meter persegi, memasukkannya ke dalam kalkulator harga dan mendapatkan prediksi 200.000 Euro.
Pengguna menyadari bahwa mereka lupa mengukur ruang penyimpanan kecil seluas 2 meter persegi.
Ruang penyimpanan memiliki dinding miring, sehingga mereka tidak yakin apakah mereka dapat menghitung seluruh area atau hanya setengahnya.
Jadi mereka memutuskan untuk mencoba keduanya 100,0 dan 101,0 meter persegi.
Hasil: Kalkulator harga menghasilkan 200.000 Euro dan 205.000 Euro, yang agak tidak intuitif, karena tidak ada perubahan dari 99 meter persegi menjadi 100.

Pohon juga cukup **tidak stabil**.
Beberapa perubahan dalam set data pelatihan dapat membuat pohon yang sama sekali berbeda.
Ini karena setiap pemisahan tergantung pada pemisahan induk.
Dan jika fitur yang berbeda dipilih sebagai fitur pemisahan pertama, seluruh struktur pohon akan berubah.
Itu tidak menciptakan kepercayaan pada model jika strukturnya berubah begitu mudah.

decision trees sangat dapat diinterpretasikan -- asalkan pendek.
**Jumlah node terminal meningkat dengan cepat seiring dengan kedalaman.**
Semakin banyak simpul terminal dan semakin dalam pohon, semakin sulit untuk memahami decision rules pohon.
Kedalaman 1 berarti 2 simpul terminal.
Kedalaman 2 berarti maks. 4 node.
Kedalaman 3 berarti maks. 8 node.
Jumlah maksimum node terminal di pohon adalah 2 pangkat kedalaman.

### Software

Untuk contoh dalam bab ini, saya menggunakan paket `rpart` R yang mengimplementasikan CART (pohon klasifikasi dan regresi).
CART diimplementasikan dalam banyak bahasa pemrograman, termasuk [Python](https://scikit-learn.org/stable/modules/tree.html).
Bisa dibilang, CART adalah algoritma yang cukup tua dan agak ketinggalan jaman dan ada beberapa algoritma baru yang menarik untuk memasang pohon.
Anda dapat menemukan ikhtisar beberapa paket R untuk decision trees di [Machine Learning and Statistical Learning CRAN Task View](https://cran.r-project.org/web/views/MachineLearning.html) di bawah kata kunci "Recursive Partitioning".
