```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->

## Partial Dependence Plot (PDP) {#pdp}

Partial dependence plot (PDP pendek atau plot PD) menunjukkan efek marginal satu atau dua fitur pada hasil prediksi model machine learning (J. H. Friedman 2001[^Friedman2001]).
Partial dependence plot dapat menunjukkan apakah hubungan antara target dan fitur adalah linier, monoton, atau lebih kompleks.
Misalnya, ketika diterapkan pada model linear regression, partial dependence plot selalu menunjukkan hubungan linier.

Fungsi ketergantungan parsial untuk regresi didefinisikan sebagai:

$$\hat{f}_{x_S}(x_S)=E_{x_C}\left[\hat{f}(x_S,x_C)\right]=\int\hat{f}(x_S,x_C)d\mathbb{P}(x_C)$$

$x_S$ adalah fitur yang fungsi ketergantungan parsialnya harus diplot dan $x_C$ adalah fitur lain yang digunakan dalam model machine learning $\hat{f}$.
Biasanya, hanya ada satu atau dua fitur di set S.
Fitur dalam S adalah fitur yang ingin kita ketahui efeknya pada prediksi.
Vektor fitur $x_S$ dan $x_C$ digabungkan membentuk total ruang fitur x.
Ketergantungan parsial bekerja dengan meminggirkan keluaran model machine learning atas distribusi fitur di set C, sehingga fungsi menunjukkan hubungan antara fitur di set S yang kita minati dan hasil yang diprediksi.
Dengan meminggirkan fitur lain, kami mendapatkan fungsi yang hanya bergantung pada fitur di S, interaksi dengan fitur lain yang disertakan.

Fungsi parsial $\hat{f}_{x_S}$ diperkirakan dengan menghitung rata-rata dalam data pelatihan, juga dikenal sebagai metode Monte Carlo:

$$\hat{f}_{x_S}(x_S)=\frac{1}{n}\sum_{i=1}^n\hat{f}(x_S,x^{(i)}_{C})$$
Fungsi parsial memberitahu kita untuk nilai yang diberikan dari fitur S apa efek marjinal rata-rata pada prediksi.
Dalam rumus ini, $x^{(i)}_{C}$ adalah nilai fitur aktual dari kumpulan data untuk fitur yang tidak kita minati, dan n adalah jumlah instance dalam kumpulan data.
Asumsi PDP adalah bahwa fitur di C tidak berkorelasi dengan fitur di S.
Jika asumsi ini dilanggar, rata-rata yang dihitung untuk partial dependence plot akan mencakup titik data yang sangat tidak mungkin atau bahkan tidak mungkin (lihat kerugian).

Untuk klasifikasi di mana model machine learning mengeluarkan probabilitas, partial dependence plot menampilkan probabilitas untuk kelas tertentu yang diberikan nilai yang berbeda untuk fitur dalam S.
Cara mudah untuk menangani banyak kelas adalah dengan menggambar satu garis atau plot per kelas.

Partial dependence plot adalah metode global:
Metode ini mempertimbangkan semua contoh dan memberikan pernyataan tentang hubungan global fitur dengan hasil yang diprediksi.

**Fitur kategoris**

Sejauh ini, kami hanya mempertimbangkan fitur numerik.
Untuk fitur kategorikal, ketergantungan parsial sangat mudah dihitung.
Untuk setiap kategori, kami mendapatkan perkiraan PDP dengan memaksa semua instance data memiliki kategori yang sama.
Misalnya, jika kita melihat dataset persewaan sepeda dan tertarik pada partial dependence plot untuk musim, kita mendapatkan 4 angka, satu untuk setiap musim.
Untuk menghitung nilai "musim panas", kami mengganti musim semua instance data dengan "musim panas" dan membuat rata-rata prediksi.



### Examples

Dalam prakteknya, himpunan fitur S biasanya hanya berisi satu fitur atau maksimal dua, karena satu fitur menghasilkan plot 2D dan dua fitur menghasilkan plot 3D.
Segala sesuatu di luar itu cukup rumit.
Bahkan 3D pada kertas atau monitor 2D sudah menantang.

Mari kita kembali ke contoh regresi, di mana kita memprediksi jumlah [sepeda yang akan disewa pada hari tertentu](#bike-data).
Pertama kami menyesuaikan model machine learning, lalu kami menganalisis dependensi parsial.
Dalam hal ini, kami telah memasang random forest untuk memprediksi jumlah sepeda dan menggunakan partial dependence plot untuk memvisualisasikan hubungan yang telah dipelajari model.
Pengaruh fitur cuaca pada jumlah sepeda yang diprediksi divisualisasikan pada gambar berikut.

```{r pdp-bike, fig.cap = 'PDPs for the bicycle count prediction model and temperature, humidity and wind speed. The largest differences can be seen in the temperature. The hotter, the more bikes are rented. This trend goes up to 20 degrees Celsius, then flattens and drops slightly at 30. Marks on the x-axis indicate the data distribution.'}
data(bike)
library("mlr")
library("iml")
library("ggplot2")

bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.randomForest', id = 'bike-rf'), bike.task)

pred.bike = Predictor$new(mod.bike, data = bike)
pdp = FeatureEffect$new(pred.bike, "temp", method = "pdp") 
p1 = pdp$plot() +  
  scale_x_continuous('Temperature', limits = c(0, NA)) + 
  scale_y_continuous('Predicted number of bikes', limits = c(0, 5500))
pdp$set.feature("hum")
p2 = pdp$plot() + 
  scale_x_continuous('Humidity', limits = c(0, NA)) + 
  scale_y_continuous('', limits = c(0, 5500))
pdp$set.feature("windspeed")
p3 = pdp$plot() + 
  scale_x_continuous('Wind speed', limits = c(0, NA)) + 
  scale_y_continuous('', limits = c(0, 5500))

gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

Untuk cuaca hangat tetapi tidak terlalu panas, model ini memperkirakan rata-rata jumlah sepeda sewaan yang tinggi.
Potensi bikers semakin terhambat dalam menyewa sepeda saat kelembaban melebihi 60%.
Selain itu, semakin banyak angin, semakin sedikit orang yang suka bersepeda, yang masuk akal.
Menariknya, jumlah persewaan sepeda yang diprediksi tidak turun saat kecepatan angin meningkat dari 25 menjadi 35 km/jam, tetapi tidak ada banyak data pelatihan, sehingga model machine learning mungkin tidak dapat mempelajari prediksi yang berarti untuk kisaran ini.
Setidaknya secara intuitif, saya berharap jumlah sepeda berkurang dengan meningkatnya kecepatan angin, terutama ketika kecepatan angin sangat tinggi.

Untuk mengilustrasikan partial dependence plot dengan fitur kategoris, kami menguji pengaruh fitur musim pada prediksi persewaan sepeda.

```{r pdp-bike-cat, fig.cap = 'PDPs for the bike count prediction model and the season. Unexpectedly all seasons show similar effect on the model predictions, only for spring the model predicts fewer bicycle rentals.'}

pdp = FeatureEffect$new(pred.bike, "season", method = "pdp") 
ggplot(pdp$results) + 
  geom_col(aes(x = season, y = .value), fill = default_color, width = 0.3) + 
  scale_x_discrete('Season') + 
  scale_y_continuous('', limits = c(0, 5500))
```

Kami juga menghitung ketergantungan parsial untuk [klasifikasi kanker serviks](#cervical).
Kali ini kami mencocokkan random forest untuk memprediksi apakah seorang wanita mungkin terkena kanker serviks berdasarkan faktor risiko.
Kami menghitung dan memvisualisasikan ketergantungan parsial dari probabilitas kanker pada fitur yang berbeda untuk random forest:

```{r pdp-cervical, fig.cap = 'PDPs of cancer probability based on age and years with hormonal contraceptives. For age, the PDP shows that the probability is low until 40 and increases after. The more years on hormonal contraceptives the higher the predicted cancer risk, especially after 10 years. For both features not many data points with large values were available, so the PD estimates are less reliable in those regions.', dev.args = list(pointsize = 5.5)}
data(cervical)
cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
mod = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', id = 'cervical-rf', predict.type = 'prob'), cervical.task)

pred.cervical = Predictor$new(mod, data = cervical, class = "Cancer")
pdp = FeatureEffect$new(pred.cervical, "Age", method = "pdp") 

p1 = pdp$plot() + 
  scale_x_continuous(limits = c(0, NA)) + 
  scale_y_continuous('Predicted cancer probability', limits = c(0, 0.4))
pdp$set.feature("Hormonal.Contraceptives..years.")
p2 = pdp$plot() + 
  scale_x_continuous("Years on hormonal contraceptives", limits = c(0, NA)) + 
  scale_y_continuous('', limits = c(0, 0.4))

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

Kita juga dapat memvisualisasikan ketergantungan parsial dari dua fitur sekaligus:

```{r pdp-cervical-2d, fig.cap = 'PDP of cancer probability and the interaction of age and number of pregnancies. The plot shows the increase in cancer probability at 45. For ages below 25, women who had 1 or 2 pregnancies have a lower predicted cancer risk, compared with women who had 0 or more than 2 pregnancies. But be careful when drawing conclusions: This might just be a correlation and not causal!'}
pd = FeatureEffect$new(pred.cervical, c("Age", "Num.of.pregnancies"), method = "pdp") 
pd$plot() +
  scale_fill_viridis(option = "D")
```





### Advantages 

Perhitungan partial dependence plot adalah **intuitif**:
Fungsi ketergantungan parsial pada nilai fitur tertentu mewakili prediksi rata-rata jika kita memaksa semua titik data untuk mengasumsikan nilai fitur tersebut.
Menurut pengalaman saya, orang awam biasanya memahami gagasan PDP dengan cepat.

Jika fitur yang Anda hitung PDPnya tidak berkorelasi dengan fitur lainnya, maka PDP secara sempurna menggambarkan bagaimana fitur tersebut mempengaruhi prediksi secara rata-rata.
Dalam kasus yang tidak berkorelasi, **interpretasinya jelas**:
partial dependence plot menunjukkan bagaimana prediksi rata-rata dalam kumpulan data Anda berubah ketika fitur ke-j diubah.
Lebih rumit ketika fitur dikorelasikan, lihat juga kerugiannya.

Plot ketergantungan sebagian **mudah diterapkan**.

Perhitungan untuk partial dependence plot memiliki **interpretasi kausal**.
Kami mengintervensi fitur dan mengukur perubahan prediksi.
Dalam melakukannya, kami menganalisis hubungan kausal antara fitur dan prediksi.[^pdpCausal]
Hubungan itu kausal untuk model -- karena kami secara eksplisit memodelkan hasil sebagai fungsi fitur -- tetapi tidak harus untuk dunia nyata!

### Disadvantages

**Jumlah fitur maksimum** yang realistis dalam fungsi ketergantungan parsial adalah dua.
Ini bukan kesalahan PDP, tetapi representasi 2 dimensi (kertas atau layar) dan juga ketidakmampuan kita untuk membayangkan lebih dari 3 dimensi.

Beberapa plot PD tidak menunjukkan **distribusi fitur**.
Mengabaikan distribusi dapat menyesatkan, karena Anda mungkin menginterpretasikan secara berlebihan wilayah yang hampir tidak memiliki data.
Masalah ini mudah diselesaikan dengan menunjukkan permadani (indikator titik data pada sumbu x) atau histogram.

**Asumsi independence** adalah masalah terbesar dengan plot PD.
Diasumsikan bahwa fitur yang ketergantungan parsialnya dihitung tidak berkorelasi dengan fitur lainnya.
Misalnya, Anda ingin memprediksi seberapa cepat seseorang berjalan, berdasarkan berat dan tinggi badan orang tersebut.
Untuk ketergantungan parsial salah satu fitur, mis. tinggi, kami berasumsi bahwa fitur lain (berat) tidak berkorelasi dengan tinggi, yang jelas merupakan asumsi yang salah.
Untuk perhitungan PDP pada ketinggian tertentu (misalnya 200 cm), kami rata-ratakan distribusi marjinal berat, yang mungkin mencakup berat di bawah 50 kg, yang tidak realistis untuk orang 2 meter.
Dengan kata lain:
Saat fitur dikorelasikan, kami membuat titik data baru di area distribusi fitur di mana probabilitas sebenarnya sangat rendah (misalnya, tidak mungkin seseorang memiliki tinggi 2 meter tetapi beratnya kurang dari 50 kg).
Salah satu solusi untuk masalah ini adalah [Accumulated Local Effect plots](#ale) atau plot ALE pendek yang bekerja dengan distribusi bersyarat dan bukan distribusi marginal.

**Efek heterogen mungkin tersembunyi** karena plot PD hanya menunjukkan efek marginal rata-rata.
Misalkan untuk fitur, separuh titik data Anda memiliki hubungan positif dengan prediksi -- semakin besar nilai fitur, semakin besar prediksi -- dan separuh lainnya memiliki asosiasi negatif -- semakin kecil nilai fitur, semakin besar prediksi.
Kurva PD dapat berupa garis horizontal, karena efek dari kedua bagian dataset dapat saling meniadakan.
Anda kemudian menyimpulkan bahwa fitur tersebut tidak berpengaruh pada prediksi.
Dengan memplot [kurva individual conditional expectation](#ice) alih-alih garis agregat, kita dapat mengungkap efek heterogen.


### Software and Alternatives

Ada sejumlah paket R yang mengimplementasikan PDP.
Saya menggunakan paket `iml` sebagai contoh, tetapi ada juga `pdp` atau `DALEX`.
Dalam Python, partial dependence plot dibangun ke dalam `scikit-learn` dan Anda dapat menggunakan `PDPBox`.

Alternatif untuk PDP yang disajikan dalam buku ini adalah [plot ALE](#ale) dan [kurva ICE](#ice).



[^Friedman2001]: Friedman, Jerome H. "Greedy function approximation: A gradient boosting machine." Annals of statistics (2001): 1189-1232.

[^pdpCausal]: Zhao, Qingyuan, and Trevor Hastie. "Causal interpretations of black-box models." Journal of Business & Economic Statistics, to appear. (2017).
