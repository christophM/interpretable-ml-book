# Interpretability {#interpretability}

So far, I haven't found a good scientific definition of "Machine learning model interpretability" or how to measure the goodness of an explanation.
Throughout the book, I will use this rather simple, yet elegant definition from \citet{miller2017explanation}: **Interpretability is the degree to which a human can understand the cause of a decision.** The higher the interpretability of a model, the easier it is for someone to comprehend why certain decisions (read: predictions) were made. A model has better interpretability than another model, if it's decisions are easier to comprehend for a human than decisions from the second model. I will be using both the terms interpretable and explainable equally.

## The importance of machine learning interpretability {#interpretability.importance}
If a machine learning model performs well, **why not just trust** it and ignore why it made a certain decision?

Let's dive deeper into the reasons why interpretability is so important.
Machine learning has come to a state where you have to make a trade-off:
Do you simply want to know **what** is predicted happen?
For example if a client will churn or if a medication will work well for a patient.
Or do you want to know **why** something is predicted to happen and paying for the interpretability with accuracy?
In some cases you don't  care why a decision was made, only the assurance that the predictive performance was good on a test dataset is enough.
But in other cases knowing the 'why' can help you understand more about the problem, the data and it can also tell you why a model might fail.
Some problems might not need explanations, because they either are low risk, meaning a mistake has no severe consequences, (e.g. a movie recommender system) or the method has already been extensively studied and evaluated (e.g. optical character recognition).
The necessity for interpretability comes from an incompleteness in the problem formalisation [@Doshi-Velez2017], meaning that for certain problems or tasks it is not enough to get the answer (the **what**), but the model also has to give an explanation how it came to the answer (the **why**), because correctly predicting is not enough to solve the problem.
Following reasons drive the demand for interpretability.

- There is a shift in many scientific disciplines from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics). The **goal of science** is to gain knowledge, but many problems can only be solved with big datasets and black box machine learning models. Interpretability allows to extract additional knowledge.
- It is **human nature** wanting to understand things and to have some form of control.
- Machine learning models are taking over real world tasks, that demand **safety measurements** and testing. A self-driving car automatically detects cyclists, which is as desired. You want to bet 100% sure that the abstraction the system learned will be fail-safe, because running over cyclists is quite bad. An explanation might reveal that the most important feature learned is to recognise the two wheels of a bike and this explanation helps to think about edge cases like bikes with side bags, that partially cover the wheels.
- By default most machine learning models pick up biases from the training data. This can turn your machine learning models into racists which discriminate against protected groups. Interpretability is a useful debugging tool to **detect bias** in machine learning models.

Even in low risk environments, like movie recommendation, interpretability in the research and development stage is valuable. Also later when some model is used in a product, things can go wrong. And need for interpretability arises when your model fucks up. Because having an explanation for a faulty classification helps to understand the cause of the fault. It delivers a direction for how to fix the system. Consider an example of a husky versus wolf classifier, that misclassifies some huskies as wolfs. If there is an explanation to the classification you can see, that the misclassification happened due to the snow on the image. The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use.

If you can ensure that the machine learning model can explain decisions, following traits can also be checked more easily [@Doshi-Velez2017].

- Fairness: Unbiased, not discriminating against protected groups (implicit or explicit). An interpretable model can tell you why it decided a certain person is not worthy of a credit and for a human it becomes easy to decide if the decision was based on a learned demographic (e.g. racial) bias.
- Privacy: Sensitive information in the data is protected.
- Reliability or Robustness: Small changes in the input don't lead to big changes in the prediction.
- Causality: Only causal relationships are picked up. Meaning a predicted change in a decision due to arbitrary changes in the input values are also happening in reality.
- Trust: It is easier for humans to trust into a system that explains it's decisions compared to a black box


## Scope of interpretability
An algorithm trains a model, which produces the predictions. Each step can be evaluated in terms of transparency or interpretability.

###  Algorithm transparency
*How does the algorithm create the model?*

Algorithm transparency is about how the algorithm learns a model from the data and what kind of relationships it is capable of picking up. If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single predictions are made. For this level of transparency only knowledge about the algorithm and not about the data or concrete learned models are required. This book  focuses on model interpretability and not algorithm transparency.
Algorithms like the least squares method for linear models are well studied and understood. They score high in transparency. Deep learning approaches (pushing a gradient through a network with millions of weights) are less well understood and the inner workings are in the focus of on-going research. It is not clear how they exactly work, so they are less transparent.


### Global, holistic model interpretability
*How does the trained model make predictions?*

You could call a model interpretable if you can comprehend the whole model at once [@Lipton2016].
To explain the global model output, you need the trained model, knowledge about the algorithm and the data. This level of interpretability is about understanding how the model makes the decisions, based on a holistic view on it's features and each learned components like weights, parameters and structures. Which features are the important ones and what kind of interactions are happening? Global model interpretability helps to understand the distribution of your target variable based on the features.
Arguably, global model interpretability is very hard to achieve in practice. Any model that exceeds a handful of parameters or weights, probably won't fit an average human's brain short term memory. I'd argue that you cannot really imagine a linear model with 5 features and draw in your head the hyperplane that was estimated in the 5-dimensional feature space. Each feature space with more than 3 dimensions is just not imaginable for humans.
Usually when people try to comprehend a model, they look at parts of it, like the weights in linear models.

### Global model interpretability on a modular level
*How do parts of the model influence predictions?*


You might not be able to comprehend a Naive Bayes model with many hundred features, because there is no way you could
hold all the feature weights in your brain's working memory. But you can understand a single weight easily. Not many models are interpretable on a strict parameter level. While global model interpretability is usually out of reach, there is a better chance to understand at least some models on a modular level. In the case of linear models, the interpretable parts are the weights and the distribution of the features, for trees it would be splits (used feature plus the cut-off point) and leaf node predictions.
Linear models for example look like they would be perfectly interpretable on a modular level, but the interpretation of a single weight is interlocked with all of the other weights. As you will see in Chapter \@ref(limo), the interpretation of a single weight always comes with the footnote that the other input features stay at the same value, which is not the case in many real world applications. A linear model predicting the value of a house, which takes into account both the size of the house and the number of rooms might have a negative weight for the rooms feature, which is counter intuitive. But it can happen, because there is already the highly correlated flat size feature and in a market where people prefer bigger rooms, a flat with less rooms might be worth less than a flat with more rooms when both have the same size. The weights only make sense in the context of the other features used in the model. But arguably the weights in a linear model still have better interpretability than the weights of a deep neural network.


### Explain the prediction for a single instance
*Why did the model make a specific decision for an instance?*

You can zoom in on a single instance and examine what kind of prediction the model makes for this input, and why it made this decision. When you look at one example, the local distribution of the target variable might behave more nicely. Locally it might depend only linearly or monotonic on some features rather than having a complex dependence on the features. For example the value of an apartment might not depend linearly on the size, but if you only look at a specific apartment of 100 square meters and check how the prize changes going up plus and minus 10 square meters there is a chance that this subregion in your data space is linear. Local explanations can be more accurate compared to global explanations because of this.
This book presents methods that can make single predictions more interpretable in Chapter \@ref(agnostic).

### Explain the predictions for a group of instances
*Why did the model make specific decisions for a group of instances?*

The model predictions for multiple instances can be explained by either using methods for global model interpretability (on a modular level) or single instance explanations. The global methods can be applied by taking the group of observations pretending it's the complete dataset and using the global methods on this subset. The single explanation methods can be used on each instance and listed or aggregated afterwards for the whole group.

## Evaluating interpretability
There is no real consensus what interpretability in machine learning is. Also it is not clear how to measure it. But there is some first research on it and the attempt to formulate some approaches for the evaluation, as described in the following section.


### Approaches for evaluating the explanation quality
(@Doshi-Velez2017) propose three major levels of evaluating explainability.

- **Application level evaluation (real task)**: Put the explanation into the product and let the end user test it. For example, on an application level, radiologists would test fracture detection software (which includes a machine learning component to suggest where fractures might be in an x-ray image) directly in order to evaluate the model. This requires a good experimental setup and an idea of how to assess the quality. A good baseline for this is always how good a human would be at explaining the same decision.
- **Human level evaluation (simple task)** is a  simplified application level evaluation. The difference is that these experiments are not conducted with the domain experts, but with lay humans. This makes experiments less expensive (especially when the domain experts are radiologists) and it is easier to find more humans. An example would be to show a user different explanations and the human would choose the best.
- **Function level evaluation (proxy task)** does not require any humans. This works best when the class of models used is already evaluated by someone else in a human level evaluation. For example it might be known that the end users understand decision trees. In this case, a proxy for explanation quality might be the depth of the tree. Shorter trees would get a better explainability rating. It would make sense to add the constraint that the predictive performance of the tree remains good and does not drop too much compared to a larger tree.


#### More on function level evaluation
Model size is an easy way to measure explanation quality, but it is too simplistic.
For example, a sparse model with features that are themselves not interpretable is still not a good explanation.

There are more dimensions to interpretability:

- Model sparsity: How many features are being used by the explanation?
- Monotonicity: Is there a monotonicity constraint? Monotonicity means that a feature has a monotonic relationship with the target. If the feature increases, the target either always increases or always decreases, but never switches between increasing and decreasing.
- Uncertainty: Is a measurement of uncertainty part of the explanation?
- Interactions: Is the explanation able to include interactions of features?
- Cognitive processing time: How long does it take to understand the explanation?
- Feature complexity: What features were used for the explanation? PCA components are harder to understand than word occurrences, for example.
- Description length of explanation
