<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2018-06-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ice.html">
<link rel="next" href="feature-importance.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> Storytime</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#lightning-never-strikes-twice"><i class="fa fa-check"></i>Lightning Never Strikes Twice</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#trust-fall"><i class="fa fa-check"></i>Trust Fall</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#fermis-paperclips"><i class="fa fa-check"></i>Fermi’s Paperclips</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>1.2</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="1.3" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>1.3</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> Interpretability</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> The Importance of Interpretability</a></li>
<li class="chapter" data-level="2.2" data-path="criteria-for-interpretability-methods.html"><a href="criteria-for-interpretability-methods.html"><i class="fa fa-check"></i><b>2.2</b> Criteria for Interpretability Methods</a></li>
<li class="chapter" data-level="2.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>2.3</b> Scope of Interpretability</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.3.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="2.3.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>2.3.2</b> Global, Holistic Model Interpretability</a></li>
<li class="chapter" data-level="2.3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>2.3.3</b> Global Model Interpretability on a Modular Level</a></li>
<li class="chapter" data-level="2.3.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>2.3.4</b> Local Interpretability for a Single Prediction</a></li>
<li class="chapter" data-level="2.3.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-prediction"><i class="fa fa-check"></i><b>2.3.5</b> Local Interpretability for a Group of Prediction</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html"><i class="fa fa-check"></i><b>2.4</b> Evaluating Interpretability</a><ul>
<li class="chapter" data-level="2.4.1" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html#approaches-for-evaluating-the-interpretability-quality"><i class="fa fa-check"></i><b>2.4.1</b> Approaches for Evaluating the Interpretability Quality</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.5</b> Human-style Explanations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>2.5.1</b> What is an explanation?</a></li>
<li class="chapter" data-level="2.5.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>2.5.2</b> What is a “good” explanation?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Bike Sharing Counts (Regression)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube Spam Comments (Text Classification)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> Risk Factors for Cervical Cancer (Classification)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> Interpretable Models</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> Linear Model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>4.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>4.1.2</b> Interpretation Example</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>4.1.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>4.1.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>4.1.5</b> Explaining Single Predictions</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>4.1.6</b> Coding Categorical Features</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>4.1.7</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>4.1.8</b> Do linear models create good explanations?</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#extending-linear-models"><i class="fa fa-check"></i><b>4.1.9</b> Extending Linear Models</a></li>
<li class="chapter" data-level="4.1.10" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.10</b> Sparse linear models</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#whats-wrong-with-linear-regression-models-for-classification"><i class="fa fa-check"></i><b>4.2.1</b> What’s Wrong with Linear Regression Models for Classification?</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic.html"><a href="logistic.html#logistic-regression"><i class="fa fa-check"></i><b>4.2.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>4.2.3</b> Interpretation</a></li>
<li class="chapter" data-level="4.2.4" data-path="logistic.html"><a href="logistic.html#example"><i class="fa fa-check"></i><b>4.2.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.3</b> Decision Tree</a><ul>
<li class="chapter" data-level="4.3.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>4.3.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.3.2" data-path="tree.html"><a href="tree.html#interpretation-example-1"><i class="fa fa-check"></i><b>4.3.2</b> Interpretation Example</a></li>
<li class="chapter" data-level="4.3.3" data-path="tree.html"><a href="tree.html#advantages"><i class="fa fa-check"></i><b>4.3.3</b> Advantages</a></li>
<li class="chapter" data-level="4.3.4" data-path="tree.html"><a href="tree.html#disadvantages"><i class="fa fa-check"></i><b>4.3.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.4</b> RuleFit</a><ul>
<li class="chapter" data-level="4.4.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>4.4.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="4.4.2" data-path="rulefit.html"><a href="rulefit.html#guidelines"><i class="fa fa-check"></i><b>4.4.2</b> Guidelines</a></li>
<li class="chapter" data-level="4.4.3" data-path="rulefit.html"><a href="rulefit.html#theory"><i class="fa fa-check"></i><b>4.4.3</b> Theory</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html"><i class="fa fa-check"></i><b>4.5</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="4.5.1" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>4.5.1</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="4.5.2" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>4.5.2</b> K-Nearest Neighbours</a></li>
<li class="chapter" data-level="4.5.3" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html#and-so-many-more"><i class="fa fa-check"></i><b>4.5.3</b> And so many more …</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>5.1.1</b> Examples</a></li>
<li class="chapter" data-level="5.1.2" data-path="pdp.html"><a href="pdp.html#advantages-1"><i class="fa fa-check"></i><b>5.1.2</b> Advantages</a></li>
<li class="chapter" data-level="5.1.3" data-path="pdp.html"><a href="pdp.html#disadvantages-1"><i class="fa fa-check"></i><b>5.1.3</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ice.html"><a href="ice.html#example-1"><i class="fa fa-check"></i><b>5.2.1</b> Example</a></li>
<li class="chapter" data-level="5.2.2" data-path="ice.html"><a href="ice.html#advantages-2"><i class="fa fa-check"></i><b>5.2.2</b> Advantages</a></li>
<li class="chapter" data-level="5.2.3" data-path="ice.html"><a href="ice.html#disadvantages-2"><i class="fa fa-check"></i><b>5.2.3</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.3</b> Feature Interaction</a><ul>
<li class="chapter" data-level="5.3.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>5.3.1</b> Feature Interaction?</a></li>
<li class="chapter" data-level="5.3.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>5.3.2</b> Theory: Friedman’s H-statistic</a></li>
<li class="chapter" data-level="5.3.3" data-path="interaction.html"><a href="interaction.html#examples-1"><i class="fa fa-check"></i><b>5.3.3</b> Examples</a></li>
<li class="chapter" data-level="5.3.4" data-path="interaction.html"><a href="interaction.html#advantages-3"><i class="fa fa-check"></i><b>5.3.4</b> Advantages</a></li>
<li class="chapter" data-level="5.3.5" data-path="interaction.html"><a href="interaction.html#disadvantages-3"><i class="fa fa-check"></i><b>5.3.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.3.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>5.3.6</b> Implementations</a></li>
<li class="chapter" data-level="5.3.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>5.3.7</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.4</b> Feature Importance</a><ul>
<li class="chapter" data-level="5.4.1" data-path="feature-importance.html"><a href="feature-importance.html#the-theory"><i class="fa fa-check"></i><b>5.4.1</b> The Theory</a></li>
<li class="chapter" data-level="5.4.2" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>5.4.2</b> Example and Interpretation</a></li>
<li class="chapter" data-level="5.4.3" data-path="feature-importance.html"><a href="feature-importance.html#advantages-4"><i class="fa fa-check"></i><b>5.4.3</b> Advantages</a></li>
<li class="chapter" data-level="5.4.4" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-4"><i class="fa fa-check"></i><b>5.4.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.5</b> Global Surrogate Models</a><ul>
<li class="chapter" data-level="5.5.1" data-path="global.html"><a href="global.html#theory-1"><i class="fa fa-check"></i><b>5.5.1</b> Theory</a></li>
<li class="chapter" data-level="5.5.2" data-path="global.html"><a href="global.html#example-3"><i class="fa fa-check"></i><b>5.5.2</b> Example</a></li>
<li class="chapter" data-level="5.5.3" data-path="global.html"><a href="global.html#advantages-5"><i class="fa fa-check"></i><b>5.5.3</b> Advantages</a></li>
<li class="chapter" data-level="5.5.4" data-path="global.html"><a href="global.html#disadvantages-5"><i class="fa fa-check"></i><b>5.5.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.6</b> Local Surrogate Models (LIME)</a><ul>
<li class="chapter" data-level="5.6.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>5.6.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="5.6.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>5.6.2</b> LIME for Text</a></li>
<li class="chapter" data-level="5.6.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.6.3</b> LIME for Images</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.7</b> Shapley Value Explanations</a><ul>
<li class="chapter" data-level="5.7.1" data-path="shapley.html"><a href="shapley.html#the-general-idea"><i class="fa fa-check"></i><b>5.7.1</b> The general idea</a></li>
<li class="chapter" data-level="5.7.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>5.7.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="5.7.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>5.7.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="5.7.4" data-path="shapley.html"><a href="shapley.html#advantages-6"><i class="fa fa-check"></i><b>5.7.4</b> Advantages</a></li>
<li class="chapter" data-level="5.7.5" data-path="shapley.html"><a href="shapley.html#disadvantages-6"><i class="fa fa-check"></i><b>5.7.5</b> Disadvantages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>6</b> A Look into the Crystal Ball</a><ul>
<li class="chapter" data-level="6.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>6.1</b> The Future of Machine Learning</a></li>
<li class="chapter" data-level="6.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>6.2</b> The Future of Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>7</b> Contribute</a></li>
<li class="chapter" data-level="8" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>8</b> Acknowledgements</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interaction" class="section level2">
<h2><span class="header-section-number">5.3</span> Feature Interaction</h2>
<p>When features in a prediction model interact with each other, then the influence of the features on the prediction is not additive but more complex. It follows that Aristotle’s predicate “the whole is greater than the sum of its parts.” only applies in the presence of feature interactions.</p>
<div id="feature-interaction" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Feature Interaction?</h3>
<p>When a machine learning model makes a prediction based on two features, we can decompose the prediction into four terms: a constant term, one term for the first feature, one for the second feature and one for the interaction effect between the two features.<br />
The interaction between two features is the change in the prediction that occurs by varying the features, after having accounted for the individual feature effects. It’s the effect that comes on top of the sum of the individual feature effects.</p>
<p>For example: a model predicts the value of a house, using house size (big or small) and location (good or bad) as features, amounting to four possible predictions:</p>
<table>
<thead>
<tr class="header">
<th align="right">Location</th>
<th align="right">Size</th>
<th align="right">Predicted value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">good</td>
<td align="right">big</td>
<td align="right">300,000</td>
</tr>
<tr class="even">
<td align="right">good</td>
<td align="right">small</td>
<td align="right">200,000</td>
</tr>
<tr class="odd">
<td align="right">bad</td>
<td align="right">big</td>
<td align="right">250,000</td>
</tr>
<tr class="even">
<td align="right">bad</td>
<td align="right">small</td>
<td align="right">150,000</td>
</tr>
</tbody>
</table>
<p>We decompose the model prediction into the following parts: A constant term (150,000), an effect for the size feature (+100,000 if big, +0 if small) and an effect for the location (+50,000 if good, +0 if bad). This decomposition fully explains the model predictions. There is no interaction effect, because the model prediction is a sum of the single feature effects for size and location. Making a small house big always adds 100,000 to the predicted value, no matter the location. Also the difference in predicted value between a good and a bad location is 50,000, independent of the size.</p>
<p>Now let’s consider an example with interaction:</p>
<table>
<thead>
<tr class="header">
<th align="right">Location</th>
<th align="right">Size</th>
<th align="right">Predicted value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">good</td>
<td align="right">big</td>
<td align="right">400,000</td>
</tr>
<tr class="even">
<td align="right">good</td>
<td align="right">small</td>
<td align="right">200,000</td>
</tr>
<tr class="odd">
<td align="right">bad</td>
<td align="right">big</td>
<td align="right">250,000</td>
</tr>
<tr class="even">
<td align="right">bad</td>
<td align="right">small</td>
<td align="right">150,000</td>
</tr>
</tbody>
</table>
<p>We decompose the prediction table into the following parts: A constant term (150,000), an effect for the size feature (+100,000 if big, +0 if small) and an effect for the location (+50,000 if good, +0 if bad). For this table, we need an extra term for the interaction: +100,000 if the house is big and in a good location. This is an interaction between the size and the location, because in this case, the difference in predicted value between a big and a small house depends on the location.</p>
<p>One way to estimate the interaction strength is to measure how much of the variation of the predicted outcome depends on the interaction of the features. This measurement is called H-statistic, introduced by Friedman and Popescu (2008)<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a></p>
</div>
<div id="theory-friedmans-h-statistic" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Theory: Friedman’s H-statistic</h3>
<p>We will look into two cases: The interaction between two features, which tells us if and how strongly two specific features interact with each other in the model; The interaction between a feature and all other features, which tells us if and how strongly (in total) the specific feature interacts in the model with all the other features. In theory, arbitrary interactions between any number of features can be measured, but those two cases represent the most interesting interaction cases.</p>
<p>If two features <span class="math inline">\(x_j\)</span> and <span class="math inline">\(x_k\)</span> don’t interact, we can decompose the <a href="pdp.html#pdp">partial dependence function</a> in the following way (assuming that the partial dependence functions are centered at zero):</p>
<p><span class="math display">\[PD_{jk}(x_j,x_k)=PD_j(x_j)+PD_k(x_k)\]</span></p>
<p>where <span class="math inline">\(PD_{jk}(x_j,x_k)\)</span> is the 2-way partial dependence function of both features and <span class="math inline">\(PD_j(x_j)\)</span> and <span class="math inline">\(PD_k(x_k)\)</span> the partial dependence functions of the single features.</p>
<p>Similarly, if a feature <span class="math inline">\(x_j\)</span> has no interaction with any of the other features, we can express the prediction function <span class="math inline">\(\hat{f}(x)\)</span> as a sum of partial dependence functions, where the first summand only depends on <span class="math inline">\(x_j\)</span> and the second depends on all other features excluding <span class="math inline">\(x_j\)</span>:</p>
<p><span class="math display">\[\hat{f}(x)=PD_j(x_j)+PD_{-j}(x_{-j})\]</span> where <span class="math inline">\(PD_{-j}(x_{-j})\)</span> is the partial dependence function that depends on all features excluding <span class="math inline">\(x_j\)</span>.</p>
<p>This decomposition expresses the partial dependence (or full prediction) function without interactions (between features <span class="math inline">\(x_j\)</span> and <span class="math inline">\(x_k\)</span> or, respectively, <span class="math inline">\(x_j\)</span> and <span class="math inline">\(x_{-j}\)</span>). In a next step we measure the difference between the observed partial dependence function and the decomposed one without interactions. We calculate the variance of the output of the partial dependence (for measuring the interaction between two features) or of the complete function (for measuring the interaction between a feature and all other features). The amount of the variance that is explained by the interaction (difference between observed and no-interaction PD) is used as the interaction strength statistic. The statistic is 0 when there is no interaction at all and 1 if all of the variance of the <span class="math inline">\(PD_{jk}\)</span> or <span class="math inline">\(\hat{f}\)</span> is explained by the sum of the partial dependence functions. An interaction statistic of 1 between two features means that each single <span class="math inline">\(PD\)</span> function is constant and the effect on the prediction only comes through the interaction.</p>
<p>In mathematical terms, the H-statistic for the interaction between feature <span class="math inline">\(x_j\)</span> and <span class="math inline">\(x_k\)</span> proposed by Friedman and Popescu is:</p>
<p><span class="math display">\[H^2_{jk}=\sum_{i=1}^n\left[PD_{jk}(x_{j}^{(i)},x_k^{(i)})-PD_j(x_j^{(i)})-PD_k(x_{k}^{(i)})\right]/\sum_{i=1}^n{PD}^2_{jk}(x_j^{(i)},x_k^{(i)})\]</span></p>
<p>Similarly for measuring if a feature <span class="math inline">\(x_j\)</span> interacts with any other feature:</p>
<p><span class="math display">\[H^2_{j}=\sum_{i=1}^n\left[\hat{f}(x^{(i)})-PD_j(x_j^{(i)})-PD_{-j}(x_{-j}^{(i)})\right]/\sum_{i=1}^n\hat{f}^2(x^{(i)})\]</span></p>
<p>The H-statistic is expensive to evaluate, because it iterates over all data points and at each point the partial dependence has to be evaluated which is done using - again - all <span class="math inline">\(n\)</span> data points. In the worst case, we need <span class="math inline">\(2n^2\)</span> calls to the machine learning models predict function to compute the <span class="math inline">\(H^2_j\)</span>-statistic and <span class="math inline">\(3n^2\)</span> for the <span class="math inline">\(H^2_{jk}\)</span>-statistic. To speed up the computation, we can sample from the <span class="math inline">\(n\)</span> data points . This has the drawback that it adds variance to the partial dependence estimates, which makes the statistic unstable. So when you are using sampling to reduce the computational burden, make sure to sample enough data points.</p>
<p>In their paper, Friedman and Popescu also propose a test for the H-statistic being significantly different from zero: The Null hypothesis is the absence of interaction. To generate the interaction statistic under the Null hypothesis, you have to be able to adjust the model so that it has no interaction between feature <span class="math inline">\(x_j\)</span> and <span class="math inline">\(x_k\)</span> or all others. This is not possible for all types of models, so running this test is model-specific and not model-agnostic and as such not covered here.</p>
<p>The interaction strength statistic can also be applied in a classification setting, when the predicted outcome is the probability for a class.</p>
</div>
<div id="examples-1" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Examples</h3>
<p>Let’s see how feature interactions look like in practice! We measure the interaction strength of features in a support vector machine that predicts the number of <a href="bike-data.html#bike-data">bike rentals</a>, given weather and calendrical features. The following plot shows the results of the feature interaction analysis:</p>
<div class="figure"><span id="fig:interaction-bike"></span>
<img src="images/interaction-bike-1.png" alt="The interaction strength for each feature with all other features for a support vector machine predicting bike rentals. Overall the interaction effects between the features are very weak (below 1 percent of variance explained by each feature)." width="1050" />
<p class="caption">
FIGURE 5.9: The interaction strength for each feature with all other features for a support vector machine predicting bike rentals. Overall the interaction effects between the features are very weak (below 1 percent of variance explained by each feature).
</p>
</div>
<p>In this next example we calculate the interaction statistic for a classification problem, where we deal with the partial dependence of the predicted probability. We analyse the interactions between features in a random forest that is trained to predict <a href="cervical.html#cervical">cervical cancer</a>, given some risk factors.</p>
<div class="figure"><span id="fig:interaction-cervical-include"></span>
<img src="images/interaction-cervical-1.png" alt="The interaction strength for each feature with all other features for a random forest predicting the probability of cervical cancer. The number of diagnosed sexually transmitted diseases has the highest interaction effect with all other features, followed by the number of pregnancies."  />
<p class="caption">
FIGURE 5.10: The interaction strength for each feature with all other features for a random forest predicting the probability of cervical cancer. The number of diagnosed sexually transmitted diseases has the highest interaction effect with all other features, followed by the number of pregnancies.
</p>
</div>
<p>After looking into the feature interactions of each feature with all other features, we can pick one of the features and specifically dive deeper into all the 2-way interactions between the chosen feature with the other features:</p>
<div class="figure"><span id="fig:interaction2-cervical-age-include"></span>
<img src="images/interaction2-cervical-age-1.png" alt="The 2-way interactions between number of pregnancies with each other feature. There is a strong interaction between the number of pregnancies and the age."  />
<p class="caption">
FIGURE 5.11: The 2-way interactions between number of pregnancies with each other feature. There is a strong interaction between the number of pregnancies and the age.
</p>
</div>
</div>
<div id="advantages-3" class="section level3">
<h3><span class="header-section-number">5.3.4</span> Advantages</h3>
<ul>
<li>The interaction statistic has an <strong>underlying theory</strong> through the partial dependence decomposition.</li>
<li>The H-statistic has a <strong>meaningful interpretation</strong>: The interaction is defined as the portion of variance explained by the interaction.</li>
<li>Since the statistic is <strong>dimensionless and always between 0 and 1</strong>, it is comparable across features and even across models.</li>
<li>The statistic <strong>detects all kinds of interactions</strong>, regardless of a specific form.</li>
<li>With the H-statistic it is also possible to analyze arbitrary <strong>higher interactions</strong>: For example the interaction strength between 3 or more features.</li>
</ul>
</div>
<div id="disadvantages-3" class="section level3">
<h3><span class="header-section-number">5.3.5</span> Disadvantages</h3>
<ul>
<li>The first thing you will notice: The interaction H-statistic takes a long time to compute, because it’s <strong>computationally expensive</strong>.</li>
<li>Sometimes the results are weird and for small simulations <strong>don’t yield the expected results</strong>. But this is more anecdotal evidence.</li>
<li>The computation involves estimating marginal distributions. These <strong>estimates also have some variance</strong>, when we don’t use all of the data points. This means when we sample points, the estimates will also vary from run to run and the results might <strong>become unstable</strong>. I recommend repeating it a few times to see if you have enough data included for a stable result.</li>
<li>It is unclear whether an interaction is significantly bigger than 0. We would need to conduct a statistical test, but this <strong>test is not (yet) available in a model-agnostic version</strong>.</li>
<li>Connected to the testing problem: It’s hard to tell when the H-statistic is large enough that we would consider it a strong interaction.</li>
<li>The H-statistic tells us how strong the interactions are, but it doesn’t tell us how the interaction is shaped. That’s what <a href="pdp.html#pdp">partial dependence plots</a> are for. A meaningful workflow is to measure the interaction strengths and then create 2D-partial dependence plots for the interactions in which you are interested.</li>
<li>The H-statistic can’t be meaningfully applied if the inputs are pixels.</li>
<li>The interaction statistic works under the assumption that we can independently shuffle features (the same problem that partial dependence plots have). When the features strongly correlate, the assumption is violated and <strong>we integrate over feature combinations that are very unlikely in reality</strong>.</li>
</ul>
</div>
<div id="implementations" class="section level3">
<h3><span class="header-section-number">5.3.6</span> Implementations</h3>
<ul>
<li>For the examples in this book, I used the R package <code>iml</code>, which is available on <a href="https://cran.r-project.org/web/packages/iml">CRAN</a> and the development version on <a href="https://github.com/christophM/iml">Github</a>.</li>
<li>There are other implementations, which focus on a specific model:
<ul>
<li>The R package <a href="https://cran.r-project.org/web/packages/pre/index.html">pre</a> implements the <a href="rulefit.html#rulefit">RuleFit algorithm</a> plus the H-statistic.</li>
<li>The R package <a href="https://github.com/gbm-developers/gbm3">gbm</a> implements gradient boosted models plus the H-statistic.</li>
</ul></li>
</ul>
</div>
<div id="alternatives" class="section level3">
<h3><span class="header-section-number">5.3.7</span> Alternatives</h3>
<p>The H-statistic is not the only way to measure interactions, there is also:</p>
<ul>
<li>Variable Interaction Networks (VIN) by Hooker (2004)<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a>: An approach that decomposes the prediction function into main effects and feature interactions. The interactions between features are then visualized as a network. Unfortunately, there is no software available yet.</li>
</ul>
<div class="figure">
<img src="images/vin.png" />

</div>
<ul>
<li>Partial dependence based feature interaction by Greenwell et. al (2018)<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a>: For measuring the interaction between two features, this approach measures the feature importance (defined as the variance of the 1D partial dependence function) of one feature conditional on different, fixed points of the other feature. When the variance is high, then the features interact with each other, if it is zero, they don’t interact. The R package <code>vip</code> is available on <a href="https://github.com/koalaverse/vip">Github</a>. They also cover partial dependence plots and feature importance.</li>
</ul>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="21">
<li id="fn21"><p>Friedman, Jerome H, and Bogdan E Popescu. 2008. “Predictive Learning via Rule Ensembles.” The Annals of Applied Statistics. JSTOR, 916–54.<a href="interaction.html#fnref21">↩</a></p></li>
<li id="fn22"><p>Hooker, G. (2004). Discovering Additive Structure in Black Box Functions. Knowledge Discovery and Data Mining, 575–580. <a href="http://doi.org/10.1145/1014052.1014122" class="uri">http://doi.org/10.1145/1014052.1014122</a><a href="interaction.html#fnref22">↩</a></p></li>
<li id="fn23"><p>Greenwell, B. M., Boehmke, B. C., &amp; McCarthy, A. J. (2018). A Simple and Effective Model-Based Variable Importance Measure, 1–27. Retrieved from <a href="http://arxiv.org/abs/1805.04755" class="uri">http://arxiv.org/abs/1805.04755</a><a href="interaction.html#fnref23">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ice.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="feature-importance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/05.4-agnostic-interaction.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
