# Definitions {#definitions}
To avoid confusion through ambiguity, here are some definitions of terms used in this book.

- An **Algorithm** is a set of rules that a machine follows to achieve a particular goal [@algorithm]
- A **Machine learning algorithm** is a set of rules that a machine follows to learn how to a achieve a particular goal. The output of a machine learning algorithm is a machine learning model.
- A **(Machine learning) Model** is the outcome of a machine learning algorithm. This can be a set of weights for a linear model or neural network plus the information about the architecture.
- **Dataset**: A table containing the data from which the machine learns.
- **Features**: The variables/information used for prediction/classification/clustering. A feature is one column in the dataset.
- **Target**: The thing the machine learns to predict.
- **(machine learning) Task**: The combination of a dataset with features and a target. Depending on the type of the target, the task can be classification, regression, survival analysis, clustering or outlier detection.
- **Prediction**: The machine learning models "guess" what the target's value should be based on the given features. 
- **Instance**: One row in the dataset.

# Interpretable models {#simple}
The most straight forward way to achieve explainable machine learning algorithms is to use only a subset algorithms that yield an understandable model structure.

These are:

* Linear models (sparse)
* Decision trees
* Decision rules

In the following chapters  we will talk about the algorithm with it's variants. Not in detail, only the basics, because there are already a ton of books, videos, tutorials, papers and so on about them. We will focus on how to interpret the models and why they are explainable.
The chapter covers linear models, decision trees, decision rules, neighbour methods and graphical models.

## Terminology

- Y is the target variable in supervised settings.
- X are the features or covariates.
- w are the weights.
- $\beta$ are regression weights.


## Overview

| Algorithm |Linear |Monotonicity |Interaction built-in |
|:--------------|:----|:----|:------|
| Linear models | Yes | Yes | No |
| Decision trees | No | Not by default | Yes |
| Decision rules | No | Not by default | Yes |
| Naive bayes | Yes | Yes | No |
| Nearest neighbours | No | No | No |

## Linear models {#limo}

Linear models have been and are still used by statistician, computer scientists and other people with quantitative problems. They learn straightforward linear (and monotonic) relationships between the target and the features. The target changes by a learned weight depending on the feature. Monotonicity makes the interpretation easy.


Linear models can be used to model the dependency of a regression variable (here Y) on K covariates. As the name says, the learned relationships are linear in the form of

$$y_{i} = \beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{K} x_{i,K} + \epsilon_{i}$$

The i-th observation's outcome is a weighted sum of it's K features. The $\beta_{k}$ represent the learned feature weights or coefficients. The $\epsilon_{i}$ is the error we are still making, the difference between the predicted and actual outcome.

The biggest advantage is the linearity: It makes the estimation procedure straight forward and most importantly these linear equations have an easy to understand interpretation. That is one of the main reasons why the linear model and all it's descendants are so widespread in academic fields like medicine, sociology, psychology and many more quantitative research fields. In this areas it is important to not only predict e.g. the clinical outcome of a patient, but also quantify the influence of the medication while at the same time accounting for things like sex, age and other variables.

Linear regression models also come with some assumptions that make them easy to use and interpret but are often not given in reality.
The assumptions are: linearity, normality, homoscedasticity,  independence, fixed features, abscence of multicollinearity.
**Linearity**: Linear regression models allow the mean of the response to be only a linear combination of the features, which is both the greatest strength and biggest limitation. Linearity makes the estimation procedure easy. Also linearity leads to interpretable models: linear effects are simple to quantify and describe (see also next chapter) and are additive, so it is easy to separate the effects. If you suspect interactions of features or a non-linear association of a feature with the target value, then you can add interaction terms and things like regression splines to estimate non-linear effects.
**Normality**: The target value given the features are assumed to follow a normal distribution. If this assumption is violated, then the estimated confidence intervals of the feature weights are not valid. Any interpretation of the p-values (p-value = Probability that the confidence interval of the feature weight covers the 0) is not valid.
**Homoscedasticity** (constant variance): The variance of the error terms $$\epsilon_{i}$$ is assumed to be constant along the whole feature space. Let's say you want to predict the value of a house given the living area in square meters. You estimate a linear model, which assumes that no matter how big the flat, the error terms around the predicted response have the same variance. This assumption is in reality often violated. In the house example it is plausible that the variance of error terms around the predicted price is higher in bigger houses, since also the prices are higher and there is more wriggle room for prices to vary.
**Indepence**: Each observation is assumed to be independent from the next one. If you have repeated measurements, like multiple records per patient, the data points are not independent from each other and there are special linear model classes to deal with these cases, like mixed effect models or GEEs.
**Fixed features**: The input features are seen as 'fixed', carrying no errors or variation, which, of course, is very unrealistic and only makes sense in controlled experimental settings. But not assuming fixed features would mean that you have to fit very complex measurement error models that account for the measurement errors. And usually you don't want to do that.
**Abscence of multicollinearity**: Basically you don't want features to be highly correlated, because this messes up the estimation of your models. In a situation where two variables are highly correlated (something >0.9) the linear model will have problems estimating the weights, since the models are additive and the model does not know to which feature to attribute the effects.

### Interpretation
The interpretation of the coefficients:

- Continuous regression variable: For an increase of one point of the variable $x_{j}$ the estimated outcome changes by $\beta_{j}$
- Binary categorical variables: One of the variables is the reference level (in some languages the one that was coded in 0). A change of the variable $x_{i}$ the reference level to the other category changes the estimated outcome by $\beta_{i}$
- categorical variables with many levels: One solution to deal with many variables is to one-hot-encode them, meaning each level gets it's own column. From a categorical variable with L levels, you only need L-1 columsn, otherwise it is over parameterized. The interpretation for each level is then according to the binary variables. Some language like R allow to
- Intercept $\beta_{0}$: The interpretation is: Given all continuous variables are zero and the categorical variables are on the reference level, the estimated outcome of $y_{i}$ is $\beta_{0}$. The interpretation of $\beta_{0}$ is usually not relevant.


Another important measurement for interpreting linear models is the $R^2$ measurement. $R^2$ tells you how much of the total variance of your target variable is explained by the model. The higher $R^2$ the better your model explains the data. The formula to calculate $R^2$ is: $R^2 = 1 - SSE/SST$, where SSE is the squared sum of the error terms ($SSE = \sum_{i=1}^n (y_i - \hat{y}_i)^2$) and SST is the squared sum of the data variance ($SST = \sum_{i=1}^n (y_i - \bar{y})^2$). $R^2$ ranges between 0 for models that explain nothing and 1 for models that explain all of the datas variance.

There is a catch, because $R^2$ increases with the number of features in the model, even if they carry no information about the target value at all. So it is better to use the adjusted R-squared $\bar{R}^2$, which accounts for number of features used in the model. It's calculation is $\bar{R}^2 = R^2 - (1-R^2)\frac{p}{n - p - 1}$, where p is the number of features and n the number of observations.

It isn't helpful to do interpretation on a model with very low $R^2$ or $\bar{R}^2$, because basically the model is not explaining much of the variance, so any interpretation of the weights are not meaningful.


### Interpretation example
We now use the linear model to predict the bike rentals on a day, given weather and calendrical information.
```{r linear_model}
features_of_interest = c('season','holiday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed', 'days_since_2010')
X = bike.train[features_of_interest]
# colnames(X) = speed_dating_names_matches[colnames(X)]

y = bike.train[,'cnt']
dat = cbind(X, y)
#colnames(dat) = c(colnames(X), target_var_regress)
mod = lm(y ~ ., data = dat, x = TRUE)
lm_summary = summary(mod)$coefficients


pretty_rownames = function(rnames){
  rnames = gsub('^`', '', rnames)
  rnames = gsub('`$', '', rnames)
  rnames = gsub('`', ':', rnames)
  rnames
}

lm_summary_print = lm_summary
rownames(lm_summary_print) = pretty_rownames(rownames(lm_summary_print))

kable(lm_summary_print[,c('Estimate', 'Std. Error')])
```

Interpretation of a numerical variable ('Temperature'): An increase of the temperature of 1 degree Celsius increases the number of bikes by `r sprintf('%.2f', lm_summary_print['temp', 'Estimate'])` given all other features stay the same.

Interpretation of a categorical variable ('weathersituation')): The number of bikes is `r sprintf('%.2f', lm_summary_print['weathersitRAIN/SNOW/STORM', 'Estimate'])` lower when it is rainy, snowing or stormy, compared to good weather, given that all features stay the same. Also if the weather was only misty, the number of bike rentals was `r sprintf('%.2f', lm_summary_print['weathersitMISTY', 'Estimate'])` lower, compared to good weather, given all features stay the same.

As you can see in the interpretation examples, the interpretations are always coming with the clause that 'all other features stay the same'.
That's because of the nature of linear models: All features are input linearly into the function with no interactions (unless explicitly specified). The good side is, that is isolates the interpretation. If you think of the features as turn-switches that you can turn up or down, it is nice to see what happens when you would just turn the switch for one feature.

### Interpretation templates

**Interpretation of a numerical feature**:

An increase of $x_{k}$ by one unit increases the expectation for $y$ by $\beta_x{k}$ units if all other features X stay the same.

**Interpretation of a categorical feature**:

The category coded with 1 of $x_{k}$ increases the expectation for $y$ by $\beta_{k}$ compared to the reference category (coded with 0).


### Visual parameter interpretation

#### Weight plot
The information of the coefficient table can also be put into a visualization, which makes the weights and the uncertainty about them can be made understandable on one glance. The weight is displayed as a point and the 95% confidence interval around the point with a line. The 95% confidence interval means that if the linear model was repeated 100 times on
```{r linear_weights}
coef_plot = function(mod, alpha = 0.05, remove_intercept = TRUE){
  lm_summary = summary(mod)$coefficients
  rownames(lm_summary) = pretty_rownames(rownames(lm_summary))

  df = data.frame(Features = rownames(lm_summary),
                  Estimate = lm_summary[,'Estimate'],
                  std_error = lm_summary[,'Std. Error'])
  df$lower = df$Estimate - qnorm(alpha/2) * df$std_error
  df$upper = df$Estimate + qnorm(alpha/2) * df$std_error


  if(remove_intercept){
    df = df[!(df$Features == '(Intercept)'),]
  }
  ggplot(df) +
    geom_point(aes(x=Estimate, y=Features)) +
    geom_segment(aes(y=Features, yend=Features, x=lower, xend=upper), arrow = arrow(angle=90, ends='both', length = unit(0.1, 'cm'))) +
    my_theme()
}

coef_plot(mod)

```
TODO: Add interpetation

#### Effect plot
The weights of the linear model only have meaning, when combined with the actual features.
The weights depend on the scale of the features and will be different if you have a features measuring some height and you switch from inches to centemeters.
The weight will change, but the actual relationships in your data will not.
Also it is important to know the distribution of your feature in the data, because if you have a very low variance, it means that almost all instances will get a similar contribution from this feature.
The effect plot can help to understand how much the combination of a weight and a feature contributes to the predictions in your data.
Start with the computation of the effects, which is the weight per feature times the feature of an instance: $eff_{i,k} = w_{k} \cdot x_{i,k}$.
The resulting effects are visualized with boxplots:
The box contains the effect range for half of your data (25% to 75% effect quantiles).
The line in the box is the median effect, so 50% of the instances have a lower and the other half a higher effect on the prediction than the median value.
The whiskers are $+/i 1.58 IQR / \sqrt{n}$, with IQR being the inter quartile range ($q_{0.75} - q_{0.25}).
The points are outlier to the whiskers.

```{r linear_effects}

get_reference_dataset = function(dat){
  df = lapply(dat, function(feature){
    if(class(feature) == 'factor'){
      factor(levels(feature)[1], levels = levels(feature))
    } else {
      0
    }
  })
  data.frame(df)
}

get_effects = function(mod, dat){

  X = data.frame(predict(mod, type = 'terms'))

  # Nicer colnames
  colnames(X) = gsub('^X\\.', '', colnames(X))
  colnames(X) = gsub('\\.', ' ', colnames(X))

  # predict with type='terms' centers the results, so we have to add the mean again
  reference_X = predict(mod, newdata=get_reference_dataset(dat), type='terms')
  X_star = data.frame(t(apply(X, 1, function(x){ x - reference_X[1,names(X)]})))
  X_star
}

effect_plot = function(mod, dat,  feature_names=NULL){
  X = get_effects(mod, dat)
  if(!missing(feature_names)){
    rownames(X) = feature_names
  }
  X = gather(X)
  ggplot(X) +
    geom_boxplot(aes(x=key, y=value, group=key)) +
    coord_flip() + my_theme()
}

effect_plot(mod, dat)
```
The largest contributions are from temperature and the days variable, which capture the trend that the bike rental became more popular over time. The temperature has a high contribution distribution.
The day trend variable has goes from zero to large positive contribution, because the first day in the dataset (1.1.2011) get's a very low contribution, and the estimated weight with this feature is positive (`r sprintf('%.2f', lm_summary_print['days_since_2010', 'Estimate'])`), so the effect gets higher with every day and is highest for the latest day in the dataset (31.12.2012).
Note that for effects from a feature with a negative effect, the instances with a positive effect (or the least negatives) are the ones that have a negative feature value (negative times negative is positive), so days with a high positive effect of windspeed on the bike rental count have the lowest windspeeds.

### Explaining single predictions
Why did a certain instance get the prediction it got from the linear model?
This can again be answered by bringing together the weights and features and computing the effect. Now the effect will tell you how much each feature contributed towards the sum of the prediction. This is only meaningful if you compare the instance specific effects with the mean effects.
```{r linear_effects_single}
i = 6
effects = get_effects(mod, dat)
predictions = predict(mod)

effects_i = gather(effects[i, ])
predictions_mean = mean(predictions)
# For proper indexing, names have to be removed
names(predictions) = NULL
pred_i = predictions[i]

effect_plot(mod, dat) +
  geom_point(aes(x=key, y=value), color = 'red', data = effects_i) +
  ggtitle(sprintf('Predicted value for instance: %.2f\n Average predicted value: %.2f\nActual value: %.2f', pred_i, predictions_mean, y[i]))
```

Let's have a look at the effect realization for the rental bike count of one observation (= one day). Some features contribute unusually much to the predicted bike count: Temperature (`r X[i, 'temp']`) and the trend variable "days_since_2010", because this instance is from late 2011 (value = `r  X[i, 'days_since_2011']`).


### Coding categorical variables:
There are several ways to represent a categorical variable, which has an influence on the interpretation: http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/ and
http://heidiseibold.github.io/page7/

Described above is the treatment coding, which is usually sufficient.
Using different codings boils down to creating different matrices from your one column with the categorical feature.
I present three different codings, but there are many more. The example has six instances and one categorical feature with 3 levels. The first two instances are in category A, instances three and four are in category B and the last two instances are in category C.

- **Treatment coding** compares each level to the reference level. The intercept is the mean of the reference group. The first column is the intercept, which is always 1. Column two is an indicator whether instance $i$ is in category B, columns three is an indicator for category C. There is no need for a column for category A, because than the system would be over specified. Knowing that an instance is neither in category B or C is enough.
$$
\begin{pmatrix}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 1 & 0 \\
1 & 0 & 1 \\
1 & 0 & 1 \\
\end{pmatrix}
$$
- **Effect coding** compares each level to the overall mean of $y$. The first column is again the intercept. The weight $\beta_{0}$ which is associated to the intercept represents the overall mean and $\beta_{1}$, the weight for column two is the difference between the overall mean and category B. The overall effect of category B is $\beta_{0} + \beta_{1}$. Interpretation for category C is equivalent. For the reference category A, $-(\beta_{1} + \beta_{2})$ is the difference of the category C to the overall mean and $\beta_{0} -(\beta_{1} + \beta_{2})$ the overall effect of category C.
$$
\begin{pmatrix}
1 & -1 & -1 \\
1 & -1 & -1 \\
1 & 1 & 0 \\
1 & 1 & 0 \\
1 & 0 & 1 \\
1 & 0 & 1 \\
\end{pmatrix}
$$
- **Dummy coding** compares each level to the level mean of $y$. If all level are have the same frequency the resulting coefficients will be the same as in effect coding. Note that the intercept was dropped here.
$$
\begin{pmatrix}
1 & 0 & 0 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 1 \\
\end{pmatrix}
$$

### The disadvantages of linear models
They can only represent linear relationships as the name suggests. Each non-linearity or interaction has to be hand-crafted and explicitly given to the model as an input feature.
Because of possible high correlation between features, it is possible that a feature that is positively correlated with the outcome might get a negative weight in a linear model, because in the high dimensional space it is negatively correlated. An example: You have a model to predict the rent price and have features like number of rooms and size of the flat. Of course flat size and room number are highly correlated, the bigger a flat the more rooms it has. If you now take both variables into a linear model it might happen, that the flat size is the better predictor and get's a large positive weight. The room number might end up getting a negative weight, because given that a flat has the same size, increasing the number of rooms could make it less valuable.


### Towards complexer relationships within linear model class
- Adding interactions
- Adding non-linear terms like polynomials
- Stratifying data by variable and fitting linear models on subsets

## Sparse linear models
The examples for the linear models that I chose look all nice and tidy, right? But in reality you might not have just a handful of features, but hundreds or thousands.
And your normal linear models?
Interpretability goes downriver.
But there are ways to introduce sparsity (= only keeping a few features) into the linear models.
The most automatic and convient to use is the LASSO method.
LASSO stands for "least absolute shrinkage and selection operator" and when added to a linear model, it performs variable selection and regularization of the selected variables.
We did not dive into the optimization problem of finding the best coefficients of the linear model.
But basically it involves solving the least-squares equation:
$$ min_{\beta_0,\beta} \left( \frac{1}{n} \sum_{i=1}^n (y_i - \beta_0 - x_i^T \beta)^2\right)$$
LASSO adds a term to this optimization problem:
$$ min_{\beta_0,\beta} \left( \frac{1}{n} \sum_{i=1}^n (y_i - \beta_0 - x_i^T \beta)^2 + \lambda ||\beta||_1\right)$$
The term $||\beta||_1$ is the L1-norm of the feature vector, that leads to a penalization of large values in $\beta$.
Since the L1-norm is used, many of the coefficients for the features will get an estimate of 0 and the others are shrinked.
The weight $\lambda$ says how strong the regularizing effect should be and is usually tuned by doing cross-validation.
Especially when $\lambda$ is large, many coefficients are driven to 0.

There are lots of other methods for reducing the number of features in your linear regression model:

Methods that include a pre-processing step:
- Hand selected features: You can always use expert knowledge to choose and discard some features. The big drawback is, that it can't be automated and you might not be an expert.
- Use some measure to pre-select features: An example is the correlation coefficient. You only take features into account that exceed some chosen threshold of correlation between the feature and the target. Disadvantage is that it only looks at the features one at a time. Some features might only show correlation after the linear model has accounted for some other features. Those you will miss with this approach.

Then there are also step-wise procedures:
- Forward selection: Fit the linear model with one feature. Do that with each feature. Choose the model that works best (for example decided by R squared measurement). Now again, for the remaining features, fit different versions of your model by adding each feature. Pick the one that performs best again. Continue until some criterium is reached, like the maximum number of features in the model.
- Backward selection: Same as forward selection, but instead of adding features, start with the model with all features and try which feature removal brings the best performance increase until some stopping criterium is reached.

I recommend using LASSO. It also works for the logistic regression model for classification models, which is the topic of the following chapter.


## Logistic regression: a linear model for classification
Logistic regression is the linear regression models counterpart for classification problems.

### What's wrong with linear regression for classification?
The gaussian linear model works well in most regression setup, but fails in the classification case.
Why is that?
In case of two classes, you could label one of the classes with 0 and the other with a 1 and use a linear model on it and it would work.
There are a few problems with that approach:

- A linear model does try to give you probabilities, but it treats the classes as numbers (0 and 1) and fits the best hyperplane (if you have one feature, it's a line) that minimizes the distances between the points and the hyperplane. So it simply interpolates between the points, but there is no meaning in it and you cannot interpret it as probabilities.
- Also a linear model will extrapolate the features and give you values below zero and above one, which are not meaningful and should tell you that there might be a more clever approach to doing classification.
- Since the predicted outcome is not a probability but some linear interpolation between points there is no meaningful threshold at which you can distinguish one class from the other. A good illustration of this issue was given on (Stackoverflow)[https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression], which I reproduced in Figure \@ref{fig:linear-class-threshold}
- Linear models don't extend to classification problems with multiple classes. You would have to start giving labeling the next class with a 2, then 3 and so on. The classes might not have any order to them, but the linear model would force a weird structure on the relationship between the features and your class predictions. So for all features with a positive weight, the higher the features value the more they contribute to the prediction of a class with a higher number, even if the classes with similar numbers are not really related.


```{r, linear-class-threshold, fig.cap="An illustration why linear regression does not work well in a binary classification setting. A linear model is fitted on the artificial task of classifying a tumor as malignant (1) or benign (0) depending on the tumor size. Each point is a tumor, the x-axis shows the size of the tumor, the y-axis the malignancy, points are slightly jittered to avoid overplotting of points. The lines display the fitted curve from the linear model. In the data setting on the left, we can use 0.5 as a threshold for the predicted outcome of the linear model for seperating benign from malignant tumors. After introducing a few more malignant tumor cases, especially one with a large tumor size the regression line shifts and a threshold of 0.5 would not separate  the classes any longer. That's a reason why logistic regression is better suited for classification problems."}
df = data.frame(x = c(1,2,3,6,7,8,9),
                y = c(0,0,0,1,1,1,1),
                case = '0.5 threshold ok')

df_extra  = data.frame(x=c(df$x, 7, 7, 7, 4, 20, 19),
                       y=c(df$y, 1,1,1,1, 1,1),
                       case = '0.5 threshold not ok')

df.lin.log = rbind(df, df_extra)
p1 = ggplot(df.lin.log, aes(x=x,y=y)) +
  geom_point(position = position_jitter(width=0, height=0.02)) +
  geom_smooth(method='lm', se=FALSE) +
  my_theme() +
  scale_y_continuous('Tumor class', breaks = c(0, 0.5, 1), labels = c('benign tumor', '0.5',  'malignant tumor'), limits = c(-0.1,1.3)) +
  scale_x_continuous('Tumor size') +
  facet_grid(. ~ case) +
  geom_hline(yintercept=0.5, linetype = 3)

p1
```


### Logistic regression
The solution is logistic regression.
Instead of fitting a straight line/hyperplane and uses a non-linear function, the logistic function to squeeze the output between 0 and 1.
The logistic function is defined as $$ logistic(\eta) = \frac{1}{1 + exp(-\eta)}$$
And it looks like this:

```{r, logistic-function, fig.cap="The logistic function. At input 0 it outputs 0.5."}
logistic = function(x){1 / (1 + exp(-x))}

x = seq(from=-6, to = 6, length.out = 100)
df = data.frame(x = x,
                y = logistic(x))
ggplot(df) + geom_line(aes(x=x,y=y)) + my_theme()
```
The step from linear regression models to logistic regression is kind of straight forward. Before we modeled the relationship like this: $$\hat{y}_{i} = \beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{K} x_{i,K} $$
Now we want probabilities, which are between 0 and 1, so we wrap the right side of the equation into the logistic regression function and simply force the output to be between 0 and 1:
$$P(y_{i}=1) =  \frac{1}{1 + exp(-(\beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{K} x_{i,K}))}$$

Let's check the tumor size example again. But now instead of the linear regression model, we use the logistic regression model:
```{r, logistic-class-threshold, fig.cap="Logistic regression successfully finds the correct decision boundary to distinguish between malignant (class=1) and benign (class=0) tumors dependent on it's size in this illustrative example. The blue line is the logistic function shifted and squeezed so that it fits the data."}
logistic1 = glm(y ~ x, family = binomial, data = df.lin.log[df.lin.log$case == '0.5 threshold ok',])
logistic2 = glm(y ~ x, family = binomial, data = df.lin.log)

lgrid = data.frame(x = seq(from=0, to=20, length.out=100))
lgrid$y1_pred = predict(logistic1, newdata = lgrid, type='response')
lgrid$y2_pred = predict(logistic2 , newdata = lgrid, type='response')


p1 = ggplot(df.lin.log, aes(x=x,y=y)) +
  geom_line(aes(x=x, y=y1_pred), data = lgrid, color='blue', size=1) +
  geom_point(position = position_jitter(width=0, height=0.02)) +
  my_theme() +
  scale_y_continuous('Tumor class', breaks = c(0, 0.5, 1), labels = c('benign tumor', '0.5',  'malignant tumor'), limits = c(-0.1,1.3)) +
  scale_x_continuous('Tumor size') +
  facet_grid(. ~ case) +
  geom_hline(yintercept=0.5, linetype = 3)

p1
```
It works better than with logistic regression and we can use 0.5 as a threshold. The line does not shift much,  when including the additional datapoints.

### Interpretation
The interpretation of the coefficients differs from linear regression models. Because now our target value is not some arbitrary number, but a probability between 0 and 1. Also through the logistic function, the influence of the features on the target probability has become non-linear. That's why we need to reformulate the equation for the interpretation>
$$log\left(\frac{P(y_{i}=1)}{(1 - P(y_{i}=1))}\right) =  log\left(\frac{P(y_{i}=1)}{ P(y_{i}=0)}\right) = \beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{K} x_{i,K}$$
$\frac{P(y_{i}=1)}{(1 - P(y_{i}=1))}$ is also called  odds (probability of event vs. probability of no event) and $log\left(\frac{P(y_{i}=1)}{(1 - P(y_{i}=1))}\right)$ are the log odds. So with a logistic regression model we have a linear model for the log odds. Great! Doesn't sound helpful!
Well, with a bit of shuffling again, you can find out how the prediction changes, when one of the features $x{\cdot, k}$ is changed by 1 point. For this we can first apply the $exp()$ function on both sides of the equation:
$$\frac{P(y_{i}=1)}{(1 - P(y_{i}=1))} = odds_i =  exp\left(\beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{K} x_{i,K}\right)$$
Then we compare what happens when we increase one of the $x_{i,j}'s$ by 1. But instead of looking at the difference, we look at the ratio of the two predictions, you will see why:
$$ \frac{odds_{i, x_i + 1}}{odds_i}= \frac{exp\left(\beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{k} \cdot (x_{i,k} + 1)  + \ldots+ \beta_{K} x_{i,K}\right)}{exp\left(\beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{k} \cdot x_{i,k}  + \ldots+ \beta_{K} x_{i,K}\right) }  $$
Using the rule that $\frac{exp(a)}{exp(b)} = exp(a - b)$ gives us:
$$ \frac{odds_{i, x_i + 1}}{odds_i}=exp\left( (\beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{k} \cdot (x_{i,k} + 1)  + \ldots+ \beta_{K} x_{i,K}\right) - \left(\beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{k} \cdot x_{i,k}  + \ldots+ \beta_{K} x_{i,K})\right)$$
And then we can remove a lot of terms from the equation, which is convenient:
$$ \frac{odds_{i, x_i + 1}}{odds_i}=  exp\left( \beta_{k} \cdot (x_{i,k} + 1) - \beta_{k} \cdot x_{i,k} \right) = exp\left(\beta_k\right)$$

And we end up with something simple like $\exp(\beta_k)$. So a change in $x_k$ by one unit changes the odds ratio (multiplicatively) by a factor of $\exp(\beta_k)$. We could also interpret it this way: A change in $x_k$ by one unit change the log odds ratio by $\beta_k$ units, but most people do the former because thinking in logs is known to be hard on the brain. Interpreting the odds ratio already needs a bit of getting used to. If you have odds of 2, it means that the probability for $y_i = 1$ is twice as big as $y_i = 0$. If you have a $\beta$ (=odds ratio) of $0.7$, then an increase in the respective x by one unit multiplies the odds by $\exp(0.7) \approx 2$ and your odds would be 4. But usually you don't deal with the odds and only interpret the $\beta$ as the odds ratios. Because for actually calculating the odds you would need to set a value for each $x_{i,k}$ for all $k$, which only makes sense if you want to look at one specific instance of your dataset.

Here are the interpretations for the logistic regression model with different variable types:

- Continuous variable: For an increase of one unit of the variable $x_{j}$ the estimated odds change (multiplicatively) by a factor of $\exp{\beta_{j}}$
- Binary categorical variables: One of the variables is the reference level (in some languages the one that was coded in 0). A change of the variable $x_{i}$ the reference level to the other category changes the estimated odds change (multiplicatively) by a factor of $\exp{\beta_{j}}$
- Categorical variables with many levels: One solution to deal with many variables is to one-hot-encode them, meaning each level gets it's own column. From a categorical variable with L levels, you only need L-1 columsn, otherwise it is over parameterized. The interpretation for each level is then according to the binary variables. Some language like R allow to
- Intercept $\beta_{0}$: The interpretation is: Given all continuous variables are zero and the categorical variables are on the reference level, the estimated odds are is $\exp{\beta_{0}}$. The interpretation of $\beta_{0}$ is usually not relevant.

### Example
With the logistic regression model we can predict cervical cancer given risk factors.
```{r logistic-example}

neat_cervical_names = c('Intercept', 'Hormonal contraceptives y/n',
                        'Smokes y/n', 'Num. of pregnancies',
                        'Num. of diagnosed STDs',
                        'Intrauterine device y/n')

# Load cervical data
source(sprintf('%s/create-cervical-cancer-data.R', src_dir))
# Fit logistic model for probability of cancer, use few variables that are interesting
mod = glm(Biopsy ~ Hormonal.Contraceptives + Smokes + Num.of.pregnancies + STDs..Number.of.diagnosis + IUD,
          data = cervical, family = binomial())
# Print table of coef, exp(coef), std, p-value
coef.table = summary(mod)$coefficients[,c('Estimate', 'Std. Error')]
coef.table = cbind(coef.table, 'Odds ratio' = as.vector(exp(coef.table[, c('Estimate')])))
# Interpret one continuous and one factor
rownames(coef.table) = neat_cervical_names
kable(coef.table[, c('Estimate', 'Odds ratio', 'Std. Error')])
```

Interpretation of a numerical variable ('Num. of diagnosed STDs'): An increase of the number of diagnosed STDs changes (decreases) the odds for cancer vs. no cancer multiplicatively by `r sprintf('%.2f', coef.table['Num. of diagnosed STDs', 'Odds ratio'])`, given all other features stay the same. Keep in mind that correlation does not imply causation. No recommendation here to get STDs.

Interpretation of a categorical variable ('Hormonal contraceptives y/n'): For women with hormonal contraceptives, the odds for cancer vs no cancer are by a factor of `r sprintf('%.2f', coef.table['Hormonal contraceptives y/n', 'Odds ratio'])` higher, compared women without hormonal contraceptives, given all other features stay the same.

Again as in the linear models, the interpretations are always coming with the clause that 'all other features stay the same'.




## Decision trees
Linear models fail in situation where the relationship is non-linear and/or where the features are interacting with each other. Time to shine for the decision trees!
Tree-based models partition the data along the features into rectangles. For predicting the outcome in each rectangle it fits a simple model (for example the average of the outcome of the instances that fall into this rectangle). Trees have an intuitive structure starting from a root and splitting into nodes, according to cutoff values of the features. After each split, the instances fall into one of the new nodes. At the end of the training all the instances from the training data set are assigned into one of the leaf nodes. See Figure \@ref{fig:tree-artificial} for illustration.

There are a lot of different tree algorithms. They differ in structure (number of splits per node), criteria for how to find the splits, when to stop splitting and how to estimate the simple models within the leaf nodes. Classification and regression trees (CART) is one of the more popular algorithms for tree building. This book will only talk about CART, because in the interpretation they are all the same. If you know of some tree algorithm with a different interpretation, I would welcome your feedback.

Each of these rectangles is associated with a simple model of the outcome of the interest. This is usually estimated by taking the mean of outcomes from all training instances that fall into a rectangle. I recommend the book 'The elements of statistical learning' [@Hastie2009] for a more detailed introduction.
```{r tree-artificial, fig.cap='Exemplary decision tree with artificial data', out.width='100%'}
set.seed(42)
n = 100
dat_sim = data.frame(feature_x1 = rep(c(1,1,2,2), times = n), feature_x2 = rep(c(2,3,3,3), times = n), y = rep(c(1, 2, 3, 4), times = n))
dat_sim = dat_sim[sample(1:nrow(dat_sim), size = 0.9 * nrow(dat_sim)), ]
dat_sim$y = dat_sim$y + rnorm(nrow(dat_sim), sd = 0.2)
ct = ctree(y ~ feature_x1 + feature_x2, dat_sim)
plot(ct, inner_panel = node_inner(ct, pval = FALSE))
```

The following formula describes relationship of y and x (in which rectangle does x fall?)
$$\hat{y}_i = \hat{f}(x_i) = \sum_{m = 1}^M c_m I\{x_i \in R_m\}$$
Each instance $x_i$ falls into exactly one leaf node (=rectangle), so $I_{\{x_i \in R_m\}}$ is only 1 for the this single leaf node ($I$ is the identity function which is 1 if $x_i \in R_m$ and else 0). If $x_i$ falls into leaf node $R_l$, the predicted outcome $\hat{y} = c_l$, where $c_l$ is the mean of all the training instances in leaf node $R_l$.

But where do the 'rectangles' come from? This is quite simple: The algorithm takes a feature and tries which cut-off point minimizes the sum of squares if it is a regression task or the Gini index in classification tasks. It's the cut-off point that makes the two resulting subsets as different as possible in terms of the outcome variable of interest. For categorial features the algorithm tries different groupings by category into to nodes. After this was done for each feature, the algorithm looks for the feature with the best cut-off and chooses this to split the node into two new nodes. The algorithm continues doing this in both new nodes until the stopping criteria is reached. Possible criteria are: A minimum number of observations that have to be in a node before the split, the minimum number of instances that have to be in a terminal node.

A common strategy is to grow a tree fully and then cut it back to optimize it's complexity measure $cp$.

### Interpretation
It's easy: Starting from the root node you go to the next nodes and the edges tell you which subsets you are looking at. Once you reach the leaf node, the node tells you the predicted outcome. All the edges are connected by 'AND'.

Template: If feature x is [smaller/bigger] than threshold c AND ..., then the predicted value is $\hat{y}$.


### Interpretation example
Let's have a look again at the speed dating example. Again we want to predict the rating from the participants, how much they will like the rating partners.

```{r tree-example, fig.cap='Regression tree fitted on the speed dating data. Categories of goal of date feature have been abbreviated for readability: f = for fun, m = meet new people, d = date, r = relationship, e = experience, o = other'}


# increases readability of tree
x = rpart(y ~ ., data = na.omit(dat), method = 'anova', control = rpart.control(cp = 0, maxdepth = 2))

xp = as.party(x)
plot(xp, type = 'extended')
```
The first split was done in the workingday variable, which tells if a day is a working day or a saturday/sunday/holiday. On days without work, the number of rental bikes was higher on average.
In both child nodes the the next feature that was chosen was temperature.

In waves of size 20 or smaller, participants who gave 5/10 or more to importance of same religion of partner, they also rated lower on average (median around 6). If religion was less important (4 or lower), than they gave higher ratings.


### Advantages
The tree structure is perfectly suited to **cover interactions** between features in the data. The data also ends up in **distinct groups**, which are easier to grasp than points on a hyperplane like in linear regression. The interpretation is arguably pretty straight forward. The tree structure also has a **natural visualization**, with it's nodes and edges.

### Disadvantages
**Handling of real linear relationships**, that's what trees suck at. Any real linear relationship between an input feature and the outcome has to be approximated by hard splits, which produces a step function. This is not efficient. This goes hand in hand with **lack of smoothness**. Slight changes in the input feature can have a big impact on the predicted outcome, which might not be desirable. Imagine a tree that predicts the worth of a house and the tree splits in the square meters multiple times. One of the splits is at 100.5 square meters. When a user measure his house and arrives at 99 square meters, types it into some nice web interface and get's 200 000 Euro. The user notices that she forgot to measure a small storeroom with 2 square meters. The storeroom has a skewed wall, so she is not sure if she can count it fully towards the whole flat area or only half of the space. So she decides to try both  100.0 and 101.0 square meters. The results: 200 000 Euro and 205 000 Euro, which is quite unintuitive.

Trees are also quite **unstable**, so a few changes in the training data set might create a completely different tree. That's because each splits depends on the parent split. It does not generate trust if the structure flips so easily.




## Other simple, interpretable models
### Naive bayes classifier
The naive Bayes classifier make use of the Bayes'theorem. For each feature it computes the probability for a class given the features value. The clue is that naive Bayes does so for each feature independently, which is the same as having a strong (=naive) assumption of independence of the features.
Naive Bayes is a conditional probability model and models the probability of a class $k$ in the following way:
$$ P(C_k|x) = \frac{1}{Z} P(C_k) \prod_{i=1}^n P(x_i | C_k)$$

The term $Z$ is a scaling parameter that ensures that the probabilities for all classes sum up to 1.

Naive Bayes is an interpretable model, because of the independence assumption. For each classification it is very clear for each feature how much it contributes towards a certain class prediction.

### k-nearest neighbours
k-nearest neighbour can be used for regression and classification and uses the closest neighbours for a data point for prediction. For classification it assigns the class most common the closest $k$ neighbours of an instance and for regression it takes the average of the outcome. The tricky parts are finding the right $k$ and defining the neighbourhood. This algorithm is different from the other intepretable models presented in this book, since it is an instance-based learning algorithm.
How is k-nearest neighbour interpretable? For starters, there is no global model interpretability, since the model is inherently local and there are no global weights or structures that are learned explicitly by the k-nearest neighbour method. Maybe it is interpretable on a local level? To explain a prediction, you can always retrieve the k-neighbours that were used. If this is interpretable solely depends if you can 'interpret' single instances in the dataset. If the dataset consists of hundreds or thousands of features, then it is not interpretable I'd argue. But if you have few features or a way to reduce your instance to the most important features, presenting the k-nearest neighbours can give you good explanations.

### RuleFit
The RuleFit algorithm from Friedman and Propescu [@friedman2008predictive] is a regression and classification approach that uses decision rules in a linear model. RuleFit consists of two components: The first component produces "rules" and the second component fits a linear model with these rules as input (hence the name "RuleFit"). It enables automatic integration of interactions between features into a linear model, while having the interpretability of a sparse linear model.

There are two steps involved:

Step 1: Rule generation:

The rules that the algorithm generates have a simple form:

if  $x2 < 3$  and  $x5 < 7$  then  $1$  else  $0$

The rules are generated from the covariates matrix X. You can also see the rules simply as new features based on your original features.

The RuleFit paper uses the Boston housing data as example: The goal is to predict the median house value in the Boston neighborhood. One of the rules that is generated by RuleFit is:

if  (number of rooms $> 6.64$)  and  (concentration of nitric oxide $< 0.67$) then $1$ else $0$

The interesting part is how those rules are generated: They are derived from Decision Trees, by basically disassembling them. Every path in a tree can be turned into a decision rule. You simply chain the binary decisions that lead to a certain node, et voilÃ , you have a rule. It is desirable to generate a lot of diverse and meaningful rules. Gradient boosting is used to fit an ensemble of decision trees (by regressing/classifying y with your original features X). Each resulting trees is turned into multiple rules.

```{r rulefit, fig.cap="4 rules can be generated from a tree with 3 terminal nodes.", out.width="80%"}
knitr::include_graphics("images/rulefit.jpg")
```

Another way to see this step is a black box, that generates a new set of features X' out of your original features X. Those features are binary and can represent quite complex interactions of your original X. The rules are chosen to maximise the prediction/classification task at hand.

Step 2: Sparse linear model

You will get A LOT of rules from the first step (and that is what you want). Since the first step is only a feature transformation function on your original data set you are still not done with fitting a model and also you want to reduce the number of rules. Lasso or L1 regularised regression is good in this scenario. Next to the rules also all numeric variables from your original data set will be used in the Lasso linear model. Every rule and numeric variable gets a coefficient (beta). And thanks to the regularisation, a lot of those betas will be estimated to zero. The numeric variables are added because trees suck at representing simple linear relationships between y and x. The outcome is a linear model that has linear effects for all of the numeric variables and also linear terms for the rules.


The interpretation is the same as with linear models, the only difference is that some features are now binary rules.

The paper not only introduces RuleFit and evaluates it, but it also comes with a bunch of useful tools, (comparable to Random Forest): Measurement tools for variable importance, degree of relevance of original input variables and interaction effects between variables.

### And so many more ...
There are lots and lots of algorithms that produce interpretable models and not all will be listed here. If you are a researcher or just a big fan and user of a certain interpreable method that is not listed here, get in touch with me and add the method to this book!
