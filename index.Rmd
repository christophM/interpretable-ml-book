---
title: "Interpretable Machine Learning"
subtitle: "A Guide for Making Black Box Models Explainable."
author: "Christoph Molnar"
date: "`r Sys.Date()`"
knit: "bookdown::render_book"
documentclass: krantz
biblio-style: apalike
link-citations: yes
colorlinks: yes
lot: no
lof: no
fontsize: 12pt
monofont: "Source Code Pro"
monofontoptions: "Scale=0.7"
site: bookdown::bookdown_site
description: "Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable."
#url:
github-repo: christophM/interpretable-ml-book
bibliography: [book.bib]
always_allow_html: yes
#cover-image: images/cover.jpg
---
```{r setup, cache=FALSE, include=FALSE}
library('knitr')
library('tm')
library('rpart')
library('mlr')
library('dplyr')
library('ggplot2')
library('tidyr')
library('partykit')
library('memoise')
library('pre')
library('iml')

opts_chunk$set(
  echo=FALSE,
  message=FALSE,
  warning=FALSE,
  out.width='80%',
  fig.align='center'
)

output <- opts_knit$get("rmarkdown.pandoc.to")

# set paths
data_dir = './data'
src_dir = './src'
devtools::load_all()



## load datasets
bike.data = get.bike.data()
bike.task = get.bike.task()

cervical.data = get.cervical.data()
cervical.task = get.cervical.task()


ycomments.data = get.ycomments.data()

```

# Preface {-}

Machine learning has a huge potential to improve products, processes and research.
But machines usually don't give an explanation for their predictions, which hurts trust and creates a barrier for the adoption of machine learning.
This book is about making machine learning models and their decisions interpretable.

Machine learning models are already used to choose the best advertisement for you, it filters out spam from your emails and it even assesses risk in the judicial system which ultimately can have consequences for your freedom.
Can everyone trust the learned model?
The model might perform well on the training data, but are the learned associations general enough to transfer to new data?
Are there some oddities in the training data which the machine learning model dutifully picked up?
This book will give you an overview over techniques that you can use to make black boxes as transparent as possible and make their predictions interpretable.
The first part of the book introduces simple, interpretable models and instructions how to do the interpretation.
The later chapters focus on general model-agnostics tools that help analysing complex models and making their decisions interpretable.
In an ideal future, machines will be able to explain their decisions and the algorithmic age we are moving towards will be as human as possible.

This books is recommended for machine learning practitioners, data scientists, statisticians and anyone else interested in making machine decisions more human.


**About me:** My name is Christoph Molnar, I am something between statistician and machine learner.
I work on making machine learning interpretable.
If you are interested in bringing interpretability to your machine learning models, feel free to contact me!

Mail: christoph.molnar.ai@gmail.com

Website: [https://christophm.github.io/](https://christophm.github.io/)

Follow me on Twitter! [\@ChristophMolnar](https://twitter.com/ChristophMolnar)

**Donate:** I am writing this book in my free time and without dedicated funding.
Donations are very welcome!
Funds will be used to pay for my coffee and other support like hiring someone to create illustrations for the book.
Via PayPal: [https://www.paypal.me/ChristophMolnar](https://www.paypal.me/ChristophMolnar) or via Bitcoin:
```{r}
shiny::includeHTML("html/btc-donate.html")
```
 ![Creative Commons License](images/by-nc-sa.png)
This book is licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).
